
 make.py started: 2018-05-25 20:11:30 /home/ubuntu/SLAB/tests/MLAlgorithms (Native Implementations)/src 


name := "SystemMLAlgs"

version := "0.1"

scalaVersion := "2.10.4"

libraryDependencies ++= Seq(
    "org.apache.spark" %% "spark-mllib" % "2.2.0" % "provided"
)

assemblyMergeStrategy in assembly := {
  case PathList("javax", "servlet", xs @ _*) => MergeStrategy.last
  case PathList("javax", "activation", xs @ _*) => MergeStrategy.last
  case PathList("org", "apache", xs @ _*) => MergeStrategy.last
  case PathList("com", "google", xs @ _*) => MergeStrategy.last
  case PathList("com", "esotericsoftware", xs @ _*) => MergeStrategy.last
  case PathList("com", "codahale", xs @ _*) => MergeStrategy.last
  case PathList("com", "yammer", xs @ _*) => MergeStrategy.last
  case "about.html" => MergeStrategy.rename
  case "META-INF/ECLIPSEF.RSA" => MergeStrategy.last
  case "META-INF/mailcap" => MergeStrategy.last
  case "META-INF/mimetypes.default" => MergeStrategy.last
  case "plugin.properties" => MergeStrategy.last
  case "log4j.properties" => MergeStrategy.last
  case x =>
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}

Running: sbt -Dsbt.log.noformat=true assembly 

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
name := "MLLibAlgs"

version := "0.1"

scalaVersion := "2.10.4"

libraryDependencies ++= Seq (
    "org.apache.spark" %% "spark-core" % "2.2.0" % "provided",
    "org.apache.spark" %% "spark-streaming" % "2.2.0" % "provided",
    "org.apache.spark" %% "spark-mllib" % "2.2.0" % "provided"
)

assemblyMergeStrategy in assembly := {
  case PathList("javax", "servlet", xs @ _*) => MergeStrategy.last
  case PathList("javax", "activation", xs @ _*) => MergeStrategy.last
  case PathList("org", "apache", xs @ _*) => MergeStrategy.last
  case PathList("com", "google", xs @ _*) => MergeStrategy.last
  case PathList("com", "esotericsoftware", xs @ _*) => MergeStrategy.last
  case PathList("com", "codahale", xs @ _*) => MergeStrategy.last
  case PathList("com", "yammer", xs @ _*) => MergeStrategy.last
  case "about.html" => MergeStrategy.rename
  case "META-INF/ECLIPSEF.RSA" => MergeStrategy.last
  case "META-INF/mailcap" => MergeStrategy.last
  case "META-INF/mimetypes.default" => MergeStrategy.last
  case "plugin.properties" => MergeStrategy.last
  case "log4j.properties" => MergeStrategy.last
  case x =>
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}

Running: sbt -Dsbt.log.noformat=true assembly 

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkContext
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.mllib.linalg._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.sql._
import scala.tools.nsc.io._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")
        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val inputPath = argMap("inputPath")
        val nodes = argMap("nodes")
        val featureNames = argMap("featureNames")
        val isSparse = inputPath.contains("sparse")

        val sparse_stub = if (isSparse) "_sparse" else "_dense"
        val stub = "/tests/MLAlgorithms (Native Implementations)/output/"
        val base = s"mllib_${opType}${nodes}${sparse_stub}.txt"
        val path = root + stub + base

        val input = spark.read.parquet(inputPath).
            withColumnRenamed(featureNames,"features")
        input.persist(MEMORY_AND_DISK_SER)
        println(input.count)

        val xRM = opType match {
            case "pca" => parquet_to_rm(input.select("features"))
            case _     => blank_row_matrix(spark.sparkContext)
        }

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test => ${ix}")
            val start = System.nanoTime()

            if (opType == "logit") {
                val lr = new LogisticRegression().
                    setMaxIter(3).
                    setLabelCol("y")
                val params = lr.fit(input)
                println(params.coefficients.size)
            } else if (opType == "reg") {
                val reg = new LinearRegression().
                    setMaxIter(3).
                    setLabelCol("y")
                val params = reg.fit(input)
                println(params.coefficients.size)
            } else if (opType == "pca") {
                val prcomp = xRM.computePrincipalComponents( 5 )
                val prj = xRM.multiply(prcomp)
                prj.rows.count
            } else {
                throw new Exception("Invalid operator")
            }
            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        File(path).appendAll(
            nodes + "," + times.mkString(",") + '\n')
    }

    def parquet_to_rm(df: DataFrame) : RowMatrix = {
        val X_rows = df.repartition(1000).rdd.
            map(tup => DenseVector.fromML(
                tup.getAs[alg.Vector](0).toDense).asInstanceOf[Vector])
        X_rows.persist(MEMORY_AND_DISK_SER)
        return new RowMatrix(X_rows)
    }

    def blank_row_matrix(sc: SparkContext) : RowMatrix = {
        val tmp = sc.parallelize(
            Seq(Vectors.dense(1,2,3), Vectors.dense(1,2,3)))
        return new RowMatrix(tmp)
    }
}

Running: spark-submit --class SparkMLAlgorithms   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=reg inputPath=/scratch/adclick_clean_1_sparse.parquet nodes=4 featureNames=features stub=_1

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SQLContext
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.runtime.instructions.spark.utils._
import org.apache.sysml.runtime.matrix.MatrixCharacteristics
import org.apache.spark.storage.StorageLevel._
import scala.tools.nsc.io._
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.sql.DataFrame
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms {
    def main(args: Array[String]) {
        val spark = SparkSession.builder.getOrCreate()
        val sc = spark.sparkContext
        val ml = new MLContext(sc)

        val root = sys.env("BENCHMARK_PROJECT_ROOT")
        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val inputPath = argMap("inputPath")
        val nodes = argMap("nodes")
        val featureNames = argMap("featureNames")
        val isSparse = inputPath.contains("sparse")
        val execSpark = argMap.get("execSpark")

        val execution_type = execSpark match {
            case Some(arg) => MLContext.ExecutionType.SPARK
            case None => MLContext.ExecutionType.DRIVER_AND_SPARK
        }

        val exec_type_stub = execSpark match {
            case Some(arg) => "spark"
            case None      => "spark_and_driver"
        }

        println(execution_type)
        ml.setExecutionType(execution_type)
        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val sparse_stub = if (isSparse) "_sparse" else "_dense"
        var stub = "/tests/MLAlgorithms (Native Implementations)/output/"
        val base = s"systemml_${opType}${nodes}${sparse_stub}_${exec_type_stub}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val script_stub = "tests/MLAlgorithms (Native Implementations)/src"
        val script_path = s"${root}/${script_stub}/systemml/src/main/dml"
        val input_df = spark.read.parquet(inputPath)
        val x = input_df.select(featureNames).repartition(1000)
        val y = input_df.select("y")
        
        println( x.count )
        var times = Array.ofDim[Double](5,1)
        if (opType == "logit") {
            val script = dmlFromFile(s"${script_path}/MultiLogitReg.dml").
                 in(Map("X"     -> x,
                        "Y_vec" -> y,
                        "$moi"  -> 3,
                        "$mii"  -> 0)).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else if (opType == "reg") {
             val script = dmlFromFile(s"${script_path}/LinearRegCG.dml").
                 in("X", x).
                 in("$maxi", 3).
                 in("y", y).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else if (opType == "pca") {
             val script = dmlFromFile(s"${script_path}/PCA.dml").
                 in("A", x).
                 in("K", 5).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else {
             throw new Exception("Invalid operator")
        }

        File(path).appendAll(
            nodes + "," + times(0).map(x => x/1000.0).mkString(",") + '\n')
    }
}
./systemml/src/main/dml/PCA.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
# 
#   http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# 
# This script performs Principal Component Analysis (PCA) on the given input data.
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME   TYPE   DEFAULT  MEANING
# ---------------------------------------------------------------------------------------------
# INPUT  String ---      Location to read the matrix A of feature vectors
# K      Int    ---      Indicates dimension of the new vector space constructed from eigen vectors
# CENTER Int    0        Indicates whether or not to center data 
# SCALE  Int    0        Indicates whether or not to scale data 
# OFMT   String ---      Output data format
# PROJDATA Int  0        This argument indicates if the data should be projected or not
# MODEL  String ---      Location to already existing model: eigenvectors and eigenvalues 
# OUTPUT String /        Location to write output matrices (covariance matrix, new basis vectors, 
#                           and data projected onto new basis vectors)
# hadoop jar SystemML.jar -f PCA.dml -nvargs INPUT=INPUT_DIR/pca-1000x1000 
# OUTPUT=OUTPUT_DIR/pca-1000x1000-model PROJDATA=1 CENTER=1 SCALE=1
# ---------------------------------------------------------------------------------------------

#A = read($INPUT);

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

sum_A = sum( A );
times = matrix(0, rows = 5, cols = 1)
for (ix in 1:5) {
    if ( sum_A > 0.0 ) {
        start = time(1)
    }

    K = ifdef($K, ncol(A));
    ofmt = ifdef($OFMT, "CSV");
    projectData = ifdef($PROJDATA,1);
    model = ifdef($MODEL,"");
    center = ifdef($CENTER,0);
    scale = ifdef($SCALE,0);
    output = ifdef($OUTPUT,"/");

    evec_dominant = matrix(0,cols=1,rows=1);

    if (model != "") {
        pass = 1.0
        # reuse existing model to project data
        #evec_dominant = read(model+"/dominant.eigen.vectors");
    } else {
        if (model == "" ){
            model = output; 
        }   

        N = nrow(A);
        D = ncol(A);

        # perform z-scoring (centering and scaling)
        if (center == 1) {
            cm = colMeans(A);
            A = A - cm;
        }
        if (scale == 1) {
            cvars = (colSums (A^2));    
            if (center == 1){
            cm = colMeans(A);
                cvars = (cvars - N*(cm^2))/(N-1);           
            }
            Azscored = (A)/sqrt(cvars);
                A = Azscored;
        }   

        # co-variance matrix 
        mu = colSums(A)/N;
        C = (t(A) %*% A)/(N-1) - (N/(N-1))*t(mu) %*% mu;


        # compute eigen vectors and values
        [evalues, evectors] = eigen(C);

        decreasing_Idx = order(target=evalues,by=1,decreasing=TRUE,index.return=TRUE);
        diagmat = table(seq(1,D),decreasing_Idx);
        # sorts eigenvalues by decreasing order
        evalues = diagmat %*% evalues;
        # sorts eigenvectors column-wise in the order of decreasing eigenvalues
        evectors = evectors %*% diagmat;


        # select K dominant eigen vectors 
        nvec = ncol(evectors);

        eval_dominant = evalues[1:K, 1];
        evec_dominant = evectors[,1:K];
        
        # the square root of eigenvalues
        eval_stdev_dominant = sqrt(eval_dominant);
        
        #write(eval_stdev_dominant, model+"/dominant.eigen.standard.deviations", format=ofmt);
        #write(eval_dominant, model+"/dominant.eigen.values", format=ofmt);
        #write(evec_dominant, model+"/dominant.eigen.vectors", format=ofmt);
    }
    if (projectData == 1 | model != ""){
        # Construct new data set by treating computed dominant eigenvectors as the basis vectors
        newA = A %*% evec_dominant;
        sum_newA = sum( newA )
        if (sum_newA > 0) {
            print(sum_newA)
        }
        #write(newA, output+"/projected.data", format=ofmt);
    }

    if (sum_A > 0.0) {
        stop = time(1)
    }
    times[ix,1] = (stop - start)
}

times = t(times)
./systemml/src/main/dml/LinearRegCG.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

#
# THIS SCRIPT SOLVES LINEAR REGRESSION USING THE CONJUGATE GRADIENT ALGORITHM
#
# INPUT PARAMETERS:
# --------------------------------------------------------------------------------------------
# NAME  TYPE   DEFAULT  MEANING
# --------------------------------------------------------------------------------------------
# X     String  ---     Location (on HDFS) to read the matrix X of feature vectors
# Y     String  ---     Location (on HDFS) to read the 1-column matrix Y of response values
# B     String  ---     Location to store estimated regression parameters (the betas)
# O     String  " "     Location to write the printed statistics; by default is standard output
# Log   String  " "     Location to write per-iteration variables for log/debugging purposes
# icpt  Int      0      Intercept presence, shifting and rescaling the columns of X:
#                       0 = no intercept, no shifting, no rescaling;
#                       1 = add intercept, but neither shift nor rescale X;
#                       2 = add intercept, shift & rescale X columns to mean = 0, variance = 1
# reg   Double 0.000001 Regularization constant (lambda) for L2-regularization; set to nonzero
#                       for highly dependend/sparse/numerous features
# tol   Double 0.000001 Tolerance (epsilon); conjugate graduent procedure terminates early if
#                       L2 norm of the beta-residual is less than tolerance * its initial norm
# maxi  Int      0      Maximum number of conjugate gradient iterations, 0 = no maximum
# fmt   String "text"   Matrix output format for B (the betas) only, usually "text" or "csv"
# --------------------------------------------------------------------------------------------
# OUTPUT: Matrix of regression parameters (the betas) and its size depend on icpt input value:
#         OUTPUT SIZE:   OUTPUT CONTENTS:                HOW TO PREDICT Y FROM X AND B:
# icpt=0: ncol(X)   x 1  Betas for X only                Y ~ X %*% B[1:ncol(X), 1], or just X %*% B
# icpt=1: ncol(X)+1 x 1  Betas for X and intercept       Y ~ X %*% B[1:ncol(X), 1] + B[ncol(X)+1, 1]
# icpt=2: ncol(X)+1 x 2  Col.1: betas for X & intercept  Y ~ X %*% B[1:ncol(X), 1] + B[ncol(X)+1, 1]
#                        Col.2: betas for shifted/rescaled X and intercept
#
# In addition, some regression statistics are provided in CSV format, one comma-separated
# name-value pair per each line, as follows:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------
# AVG_TOT_Y             Average of the response value Y
# STDEV_TOT_Y           Standard Deviation of the response value Y
# AVG_RES_Y             Average of the residual Y - pred(Y|X), i.e. residual bias
# STDEV_RES_Y           Standard Deviation of the residual Y - pred(Y|X)
# DISPERSION            GLM-style dispersion, i.e. residual sum of squares / # deg. fr.
# R2                    R^2 of residual with bias included vs. total average
# ADJUSTED_R2           Adjusted R^2 of residual with bias included vs. total average
# R2_NOBIAS             R^2 of residual with bias subtracted vs. total average
# ADJUSTED_R2_NOBIAS    Adjusted R^2 of residual with bias subtracted vs. total average
# R2_VS_0               * R^2 of residual with bias included vs. zero constant
# ADJUSTED_R2_VS_0      * Adjusted R^2 of residual with bias included vs. zero constant
# -------------------------------------------------------------------------------------
# * The last two statistics are only printed if there is no intercept (icpt=0)
#
# The Log file, when requested, contains the following per-iteration variables in CSV
# format, each line containing triple (NAME, ITERATION, VALUE) with ITERATION = 0 for
# initial values:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------
# CG_RESIDUAL_NORM      L2-norm of Conj.Grad.residual, which is A %*% beta - t(X) %*% y
#                           where A = t(X) %*% X + diag (lambda), or a similar quantity
# CG_RESIDUAL_RATIO     Ratio of current L2-norm of Conj.Grad.residual over the initial
# -------------------------------------------------------------------------------------
#
# HOW TO INVOKE THIS SCRIPT - EXAMPLE:
# hadoop jar SystemML.jar -f LinearRegCG.dml -nvargs X=INPUT_DIR/X Y=INPUT_DIR/Y B=OUTPUT_DIR/B
#     O=OUTPUT_DIR/Out icpt=2 reg=1.0 tol=0.001 maxi=100 fmt=csv Log=OUTPUT_DIR/log

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

#fileX = $X;
#fileY = $Y;
#fileB = $B;
#fileO = ifdef ($O, " ");
#fileLog = ifdef ($Log, " ");
#fmtB = ifdef ($fmt, "text");

intercept_status = ifdef ($icpt, 0);     # $icpt=0;
tolerance = ifdef ($tol, 0.000001);      # $tol=0.000001;
max_iteration = ifdef ($maxi, 0);        # $maxi=0;
regularization = ifdef ($reg, 0.000001); # $reg=0.000001;

print ("BEGIN LINEAR REGRESSION SCRIPT");
print ("Reading X and Y...");

sum_x = sum( X )
sum_y = sum( y )
times = matrix(0, rows = 5, cols = 1)
    for (ix in 1:5) {
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        start = time(1)
    }

    n = nrow (X);
    m = ncol (X);
    ones_n = matrix (1, rows = n, cols = 1);
    zero_cell = matrix (0, rows = 1, cols = 1);

    # Introduce the intercept, shift and rescale the columns of X if needed

    m_ext = m;
    if (intercept_status == 1 | intercept_status == 2)  # add the intercept column
    {
        X = cbind (X, ones_n);
        m_ext = ncol (X);
    }

    scale_lambda = matrix (1, rows = m_ext, cols = 1);
    if (intercept_status == 1 | intercept_status == 2)
    {
        scale_lambda [m_ext, 1] = 0;
    }

    if (intercept_status == 2)  # scale-&-shift X columns to mean 0, variance 1
    {                           # Important assumption: X [, m_ext] = ones_n
        avg_X_cols = t(colSums(X)) / n;
        var_X_cols = (t(colSums (X ^ 2)) - n * (avg_X_cols ^ 2)) / (n - 1);
        is_unsafe = (var_X_cols <= 0);
        scale_X = 1.0 / sqrt (var_X_cols * (1 - is_unsafe) + is_unsafe);
        scale_X [m_ext, 1] = 1;
        shift_X = - avg_X_cols * scale_X;
        shift_X [m_ext, 1] = 0;
    } else {
        scale_X = matrix (1, rows = m_ext, cols = 1);
        shift_X = matrix (0, rows = m_ext, cols = 1);
    }

    # Henceforth, if intercept_status == 2, we use "X %*% (SHIFT/SCALE TRANSFORM)"
    # instead of "X".  However, in order to preserve the sparsity of X,
    # we apply the transform associatively to some other part of the expression
    # in which it occurs.  To avoid materializing a large matrix, we rewrite it:
    #
    # ssX_A  = (SHIFT/SCALE TRANSFORM) %*% A    --- is rewritten as:
    # ssX_A  = diag (scale_X) %*% A;
    # ssX_A [m_ext, ] = ssX_A [m_ext, ] + t(shift_X) %*% A;
    #
    # tssX_A = t(SHIFT/SCALE TRANSFORM) %*% A   --- is rewritten as:
    # tssX_A = diag (scale_X) %*% A + shift_X %*% A [m_ext, ];

    lambda = scale_lambda * regularization;
    beta_unscaled = matrix (0, rows = m_ext, cols = 1);

    if (max_iteration == 0) {
        max_iteration = m_ext;
    }
    i = 0;

    # BEGIN THE CONJUGATE GRADIENT ALGORITHM
    print ("Running the CG algorithm...");

    r = - t(X) %*% y;

    if (intercept_status == 2) {
        r = scale_X * r + shift_X %*% r [m_ext, ];
    }

    p = - r;
    norm_r2 = sum (r ^ 2);
    norm_r2_initial = norm_r2;
    norm_r2_target = norm_r2_initial * tolerance ^ 2;
    print ("||r|| initial value = " + sqrt (norm_r2_initial) + ",  target value = " + sqrt (norm_r2_target));
    log_str = "CG_RESIDUAL_NORM,0," + sqrt (norm_r2_initial);
    log_str = append (log_str, "CG_RESIDUAL_RATIO,0,1.0");

    while (i < max_iteration & norm_r2 > norm_r2_target)
    {
        if (intercept_status == 2) {
            ssX_p = scale_X * p;
            ssX_p [m_ext, ] = ssX_p [m_ext, ] + t(shift_X) %*% p;
        } else {
            ssX_p = p;
        }

        q = t(X) %*% (X %*% ssX_p);

        if (intercept_status == 2) {
            q = scale_X * q + shift_X %*% q [m_ext, ];
        }

    	q = q + lambda * p;
    	a = norm_r2 / sum (p * q);
    	beta_unscaled = beta_unscaled + a * p;
    	r = r + a * q;
    	old_norm_r2 = norm_r2;
    	norm_r2 = sum (r ^ 2);
    	p = -r + (norm_r2 / old_norm_r2) * p;
    	i = i + 1;
    	print ("Iteration " + i + ":  ||r|| / ||r init|| = " + sqrt (norm_r2 / norm_r2_initial));
    	log_str = append (log_str, "CG_RESIDUAL_NORM,"  + i + "," + sqrt (norm_r2));
        log_str = append (log_str, "CG_RESIDUAL_RATIO," + i + "," + sqrt (norm_r2 / norm_r2_initial));
    }

    if (i >= max_iteration) {
        print ("Warning: the maximum number of iterations has been reached.");
    }
    print ("The CG algorithm is done.");
    # END THE CONJUGATE GRADIENT ALGORITHM

    if (intercept_status == 2) {
        beta = scale_X * beta_unscaled;
        beta [m_ext, ] = beta [m_ext, ] + t(shift_X) %*% beta_unscaled;
    } else {
        beta = beta_unscaled;
    }

    print ("Computing the statistics...");

    avg_tot = sum (y) / n;
    ss_tot = sum (y ^ 2);
    ss_avg_tot = ss_tot - n * avg_tot ^ 2;
    var_tot = ss_avg_tot / (n - 1);
    y_residual = y - X %*% beta;
    avg_res = sum (y_residual) / n;
    ss_res = sum (y_residual ^ 2);
    ss_avg_res = ss_res - n * avg_res ^ 2;

    R2 = 1 - ss_res / ss_avg_tot;
    if (n > m_ext) {
        dispersion  = ss_res / (n - m_ext);
        adjusted_R2 = 1 - dispersion / (ss_avg_tot / (n - 1));
    } else {
        dispersion  = 0.0 / 0.0;
        adjusted_R2 = 0.0 / 0.0;
    }

    R2_nobias = 1 - ss_avg_res / ss_avg_tot;
    deg_freedom = n - m - 1;
    if (deg_freedom > 0) {
        var_res = ss_avg_res / deg_freedom;
        adjusted_R2_nobias = 1 - var_res / (ss_avg_tot / (n - 1));
    } else {
        var_res = 0.0 / 0.0;
        adjusted_R2_nobias = 0.0 / 0.0;
        print ("Warning: zero or negative number of degrees of freedom.");
    }

    R2_vs_0 = 1 - ss_res / ss_tot;
    if (n > m) {
        adjusted_R2_vs_0 = 1 - (ss_res / (n - m)) / (ss_tot / n);
    } else {
        adjusted_R2_vs_0 = 0.0 / 0.0;
    }

    str = "AVG_TOT_Y," + avg_tot;                                    #  Average of the response value Y
    str = append (str, "STDEV_TOT_Y," + sqrt (var_tot));             #  Standard Deviation of the response value Y
    str = append (str, "AVG_RES_Y," + avg_res);                      #  Average of the residual Y - pred(Y|X), i.e. residual bias
    str = append (str, "STDEV_RES_Y," + sqrt (var_res));             #  Standard Deviation of the residual Y - pred(Y|X)
    str = append (str, "DISPERSION," + dispersion);                  #  GLM-style dispersion, i.e. residual sum of squares / # d.f.
    str = append (str, "R2," + R2);                                  #  R^2 of residual with bias included vs. total average
    str = append (str, "ADJUSTED_R2," + adjusted_R2);                #  Adjusted R^2 of residual with bias included vs. total average
    str = append (str, "R2_NOBIAS," + R2_nobias);                    #  R^2 of residual with bias subtracted vs. total average
    str = append (str, "ADJUSTED_R2_NOBIAS," + adjusted_R2_nobias);  #  Adjusted R^2 of residual with bias subtracted vs. total average
    if (intercept_status == 0) {
        str = append (str, "R2_VS_0," + R2_vs_0);                    #  R^2 of residual with bias included vs. zero constant
        str = append (str, "ADJUSTED_R2_VS_0," + adjusted_R2_vs_0);  #  Adjusted R^2 of residual with bias included vs. zero constant
    }

    print (str);

    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        stop = time(1)
    }
    times[ix,1] = stop - start
}

times = t(times)
./systemml/src/main/dml/MultiLogitReg.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# Solves Multinomial Logistic Regression using Trust Region methods.
# (See: Trust Region Newton Method for Logistic Regression, Lin, Weng and Keerthi, JMLR 9 (2008) 627-650)

# INPUT PARAMETERS:
# --------------------------------------------------------------------------------------------
# NAME  TYPE   DEFAULT  MEANING
# --------------------------------------------------------------------------------------------
# X     String  ---     Location to read the matrix of feature vectors
# Y     String  ---     Location to read the matrix with category labels
# B     String  ---     Location to store estimated regression parameters (the betas)
# Log   String  " "     Location to write per-iteration variables for log/debugging purposes
# icpt  Int      0      Intercept presence, shifting and rescaling X columns:
#                       0 = no intercept, no shifting, no rescaling;
#                       1 = add intercept, but neither shift nor rescale X;
#                       2 = add intercept, shift & rescale X columns to mean = 0, variance = 1
# reg   Double  0.0     regularization parameter (lambda = 1/C); intercept is not regularized
# tol   Double 0.000001 tolerance ("epsilon")
# moi   Int     100     max. number of outer (Newton) iterations
# mii   Int      0      max. number of inner (conjugate gradient) iterations, 0 = no max
# fmt   String "text"   Matrix output format, usually "text" or "csv" (for matrices only)
# --------------------------------------------------------------------------------------------
# The largest label represents the baseline category; if label -1 or 0 is present, then it is
# the baseline label (and it is converted to the largest label).
#
# The Log file, when requested, contains the following per-iteration variables in CSV format,
# each line containing triple (NAME, ITERATION, VALUE) with ITERATION = 0 for initial values:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------------
# LINEAR_TERM_MIN       The minimum value of X %*% B, used to check for overflows
# LINEAR_TERM_MAX       The maximum value of X %*% B, used to check for overflows
# NUM_CG_ITERS          Number of inner (Conj.Gradient) iterations in this outer iteration
# IS_TRUST_REACHED      1 = trust region boundary was reached, 0 = otherwise
# POINT_STEP_NORM       L2-norm of iteration step from old point (i.e. matrix B) to new point
# OBJECTIVE             The loss function we minimize (negative regularized log-likelihood)
# OBJ_DROP_REAL         Reduction in the objective during this iteration, actual value
# OBJ_DROP_PRED         Reduction in the objective predicted by a quadratic approximation
# OBJ_DROP_RATIO        Actual-to-predicted reduction ratio, used to update the trust region
# IS_POINT_UPDATED      1 = new point accepted; 0 = new point rejected, old point restored
# GRADIENT_NORM         L2-norm of the loss function gradient (omitted if point is rejected)
# TRUST_DELTA           Updated trust region size, the "delta"
# -------------------------------------------------------------------------------------------
#
# Script invocation example:
# hadoop jar SystemML.jar -f MultiLogReg.dml -nvargs icpt=2 reg=1.0 tol=0.000001 moi=100 mii=20
#     X=INPUT_DIR/X123 Y=INPUT_DIR/Y123 B=OUTPUT_DIR/B123 fmt=csv Log=OUTPUT_DIR/log

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

#fileX = $X;
#fileY = $Y;
#fileB = $B;
fileLog = ifdef ($Log, " ");
fmtB = ifdef ($fmt, "text");

intercept_status = ifdef ($icpt, 0); # $icpt = 0;
regularization = ifdef ($reg, 0.0);  # $reg  = 0.0;
tol = ifdef ($tol, 0.000001);        # $tol  = 0.000001;
maxiter = ifdef ($moi, 100);         # $moi  = 100;
maxinneriter = ifdef ($mii, 0);      # $mii  = 0;
tol = as.double (tol);

print ("BEGIN MULTINOMIAL LOGISTIC REGRESSION SCRIPT");
print ("Reading X...");
#X = read (fileX);
print ("Reading Y...");
#Y_vec = read (fileY);

# force a pass over the data

sum_x = sum( X )
sum_y = sum( Y_vec )
times = matrix(0, rows=5, cols=1)

for (ix in 1:5) {
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        start = time(1)
    }

    eta0 = 0.0001;
    eta1 = 0.25;
    eta2 = 0.75;
    sigma1 = 0.25;
    sigma2 = 0.5;
    sigma3 = 4.0;
    psi = 0.1;

    N = nrow (X);
    D = ncol (X);

    # Introduce the intercept, shift and rescale the columns of X if needed
    if (intercept_status == 1 | intercept_status == 2)  # add the intercept column
    {
        X = cbind (X, matrix (1, rows = N, cols = 1));
        D = ncol (X);
    }

    scale_lambda = matrix (1, rows = D, cols = 1);
    if (intercept_status == 1 | intercept_status == 2)
    {
        scale_lambda [D, 1] = 0;
    }

    if (intercept_status == 2)  # scale-&-shift X columns to mean 0, variance 1
    {                           # Important assumption: X [, D] = matrix (1, rows = N, cols = 1)
        avg_X_cols = t(colSums(X)) / N;
        var_X_cols = (t(colSums (X ^ 2)) - N * (avg_X_cols ^ 2)) / (N - 1);
        is_unsafe = var_X_cols <= 0;
        scale_X = 1.0 / sqrt (var_X_cols * (1 - is_unsafe) + is_unsafe);
        scale_X [D, 1] = 1;
        shift_X = - avg_X_cols * scale_X;
        shift_X [D, 1] = 0;
        rowSums_X_sq = (X ^ 2) %*% (scale_X ^ 2) + X %*% (2 * scale_X * shift_X) + sum (shift_X ^ 2);
    } else {
        scale_X = matrix (1, rows = D, cols = 1);
        shift_X = matrix (0, rows = D, cols = 1);
        rowSums_X_sq = rowSums (X ^ 2);
    }

    # Henceforth we replace "X" with "X %*% (SHIFT/SCALE TRANSFORM)" and rowSums(X ^ 2)
    # with "rowSums_X_sq" in order to preserve the sparsity of X under shift and scale.
    # The transform is then associatively applied to the other side of the expression,
    # and is rewritten via "scale_X" and "shift_X" as follows:
    #
    # ssX_A  = (SHIFT/SCALE TRANSFORM) %*% A    --- is rewritten as:
    # ssX_A  = diag (scale_X) %*% A;
    # ssX_A [D, ] = ssX_A [D, ] + t(shift_X) %*% A;
    #
    # tssX_A = t(SHIFT/SCALE TRANSFORM) %*% A   --- is rewritten as:
    # tssX_A = diag (scale_X) %*% A + shift_X %*% A [D, ];

    # Convert "Y_vec" into indicator matrix:
    max_y = max (Y_vec);
    if (min (Y_vec) <= 0) {
        # Category labels "0", "-1" etc. are converted into the largest label
        Y_vec  = Y_vec  + (- Y_vec  + max_y + 1) * (Y_vec <= 0);
        max_y = max_y + 1;
    }
    Y = table (seq (1, N, 1), Y_vec, N, max_y);
    K = ncol (Y) - 1;   # The number of  non-baseline categories

    lambda = (scale_lambda %*% matrix (1, rows = 1, cols = K)) * regularization;
    delta = 0.5 * sqrt (D) / max (sqrt (rowSums_X_sq));

    B = matrix (0, rows = D, cols = K);     ### LT = X %*% (SHIFT/SCALE TRANSFORM) %*% B;
                                            ### LT = cbind (LT, matrix (0, rows = N, cols = 1));
                                            ### LT = LT - rowMaxs (LT) %*% matrix (1, rows = 1, cols = K+1);
    P = matrix (1, rows = N, cols = K+1);   ### exp_LT = exp (LT);
    P = P / (K + 1);                        ### P =  exp_LT / (rowSums (exp_LT) %*% matrix (1, rows = 1, cols = K+1));
    obj = N * log (K + 1);                  ### obj = - sum (Y * LT) + sum (log (rowSums (exp_LT))) + 0.5 * sum (lambda * (B_new ^ 2));

    Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
    if (intercept_status == 2) {
        Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
    }
    Grad = Grad + lambda * B;
    norm_Grad = sqrt (sum (Grad ^ 2));
    norm_Grad_initial = norm_Grad;

    if (maxinneriter == 0) {
        maxinneriter = D * K;
    }
    iter = 1;

    # boolean for convergence check
    converge = (norm_Grad < tol) | (iter > maxiter);

    print ("-- Initially:  Objective = " + obj + ",  Gradient Norm = " + norm_Grad + ",  Trust Delta = " + delta);

    if (fileLog != " ") {
        log_str = "OBJECTIVE,0," + obj;
        log_str = append (log_str, "GRADIENT_NORM,0," + norm_Grad);
        log_str = append (log_str, "TRUST_DELTA,0," + delta);
    } else {
        log_str = " ";
    }

    while (! converge)
    {
        # SOLVE TRUST REGION SUB-PROBLEM
        S = matrix (0, rows = D, cols = K);
        R = - Grad;
        V = R;
        delta2 = delta ^ 2;
        inneriter = 1;
        norm_R2 = sum (R ^ 2);
        innerconverge = (sqrt (norm_R2) <= psi * norm_Grad);
        is_trust_boundary_reached = 0;

        while (! innerconverge)
        {
            if (intercept_status == 2) {
                ssX_V = diag (scale_X) %*% V;
                ssX_V [D, ] = ssX_V [D, ] + t(shift_X) %*% V;
            } else {
                ssX_V = V;
            }
            Q = P [, 1:K] * (X %*% ssX_V);
            HV = t(X) %*% (Q - P [, 1:K] * (rowSums (Q) %*% matrix (1, rows = 1, cols = K)));
            if (intercept_status == 2) {
                HV = diag (scale_X) %*% HV + shift_X %*% HV [D, ];
            }
            HV = HV + lambda * V;
            alpha = norm_R2 / sum (V * HV);
            Snew = S + alpha * V;
            norm_Snew2 = sum (Snew ^ 2);
            if (norm_Snew2 <= delta2)
            {
                S = Snew;
                R = R - alpha * HV;
                old_norm_R2 = norm_R2
                norm_R2 = sum (R ^ 2);
                V = R + (norm_R2 / old_norm_R2) * V;
                innerconverge = (sqrt (norm_R2) <= psi * norm_Grad);
            } else {
                is_trust_boundary_reached = 1;
                sv = sum (S * V);
                v2 = sum (V ^ 2);
                s2 = sum (S ^ 2);
                rad = sqrt (sv ^ 2 + v2 * (delta2 - s2));
                if (sv >= 0) {
                    alpha = (delta2 - s2) / (sv + rad);
                } else {
                    alpha = (rad - sv) / v2;
                }
                S = S + alpha * V;
                R = R - alpha * HV;
                innerconverge = TRUE;
            }
            inneriter = inneriter + 1;
            innerconverge = innerconverge | (inneriter > maxinneriter);
        }

        # END TRUST REGION SUB-PROBLEM

        # compute rho, update B, obtain delta
        gs = sum (S * Grad);
        qk = - 0.5 * (gs - sum (S * R));
        B_new = B + S;
        if (intercept_status == 2) {
            ssX_B_new = diag (scale_X) %*% B_new;
            ssX_B_new [D, ] = ssX_B_new [D, ] + t(shift_X) %*% B_new;
        } else {
            ssX_B_new = B_new;
        }

        LT = cbind ((X %*% ssX_B_new), matrix (0, rows = N, cols = 1));
        if (fileLog != " ") {
            log_str = append (log_str, "LINEAR_TERM_MIN,"  + iter + "," + min (LT));
            log_str = append (log_str, "LINEAR_TERM_MAX,"  + iter + "," + max (LT));
        }
        LT = LT - rowMaxs (LT) %*% matrix (1, rows = 1, cols = K+1);
        exp_LT = exp (LT);
        P_new  = exp_LT / (rowSums (exp_LT) %*% matrix (1, rows = 1, cols = K+1));
        obj_new = - sum (Y * LT) + sum (log (rowSums (exp_LT))) + 0.5 * sum (lambda * (B_new ^ 2));

        # Consider updating LT in the inner loop
        # Consider the big "obj" and "obj_new" rounding-off their small difference below:

        actred = (obj - obj_new);

        rho = actred / qk;
        is_rho_accepted = (rho > eta0);
        snorm = sqrt (sum (S ^ 2));

        if (fileLog != " ") {
            log_str = append (log_str, "NUM_CG_ITERS,"     + iter + "," + (inneriter - 1));
            log_str = append (log_str, "IS_TRUST_REACHED," + iter + "," + is_trust_boundary_reached);
            log_str = append (log_str, "POINT_STEP_NORM,"  + iter + "," + snorm);
            log_str = append (log_str, "OBJECTIVE,"        + iter + "," + obj_new);
            log_str = append (log_str, "OBJ_DROP_REAL,"    + iter + "," + actred);
            log_str = append (log_str, "OBJ_DROP_PRED,"    + iter + "," + qk);
            log_str = append (log_str, "OBJ_DROP_RATIO,"   + iter + "," + rho);
        }

    	if (iter == 1) {
    	   delta = min (delta, snorm);
    	}

    	alpha2 = obj_new - obj - gs;
    	if (alpha2 <= 0) {
    	   alpha = sigma3;
    	}
    	else {
    	   alpha = max (sigma1, -0.5 * gs / alpha2);
    	}

    	if (rho < eta0) {
    		delta = min (max (alpha, sigma1) * snorm, sigma2 * delta);
    	}
    	else {
    		if (rho < eta1) {
    			delta = max (sigma1 * delta, min (alpha * snorm, sigma2 * delta));
    		}
    		else {
    			if (rho < eta2) {
    				delta = max (sigma1 * delta, min (alpha * snorm, sigma3 * delta));
    			}
    			else {
    				delta = max (delta, min (alpha * snorm, sigma3 * delta));
    			}
    		}
    	}

    	if (is_trust_boundary_reached == 1)
    	{
    	    print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations, trust bound REACHED");
    	} else {
    	    print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations");
    	}
    	print ("   -- Obj.Reduction:  Actual = " + actred + ",  Predicted = " + qk +
    	       "  (A/P: " + (round (10000.0 * rho) / 10000.0) + "),  Trust Delta = " + delta);

    	if (is_rho_accepted)
    	{
    		B = B_new;
    		P = P_new;
    		Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
    		if (intercept_status == 2) {
    		    Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
    		}
    		Grad = Grad + lambda * B;
    		norm_Grad = sqrt (sum (Grad ^ 2));
    		obj = obj_new;
    	    print ("   -- New Objective = " + obj + ",  Beta Change Norm = " + snorm + ",  Gradient Norm = " + norm_Grad);
        }
        if (iter == 1) {
           delta = min (delta, snorm);
        }

        alpha2 = obj_new - obj - gs;
        if (alpha2 <= 0) {
           alpha = sigma3;
        }
        else {
           alpha = max (sigma1, -0.5 * gs / alpha2);
        }

        if (rho < eta0) {
            delta = min (max (alpha, sigma1) * snorm, sigma2 * delta);
        }
        else {
            if (rho < eta1) {
                delta = max (sigma1 * delta, min (alpha * snorm, sigma2 * delta));
            }
            else {
                if (rho < eta2) {
                    delta = max (sigma1 * delta, min (alpha * snorm, sigma3 * delta));
                }
                else {
                    delta = max (delta, min (alpha * snorm, sigma3 * delta));
                }
            }
        }

        if (is_trust_boundary_reached == 1)
        {
            print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations, trust bound REACHED");
        } else {
            print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations");
        }
        print ("   -- Obj.Reduction:  Actual = " + actred + ",  Predicted = " + qk +
               "  (A/P: " + (round (10000.0 * rho) / 10000.0) + "),  Trust Delta = " + delta);

        if (is_rho_accepted)
        {
            B = B_new;
            P = P_new;
            Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
            if (intercept_status == 2) {
                Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
            }
            Grad = Grad + lambda * B;
            norm_Grad = sqrt (sum (Grad ^ 2));
            obj = obj_new;
            print ("   -- New Objective = " + obj + ",  Beta Change Norm = " + snorm + ",  Gradient Norm = " + norm_Grad);
            if (fileLog != " ") {
                log_str = append (log_str, "IS_POINT_UPDATED," + iter + ",1");
                log_str = append (log_str, "GRADIENT_NORM,"    + iter + "," + norm_Grad);
            }
        } else {
            if (fileLog != " ") {
                log_str = append (log_str, "IS_POINT_UPDATED," + iter + ",0");
            }
        }

        if (fileLog != " ") {
            log_str = append (log_str, "TRUST_DELTA," + iter + "," + delta);
        }

        iter = iter + 1;
        converge = ((norm_Grad < (tol * norm_Grad_initial)) | (iter > maxiter) |
            ((is_trust_boundary_reached == 0) & (abs (actred) < (abs (obj) + abs (obj_new)) * 0.00000000000001)));
        if (converge) { print ("Termination / Convergence condition satisfied."); } else { print (" "); }
    }

    if (intercept_status == 2) {
        B_out = diag (scale_X) %*% B;
        B_out [D, ] = B_out [D, ] + t(shift_X) %*% B;
    } else {
        B_out = B;
    }
    #write (B_out, fileB, format=fmtB);

    if (sum_x > 0.0) {
        print (as.scalar(B[1,1]))
    }
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        stop = time(1)
    }
    times[ix,1] = stop-start
}

if (fileLog != " ") {
    write (log_str, fileLog);
}

times = t(times)

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 64G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg inputPath=/scratch/adclick_clean_1_sparse.parquet nodes=4 featureNames=features stub=_1

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SQLContext
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.runtime.instructions.spark.utils._
import org.apache.sysml.runtime.matrix.MatrixCharacteristics
import org.apache.spark.storage.StorageLevel._
import scala.tools.nsc.io._
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.sql.DataFrame
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms {
    def main(args: Array[String]) {
        val spark = SparkSession.builder.getOrCreate()
        val sc = spark.sparkContext
        val ml = new MLContext(sc)

        val root = sys.env("BENCHMARK_PROJECT_ROOT")
        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val inputPath = argMap("inputPath")
        val nodes = argMap("nodes")
        val featureNames = argMap("featureNames")
        val isSparse = inputPath.contains("sparse")
        val execSpark = argMap.get("execSpark")

        val execution_type = execSpark match {
            case Some(arg) => MLContext.ExecutionType.SPARK
            case None => MLContext.ExecutionType.DRIVER_AND_SPARK
        }

        val exec_type_stub = execSpark match {
            case Some(arg) => "spark"
            case None      => "spark_and_driver"
        }

        println(execution_type)
        ml.setExecutionType(execution_type)
        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val sparse_stub = if (isSparse) "_sparse" else "_dense"
        var stub = "/tests/MLAlgorithms (Native Implementations)/output/"
        val base = s"systemml_${opType}${nodes}${sparse_stub}_${exec_type_stub}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val script_stub = "tests/MLAlgorithms (Native Implementations)/src"
        val script_path = s"${root}/${script_stub}/systemml/src/main/dml"
        val input_df = spark.read.parquet(inputPath)
        val x = input_df.select(featureNames).repartition(1000)
        val y = input_df.select("y")
        
        println( x.count )
        var times = Array.ofDim[Double](5,1)
        if (opType == "logit") {
            val script = dmlFromFile(s"${script_path}/MultiLogitReg.dml").
                 in(Map("X"     -> x,
                        "Y_vec" -> y,
                        "$moi"  -> 3,
                        "$mii"  -> 0)).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else if (opType == "reg") {
             val script = dmlFromFile(s"${script_path}/LinearRegCG.dml").
                 in("X", x).
                 in("$maxi", 3).
                 in("y", y).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else if (opType == "pca") {
             val script = dmlFromFile(s"${script_path}/PCA.dml").
                 in("A", x).
                 in("K", 5).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else {
             throw new Exception("Invalid operator")
        }

        File(path).appendAll(
            nodes + "," + times(0).map(x => x/1000.0).mkString(",") + '\n')
    }
}
./systemml/src/main/dml/PCA.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
# 
#   http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# 
# This script performs Principal Component Analysis (PCA) on the given input data.
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME   TYPE   DEFAULT  MEANING
# ---------------------------------------------------------------------------------------------
# INPUT  String ---      Location to read the matrix A of feature vectors
# K      Int    ---      Indicates dimension of the new vector space constructed from eigen vectors
# CENTER Int    0        Indicates whether or not to center data 
# SCALE  Int    0        Indicates whether or not to scale data 
# OFMT   String ---      Output data format
# PROJDATA Int  0        This argument indicates if the data should be projected or not
# MODEL  String ---      Location to already existing model: eigenvectors and eigenvalues 
# OUTPUT String /        Location to write output matrices (covariance matrix, new basis vectors, 
#                           and data projected onto new basis vectors)
# hadoop jar SystemML.jar -f PCA.dml -nvargs INPUT=INPUT_DIR/pca-1000x1000 
# OUTPUT=OUTPUT_DIR/pca-1000x1000-model PROJDATA=1 CENTER=1 SCALE=1
# ---------------------------------------------------------------------------------------------

#A = read($INPUT);

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

sum_A = sum( A );
times = matrix(0, rows = 5, cols = 1)
for (ix in 1:5) {
    if ( sum_A > 0.0 ) {
        start = time(1)
    }

    K = ifdef($K, ncol(A));
    ofmt = ifdef($OFMT, "CSV");
    projectData = ifdef($PROJDATA,1);
    model = ifdef($MODEL,"");
    center = ifdef($CENTER,0);
    scale = ifdef($SCALE,0);
    output = ifdef($OUTPUT,"/");

    evec_dominant = matrix(0,cols=1,rows=1);

    if (model != "") {
        pass = 1.0
        # reuse existing model to project data
        #evec_dominant = read(model+"/dominant.eigen.vectors");
    } else {
        if (model == "" ){
            model = output; 
        }   

        N = nrow(A);
        D = ncol(A);

        # perform z-scoring (centering and scaling)
        if (center == 1) {
            cm = colMeans(A);
            A = A - cm;
        }
        if (scale == 1) {
            cvars = (colSums (A^2));    
            if (center == 1){
            cm = colMeans(A);
                cvars = (cvars - N*(cm^2))/(N-1);           
            }
            Azscored = (A)/sqrt(cvars);
                A = Azscored;
        }   

        # co-variance matrix 
        mu = colSums(A)/N;
        C = (t(A) %*% A)/(N-1) - (N/(N-1))*t(mu) %*% mu;


        # compute eigen vectors and values
        [evalues, evectors] = eigen(C);

        decreasing_Idx = order(target=evalues,by=1,decreasing=TRUE,index.return=TRUE);
        diagmat = table(seq(1,D),decreasing_Idx);
        # sorts eigenvalues by decreasing order
        evalues = diagmat %*% evalues;
        # sorts eigenvectors column-wise in the order of decreasing eigenvalues
        evectors = evectors %*% diagmat;


        # select K dominant eigen vectors 
        nvec = ncol(evectors);

        eval_dominant = evalues[1:K, 1];
        evec_dominant = evectors[,1:K];
        
        # the square root of eigenvalues
        eval_stdev_dominant = sqrt(eval_dominant);
        
        #write(eval_stdev_dominant, model+"/dominant.eigen.standard.deviations", format=ofmt);
        #write(eval_dominant, model+"/dominant.eigen.values", format=ofmt);
        #write(evec_dominant, model+"/dominant.eigen.vectors", format=ofmt);
    }
    if (projectData == 1 | model != ""){
        # Construct new data set by treating computed dominant eigenvectors as the basis vectors
        newA = A %*% evec_dominant;
        sum_newA = sum( newA )
        if (sum_newA > 0) {
            print(sum_newA)
        }
        #write(newA, output+"/projected.data", format=ofmt);
    }

    if (sum_A > 0.0) {
        stop = time(1)
    }
    times[ix,1] = (stop - start)
}

times = t(times)
./systemml/src/main/dml/LinearRegCG.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

#
# THIS SCRIPT SOLVES LINEAR REGRESSION USING THE CONJUGATE GRADIENT ALGORITHM
#
# INPUT PARAMETERS:
# --------------------------------------------------------------------------------------------
# NAME  TYPE   DEFAULT  MEANING
# --------------------------------------------------------------------------------------------
# X     String  ---     Location (on HDFS) to read the matrix X of feature vectors
# Y     String  ---     Location (on HDFS) to read the 1-column matrix Y of response values
# B     String  ---     Location to store estimated regression parameters (the betas)
# O     String  " "     Location to write the printed statistics; by default is standard output
# Log   String  " "     Location to write per-iteration variables for log/debugging purposes
# icpt  Int      0      Intercept presence, shifting and rescaling the columns of X:
#                       0 = no intercept, no shifting, no rescaling;
#                       1 = add intercept, but neither shift nor rescale X;
#                       2 = add intercept, shift & rescale X columns to mean = 0, variance = 1
# reg   Double 0.000001 Regularization constant (lambda) for L2-regularization; set to nonzero
#                       for highly dependend/sparse/numerous features
# tol   Double 0.000001 Tolerance (epsilon); conjugate graduent procedure terminates early if
#                       L2 norm of the beta-residual is less than tolerance * its initial norm
# maxi  Int      0      Maximum number of conjugate gradient iterations, 0 = no maximum
# fmt   String "text"   Matrix output format for B (the betas) only, usually "text" or "csv"
# --------------------------------------------------------------------------------------------
# OUTPUT: Matrix of regression parameters (the betas) and its size depend on icpt input value:
#         OUTPUT SIZE:   OUTPUT CONTENTS:                HOW TO PREDICT Y FROM X AND B:
# icpt=0: ncol(X)   x 1  Betas for X only                Y ~ X %*% B[1:ncol(X), 1], or just X %*% B
# icpt=1: ncol(X)+1 x 1  Betas for X and intercept       Y ~ X %*% B[1:ncol(X), 1] + B[ncol(X)+1, 1]
# icpt=2: ncol(X)+1 x 2  Col.1: betas for X & intercept  Y ~ X %*% B[1:ncol(X), 1] + B[ncol(X)+1, 1]
#                        Col.2: betas for shifted/rescaled X and intercept
#
# In addition, some regression statistics are provided in CSV format, one comma-separated
# name-value pair per each line, as follows:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------
# AVG_TOT_Y             Average of the response value Y
# STDEV_TOT_Y           Standard Deviation of the response value Y
# AVG_RES_Y             Average of the residual Y - pred(Y|X), i.e. residual bias
# STDEV_RES_Y           Standard Deviation of the residual Y - pred(Y|X)
# DISPERSION            GLM-style dispersion, i.e. residual sum of squares / # deg. fr.
# R2                    R^2 of residual with bias included vs. total average
# ADJUSTED_R2           Adjusted R^2 of residual with bias included vs. total average
# R2_NOBIAS             R^2 of residual with bias subtracted vs. total average
# ADJUSTED_R2_NOBIAS    Adjusted R^2 of residual with bias subtracted vs. total average
# R2_VS_0               * R^2 of residual with bias included vs. zero constant
# ADJUSTED_R2_VS_0      * Adjusted R^2 of residual with bias included vs. zero constant
# -------------------------------------------------------------------------------------
# * The last two statistics are only printed if there is no intercept (icpt=0)
#
# The Log file, when requested, contains the following per-iteration variables in CSV
# format, each line containing triple (NAME, ITERATION, VALUE) with ITERATION = 0 for
# initial values:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------
# CG_RESIDUAL_NORM      L2-norm of Conj.Grad.residual, which is A %*% beta - t(X) %*% y
#                           where A = t(X) %*% X + diag (lambda), or a similar quantity
# CG_RESIDUAL_RATIO     Ratio of current L2-norm of Conj.Grad.residual over the initial
# -------------------------------------------------------------------------------------
#
# HOW TO INVOKE THIS SCRIPT - EXAMPLE:
# hadoop jar SystemML.jar -f LinearRegCG.dml -nvargs X=INPUT_DIR/X Y=INPUT_DIR/Y B=OUTPUT_DIR/B
#     O=OUTPUT_DIR/Out icpt=2 reg=1.0 tol=0.001 maxi=100 fmt=csv Log=OUTPUT_DIR/log

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

#fileX = $X;
#fileY = $Y;
#fileB = $B;
#fileO = ifdef ($O, " ");
#fileLog = ifdef ($Log, " ");
#fmtB = ifdef ($fmt, "text");

intercept_status = ifdef ($icpt, 0);     # $icpt=0;
tolerance = ifdef ($tol, 0.000001);      # $tol=0.000001;
max_iteration = ifdef ($maxi, 0);        # $maxi=0;
regularization = ifdef ($reg, 0.000001); # $reg=0.000001;

print ("BEGIN LINEAR REGRESSION SCRIPT");
print ("Reading X and Y...");

sum_x = sum( X )
sum_y = sum( y )
times = matrix(0, rows = 5, cols = 1)
    for (ix in 1:5) {
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        start = time(1)
    }

    n = nrow (X);
    m = ncol (X);
    ones_n = matrix (1, rows = n, cols = 1);
    zero_cell = matrix (0, rows = 1, cols = 1);

    # Introduce the intercept, shift and rescale the columns of X if needed

    m_ext = m;
    if (intercept_status == 1 | intercept_status == 2)  # add the intercept column
    {
        X = cbind (X, ones_n);
        m_ext = ncol (X);
    }

    scale_lambda = matrix (1, rows = m_ext, cols = 1);
    if (intercept_status == 1 | intercept_status == 2)
    {
        scale_lambda [m_ext, 1] = 0;
    }

    if (intercept_status == 2)  # scale-&-shift X columns to mean 0, variance 1
    {                           # Important assumption: X [, m_ext] = ones_n
        avg_X_cols = t(colSums(X)) / n;
        var_X_cols = (t(colSums (X ^ 2)) - n * (avg_X_cols ^ 2)) / (n - 1);
        is_unsafe = (var_X_cols <= 0);
        scale_X = 1.0 / sqrt (var_X_cols * (1 - is_unsafe) + is_unsafe);
        scale_X [m_ext, 1] = 1;
        shift_X = - avg_X_cols * scale_X;
        shift_X [m_ext, 1] = 0;
    } else {
        scale_X = matrix (1, rows = m_ext, cols = 1);
        shift_X = matrix (0, rows = m_ext, cols = 1);
    }

    # Henceforth, if intercept_status == 2, we use "X %*% (SHIFT/SCALE TRANSFORM)"
    # instead of "X".  However, in order to preserve the sparsity of X,
    # we apply the transform associatively to some other part of the expression
    # in which it occurs.  To avoid materializing a large matrix, we rewrite it:
    #
    # ssX_A  = (SHIFT/SCALE TRANSFORM) %*% A    --- is rewritten as:
    # ssX_A  = diag (scale_X) %*% A;
    # ssX_A [m_ext, ] = ssX_A [m_ext, ] + t(shift_X) %*% A;
    #
    # tssX_A = t(SHIFT/SCALE TRANSFORM) %*% A   --- is rewritten as:
    # tssX_A = diag (scale_X) %*% A + shift_X %*% A [m_ext, ];

    lambda = scale_lambda * regularization;
    beta_unscaled = matrix (0, rows = m_ext, cols = 1);

    if (max_iteration == 0) {
        max_iteration = m_ext;
    }
    i = 0;

    # BEGIN THE CONJUGATE GRADIENT ALGORITHM
    print ("Running the CG algorithm...");

    r = - t(X) %*% y;

    if (intercept_status == 2) {
        r = scale_X * r + shift_X %*% r [m_ext, ];
    }

    p = - r;
    norm_r2 = sum (r ^ 2);
    norm_r2_initial = norm_r2;
    norm_r2_target = norm_r2_initial * tolerance ^ 2;
    print ("||r|| initial value = " + sqrt (norm_r2_initial) + ",  target value = " + sqrt (norm_r2_target));
    log_str = "CG_RESIDUAL_NORM,0," + sqrt (norm_r2_initial);
    log_str = append (log_str, "CG_RESIDUAL_RATIO,0,1.0");

    while (i < max_iteration & norm_r2 > norm_r2_target)
    {
        if (intercept_status == 2) {
            ssX_p = scale_X * p;
            ssX_p [m_ext, ] = ssX_p [m_ext, ] + t(shift_X) %*% p;
        } else {
            ssX_p = p;
        }

        q = t(X) %*% (X %*% ssX_p);

        if (intercept_status == 2) {
            q = scale_X * q + shift_X %*% q [m_ext, ];
        }

    	q = q + lambda * p;
    	a = norm_r2 / sum (p * q);
    	beta_unscaled = beta_unscaled + a * p;
    	r = r + a * q;
    	old_norm_r2 = norm_r2;
    	norm_r2 = sum (r ^ 2);
    	p = -r + (norm_r2 / old_norm_r2) * p;
    	i = i + 1;
    	print ("Iteration " + i + ":  ||r|| / ||r init|| = " + sqrt (norm_r2 / norm_r2_initial));
    	log_str = append (log_str, "CG_RESIDUAL_NORM,"  + i + "," + sqrt (norm_r2));
        log_str = append (log_str, "CG_RESIDUAL_RATIO," + i + "," + sqrt (norm_r2 / norm_r2_initial));
    }

    if (i >= max_iteration) {
        print ("Warning: the maximum number of iterations has been reached.");
    }
    print ("The CG algorithm is done.");
    # END THE CONJUGATE GRADIENT ALGORITHM

    if (intercept_status == 2) {
        beta = scale_X * beta_unscaled;
        beta [m_ext, ] = beta [m_ext, ] + t(shift_X) %*% beta_unscaled;
    } else {
        beta = beta_unscaled;
    }

    print ("Computing the statistics...");

    avg_tot = sum (y) / n;
    ss_tot = sum (y ^ 2);
    ss_avg_tot = ss_tot - n * avg_tot ^ 2;
    var_tot = ss_avg_tot / (n - 1);
    y_residual = y - X %*% beta;
    avg_res = sum (y_residual) / n;
    ss_res = sum (y_residual ^ 2);
    ss_avg_res = ss_res - n * avg_res ^ 2;

    R2 = 1 - ss_res / ss_avg_tot;
    if (n > m_ext) {
        dispersion  = ss_res / (n - m_ext);
        adjusted_R2 = 1 - dispersion / (ss_avg_tot / (n - 1));
    } else {
        dispersion  = 0.0 / 0.0;
        adjusted_R2 = 0.0 / 0.0;
    }

    R2_nobias = 1 - ss_avg_res / ss_avg_tot;
    deg_freedom = n - m - 1;
    if (deg_freedom > 0) {
        var_res = ss_avg_res / deg_freedom;
        adjusted_R2_nobias = 1 - var_res / (ss_avg_tot / (n - 1));
    } else {
        var_res = 0.0 / 0.0;
        adjusted_R2_nobias = 0.0 / 0.0;
        print ("Warning: zero or negative number of degrees of freedom.");
    }

    R2_vs_0 = 1 - ss_res / ss_tot;
    if (n > m) {
        adjusted_R2_vs_0 = 1 - (ss_res / (n - m)) / (ss_tot / n);
    } else {
        adjusted_R2_vs_0 = 0.0 / 0.0;
    }

    str = "AVG_TOT_Y," + avg_tot;                                    #  Average of the response value Y
    str = append (str, "STDEV_TOT_Y," + sqrt (var_tot));             #  Standard Deviation of the response value Y
    str = append (str, "AVG_RES_Y," + avg_res);                      #  Average of the residual Y - pred(Y|X), i.e. residual bias
    str = append (str, "STDEV_RES_Y," + sqrt (var_res));             #  Standard Deviation of the residual Y - pred(Y|X)
    str = append (str, "DISPERSION," + dispersion);                  #  GLM-style dispersion, i.e. residual sum of squares / # d.f.
    str = append (str, "R2," + R2);                                  #  R^2 of residual with bias included vs. total average
    str = append (str, "ADJUSTED_R2," + adjusted_R2);                #  Adjusted R^2 of residual with bias included vs. total average
    str = append (str, "R2_NOBIAS," + R2_nobias);                    #  R^2 of residual with bias subtracted vs. total average
    str = append (str, "ADJUSTED_R2_NOBIAS," + adjusted_R2_nobias);  #  Adjusted R^2 of residual with bias subtracted vs. total average
    if (intercept_status == 0) {
        str = append (str, "R2_VS_0," + R2_vs_0);                    #  R^2 of residual with bias included vs. zero constant
        str = append (str, "ADJUSTED_R2_VS_0," + adjusted_R2_vs_0);  #  Adjusted R^2 of residual with bias included vs. zero constant
    }

    print (str);

    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        stop = time(1)
    }
    times[ix,1] = stop - start
}

times = t(times)
./systemml/src/main/dml/MultiLogitReg.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# Solves Multinomial Logistic Regression using Trust Region methods.
# (See: Trust Region Newton Method for Logistic Regression, Lin, Weng and Keerthi, JMLR 9 (2008) 627-650)

# INPUT PARAMETERS:
# --------------------------------------------------------------------------------------------
# NAME  TYPE   DEFAULT  MEANING
# --------------------------------------------------------------------------------------------
# X     String  ---     Location to read the matrix of feature vectors
# Y     String  ---     Location to read the matrix with category labels
# B     String  ---     Location to store estimated regression parameters (the betas)
# Log   String  " "     Location to write per-iteration variables for log/debugging purposes
# icpt  Int      0      Intercept presence, shifting and rescaling X columns:
#                       0 = no intercept, no shifting, no rescaling;
#                       1 = add intercept, but neither shift nor rescale X;
#                       2 = add intercept, shift & rescale X columns to mean = 0, variance = 1
# reg   Double  0.0     regularization parameter (lambda = 1/C); intercept is not regularized
# tol   Double 0.000001 tolerance ("epsilon")
# moi   Int     100     max. number of outer (Newton) iterations
# mii   Int      0      max. number of inner (conjugate gradient) iterations, 0 = no max
# fmt   String "text"   Matrix output format, usually "text" or "csv" (for matrices only)
# --------------------------------------------------------------------------------------------
# The largest label represents the baseline category; if label -1 or 0 is present, then it is
# the baseline label (and it is converted to the largest label).
#
# The Log file, when requested, contains the following per-iteration variables in CSV format,
# each line containing triple (NAME, ITERATION, VALUE) with ITERATION = 0 for initial values:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------------
# LINEAR_TERM_MIN       The minimum value of X %*% B, used to check for overflows
# LINEAR_TERM_MAX       The maximum value of X %*% B, used to check for overflows
# NUM_CG_ITERS          Number of inner (Conj.Gradient) iterations in this outer iteration
# IS_TRUST_REACHED      1 = trust region boundary was reached, 0 = otherwise
# POINT_STEP_NORM       L2-norm of iteration step from old point (i.e. matrix B) to new point
# OBJECTIVE             The loss function we minimize (negative regularized log-likelihood)
# OBJ_DROP_REAL         Reduction in the objective during this iteration, actual value
# OBJ_DROP_PRED         Reduction in the objective predicted by a quadratic approximation
# OBJ_DROP_RATIO        Actual-to-predicted reduction ratio, used to update the trust region
# IS_POINT_UPDATED      1 = new point accepted; 0 = new point rejected, old point restored
# GRADIENT_NORM         L2-norm of the loss function gradient (omitted if point is rejected)
# TRUST_DELTA           Updated trust region size, the "delta"
# -------------------------------------------------------------------------------------------
#
# Script invocation example:
# hadoop jar SystemML.jar -f MultiLogReg.dml -nvargs icpt=2 reg=1.0 tol=0.000001 moi=100 mii=20
#     X=INPUT_DIR/X123 Y=INPUT_DIR/Y123 B=OUTPUT_DIR/B123 fmt=csv Log=OUTPUT_DIR/log

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

#fileX = $X;
#fileY = $Y;
#fileB = $B;
fileLog = ifdef ($Log, " ");
fmtB = ifdef ($fmt, "text");

intercept_status = ifdef ($icpt, 0); # $icpt = 0;
regularization = ifdef ($reg, 0.0);  # $reg  = 0.0;
tol = ifdef ($tol, 0.000001);        # $tol  = 0.000001;
maxiter = ifdef ($moi, 100);         # $moi  = 100;
maxinneriter = ifdef ($mii, 0);      # $mii  = 0;
tol = as.double (tol);

print ("BEGIN MULTINOMIAL LOGISTIC REGRESSION SCRIPT");
print ("Reading X...");
#X = read (fileX);
print ("Reading Y...");
#Y_vec = read (fileY);

# force a pass over the data

sum_x = sum( X )
sum_y = sum( Y_vec )
times = matrix(0, rows=5, cols=1)

for (ix in 1:5) {
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        start = time(1)
    }

    eta0 = 0.0001;
    eta1 = 0.25;
    eta2 = 0.75;
    sigma1 = 0.25;
    sigma2 = 0.5;
    sigma3 = 4.0;
    psi = 0.1;

    N = nrow (X);
    D = ncol (X);

    # Introduce the intercept, shift and rescale the columns of X if needed
    if (intercept_status == 1 | intercept_status == 2)  # add the intercept column
    {
        X = cbind (X, matrix (1, rows = N, cols = 1));
        D = ncol (X);
    }

    scale_lambda = matrix (1, rows = D, cols = 1);
    if (intercept_status == 1 | intercept_status == 2)
    {
        scale_lambda [D, 1] = 0;
    }

    if (intercept_status == 2)  # scale-&-shift X columns to mean 0, variance 1
    {                           # Important assumption: X [, D] = matrix (1, rows = N, cols = 1)
        avg_X_cols = t(colSums(X)) / N;
        var_X_cols = (t(colSums (X ^ 2)) - N * (avg_X_cols ^ 2)) / (N - 1);
        is_unsafe = var_X_cols <= 0;
        scale_X = 1.0 / sqrt (var_X_cols * (1 - is_unsafe) + is_unsafe);
        scale_X [D, 1] = 1;
        shift_X = - avg_X_cols * scale_X;
        shift_X [D, 1] = 0;
        rowSums_X_sq = (X ^ 2) %*% (scale_X ^ 2) + X %*% (2 * scale_X * shift_X) + sum (shift_X ^ 2);
    } else {
        scale_X = matrix (1, rows = D, cols = 1);
        shift_X = matrix (0, rows = D, cols = 1);
        rowSums_X_sq = rowSums (X ^ 2);
    }

    # Henceforth we replace "X" with "X %*% (SHIFT/SCALE TRANSFORM)" and rowSums(X ^ 2)
    # with "rowSums_X_sq" in order to preserve the sparsity of X under shift and scale.
    # The transform is then associatively applied to the other side of the expression,
    # and is rewritten via "scale_X" and "shift_X" as follows:
    #
    # ssX_A  = (SHIFT/SCALE TRANSFORM) %*% A    --- is rewritten as:
    # ssX_A  = diag (scale_X) %*% A;
    # ssX_A [D, ] = ssX_A [D, ] + t(shift_X) %*% A;
    #
    # tssX_A = t(SHIFT/SCALE TRANSFORM) %*% A   --- is rewritten as:
    # tssX_A = diag (scale_X) %*% A + shift_X %*% A [D, ];

    # Convert "Y_vec" into indicator matrix:
    max_y = max (Y_vec);
    if (min (Y_vec) <= 0) {
        # Category labels "0", "-1" etc. are converted into the largest label
        Y_vec  = Y_vec  + (- Y_vec  + max_y + 1) * (Y_vec <= 0);
        max_y = max_y + 1;
    }
    Y = table (seq (1, N, 1), Y_vec, N, max_y);
    K = ncol (Y) - 1;   # The number of  non-baseline categories

    lambda = (scale_lambda %*% matrix (1, rows = 1, cols = K)) * regularization;
    delta = 0.5 * sqrt (D) / max (sqrt (rowSums_X_sq));

    B = matrix (0, rows = D, cols = K);     ### LT = X %*% (SHIFT/SCALE TRANSFORM) %*% B;
                                            ### LT = cbind (LT, matrix (0, rows = N, cols = 1));
                                            ### LT = LT - rowMaxs (LT) %*% matrix (1, rows = 1, cols = K+1);
    P = matrix (1, rows = N, cols = K+1);   ### exp_LT = exp (LT);
    P = P / (K + 1);                        ### P =  exp_LT / (rowSums (exp_LT) %*% matrix (1, rows = 1, cols = K+1));
    obj = N * log (K + 1);                  ### obj = - sum (Y * LT) + sum (log (rowSums (exp_LT))) + 0.5 * sum (lambda * (B_new ^ 2));

    Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
    if (intercept_status == 2) {
        Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
    }
    Grad = Grad + lambda * B;
    norm_Grad = sqrt (sum (Grad ^ 2));
    norm_Grad_initial = norm_Grad;

    if (maxinneriter == 0) {
        maxinneriter = D * K;
    }
    iter = 1;

    # boolean for convergence check
    converge = (norm_Grad < tol) | (iter > maxiter);

    print ("-- Initially:  Objective = " + obj + ",  Gradient Norm = " + norm_Grad + ",  Trust Delta = " + delta);

    if (fileLog != " ") {
        log_str = "OBJECTIVE,0," + obj;
        log_str = append (log_str, "GRADIENT_NORM,0," + norm_Grad);
        log_str = append (log_str, "TRUST_DELTA,0," + delta);
    } else {
        log_str = " ";
    }

    while (! converge)
    {
        # SOLVE TRUST REGION SUB-PROBLEM
        S = matrix (0, rows = D, cols = K);
        R = - Grad;
        V = R;
        delta2 = delta ^ 2;
        inneriter = 1;
        norm_R2 = sum (R ^ 2);
        innerconverge = (sqrt (norm_R2) <= psi * norm_Grad);
        is_trust_boundary_reached = 0;

        while (! innerconverge)
        {
            if (intercept_status == 2) {
                ssX_V = diag (scale_X) %*% V;
                ssX_V [D, ] = ssX_V [D, ] + t(shift_X) %*% V;
            } else {
                ssX_V = V;
            }
            Q = P [, 1:K] * (X %*% ssX_V);
            HV = t(X) %*% (Q - P [, 1:K] * (rowSums (Q) %*% matrix (1, rows = 1, cols = K)));
            if (intercept_status == 2) {
                HV = diag (scale_X) %*% HV + shift_X %*% HV [D, ];
            }
            HV = HV + lambda * V;
            alpha = norm_R2 / sum (V * HV);
            Snew = S + alpha * V;
            norm_Snew2 = sum (Snew ^ 2);
            if (norm_Snew2 <= delta2)
            {
                S = Snew;
                R = R - alpha * HV;
                old_norm_R2 = norm_R2
                norm_R2 = sum (R ^ 2);
                V = R + (norm_R2 / old_norm_R2) * V;
                innerconverge = (sqrt (norm_R2) <= psi * norm_Grad);
            } else {
                is_trust_boundary_reached = 1;
                sv = sum (S * V);
                v2 = sum (V ^ 2);
                s2 = sum (S ^ 2);
                rad = sqrt (sv ^ 2 + v2 * (delta2 - s2));
                if (sv >= 0) {
                    alpha = (delta2 - s2) / (sv + rad);
                } else {
                    alpha = (rad - sv) / v2;
                }
                S = S + alpha * V;
                R = R - alpha * HV;
                innerconverge = TRUE;
            }
            inneriter = inneriter + 1;
            innerconverge = innerconverge | (inneriter > maxinneriter);
        }

        # END TRUST REGION SUB-PROBLEM

        # compute rho, update B, obtain delta
        gs = sum (S * Grad);
        qk = - 0.5 * (gs - sum (S * R));
        B_new = B + S;
        if (intercept_status == 2) {
            ssX_B_new = diag (scale_X) %*% B_new;
            ssX_B_new [D, ] = ssX_B_new [D, ] + t(shift_X) %*% B_new;
        } else {
            ssX_B_new = B_new;
        }

        LT = cbind ((X %*% ssX_B_new), matrix (0, rows = N, cols = 1));
        if (fileLog != " ") {
            log_str = append (log_str, "LINEAR_TERM_MIN,"  + iter + "," + min (LT));
            log_str = append (log_str, "LINEAR_TERM_MAX,"  + iter + "," + max (LT));
        }
        LT = LT - rowMaxs (LT) %*% matrix (1, rows = 1, cols = K+1);
        exp_LT = exp (LT);
        P_new  = exp_LT / (rowSums (exp_LT) %*% matrix (1, rows = 1, cols = K+1));
        obj_new = - sum (Y * LT) + sum (log (rowSums (exp_LT))) + 0.5 * sum (lambda * (B_new ^ 2));

        # Consider updating LT in the inner loop
        # Consider the big "obj" and "obj_new" rounding-off their small difference below:

        actred = (obj - obj_new);

        rho = actred / qk;
        is_rho_accepted = (rho > eta0);
        snorm = sqrt (sum (S ^ 2));

        if (fileLog != " ") {
            log_str = append (log_str, "NUM_CG_ITERS,"     + iter + "," + (inneriter - 1));
            log_str = append (log_str, "IS_TRUST_REACHED," + iter + "," + is_trust_boundary_reached);
            log_str = append (log_str, "POINT_STEP_NORM,"  + iter + "," + snorm);
            log_str = append (log_str, "OBJECTIVE,"        + iter + "," + obj_new);
            log_str = append (log_str, "OBJ_DROP_REAL,"    + iter + "," + actred);
            log_str = append (log_str, "OBJ_DROP_PRED,"    + iter + "," + qk);
            log_str = append (log_str, "OBJ_DROP_RATIO,"   + iter + "," + rho);
        }

    	if (iter == 1) {
    	   delta = min (delta, snorm);
    	}

    	alpha2 = obj_new - obj - gs;
    	if (alpha2 <= 0) {
    	   alpha = sigma3;
    	}
    	else {
    	   alpha = max (sigma1, -0.5 * gs / alpha2);
    	}

    	if (rho < eta0) {
    		delta = min (max (alpha, sigma1) * snorm, sigma2 * delta);
    	}
    	else {
    		if (rho < eta1) {
    			delta = max (sigma1 * delta, min (alpha * snorm, sigma2 * delta));
    		}
    		else {
    			if (rho < eta2) {
    				delta = max (sigma1 * delta, min (alpha * snorm, sigma3 * delta));
    			}
    			else {
    				delta = max (delta, min (alpha * snorm, sigma3 * delta));
    			}
    		}
    	}

    	if (is_trust_boundary_reached == 1)
    	{
    	    print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations, trust bound REACHED");
    	} else {
    	    print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations");
    	}
    	print ("   -- Obj.Reduction:  Actual = " + actred + ",  Predicted = " + qk +
    	       "  (A/P: " + (round (10000.0 * rho) / 10000.0) + "),  Trust Delta = " + delta);

    	if (is_rho_accepted)
    	{
    		B = B_new;
    		P = P_new;
    		Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
    		if (intercept_status == 2) {
    		    Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
    		}
    		Grad = Grad + lambda * B;
    		norm_Grad = sqrt (sum (Grad ^ 2));
    		obj = obj_new;
    	    print ("   -- New Objective = " + obj + ",  Beta Change Norm = " + snorm + ",  Gradient Norm = " + norm_Grad);
        }
        if (iter == 1) {
           delta = min (delta, snorm);
        }

        alpha2 = obj_new - obj - gs;
        if (alpha2 <= 0) {
           alpha = sigma3;
        }
        else {
           alpha = max (sigma1, -0.5 * gs / alpha2);
        }

        if (rho < eta0) {
            delta = min (max (alpha, sigma1) * snorm, sigma2 * delta);
        }
        else {
            if (rho < eta1) {
                delta = max (sigma1 * delta, min (alpha * snorm, sigma2 * delta));
            }
            else {
                if (rho < eta2) {
                    delta = max (sigma1 * delta, min (alpha * snorm, sigma3 * delta));
                }
                else {
                    delta = max (delta, min (alpha * snorm, sigma3 * delta));
                }
            }
        }

        if (is_trust_boundary_reached == 1)
        {
            print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations, trust bound REACHED");
        } else {
            print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations");
        }
        print ("   -- Obj.Reduction:  Actual = " + actred + ",  Predicted = " + qk +
               "  (A/P: " + (round (10000.0 * rho) / 10000.0) + "),  Trust Delta = " + delta);

        if (is_rho_accepted)
        {
            B = B_new;
            P = P_new;
            Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
            if (intercept_status == 2) {
                Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
            }
            Grad = Grad + lambda * B;
            norm_Grad = sqrt (sum (Grad ^ 2));
            obj = obj_new;
            print ("   -- New Objective = " + obj + ",  Beta Change Norm = " + snorm + ",  Gradient Norm = " + norm_Grad);
            if (fileLog != " ") {
                log_str = append (log_str, "IS_POINT_UPDATED," + iter + ",1");
                log_str = append (log_str, "GRADIENT_NORM,"    + iter + "," + norm_Grad);
            }
        } else {
            if (fileLog != " ") {
                log_str = append (log_str, "IS_POINT_UPDATED," + iter + ",0");
            }
        }

        if (fileLog != " ") {
            log_str = append (log_str, "TRUST_DELTA," + iter + "," + delta);
        }

        iter = iter + 1;
        converge = ((norm_Grad < (tol * norm_Grad_initial)) | (iter > maxiter) |
            ((is_trust_boundary_reached == 0) & (abs (actred) < (abs (obj) + abs (obj_new)) * 0.00000000000001)));
        if (converge) { print ("Termination / Convergence condition satisfied."); } else { print (" "); }
    }

    if (intercept_status == 2) {
        B_out = diag (scale_X) %*% B;
        B_out [D, ] = B_out [D, ] + t(shift_X) %*% B;
    } else {
        B_out = B;
    }
    #write (B_out, fileB, format=fmtB);

    if (sum_x > 0.0) {
        print (as.scalar(B[1,1]))
    }
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        stop = time(1)
    }
    times[ix,1] = stop-start
}

if (fileLog != " ") {
    write (log_str, fileLog);
}

times = t(times)

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg inputPath=/scratch/adclick_clean_1_sparse.parquet nodes=4 featureNames=features stub=_1 execSpark=true

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkContext
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.mllib.linalg._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.sql._
import scala.tools.nsc.io._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")
        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val inputPath = argMap("inputPath")
        val nodes = argMap("nodes")
        val featureNames = argMap("featureNames")
        val isSparse = inputPath.contains("sparse")

        val sparse_stub = if (isSparse) "_sparse" else "_dense"
        val stub = "/tests/MLAlgorithms (Native Implementations)/output/"
        val base = s"mllib_${opType}${nodes}${sparse_stub}.txt"
        val path = root + stub + base

        val input = spark.read.parquet(inputPath).
            withColumnRenamed(featureNames,"features")
        input.persist(MEMORY_AND_DISK_SER)
        println(input.count)

        val xRM = opType match {
            case "pca" => parquet_to_rm(input.select("features"))
            case _     => blank_row_matrix(spark.sparkContext)
        }

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test => ${ix}")
            val start = System.nanoTime()

            if (opType == "logit") {
                val lr = new LogisticRegression().
                    setMaxIter(3).
                    setLabelCol("y")
                val params = lr.fit(input)
                println(params.coefficients.size)
            } else if (opType == "reg") {
                val reg = new LinearRegression().
                    setMaxIter(3).
                    setLabelCol("y")
                val params = reg.fit(input)
                println(params.coefficients.size)
            } else if (opType == "pca") {
                val prcomp = xRM.computePrincipalComponents( 5 )
                val prj = xRM.multiply(prcomp)
                prj.rows.count
            } else {
                throw new Exception("Invalid operator")
            }
            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        File(path).appendAll(
            nodes + "," + times.mkString(",") + '\n')
    }

    def parquet_to_rm(df: DataFrame) : RowMatrix = {
        val X_rows = df.repartition(1000).rdd.
            map(tup => DenseVector.fromML(
                tup.getAs[alg.Vector](0).toDense).asInstanceOf[Vector])
        X_rows.persist(MEMORY_AND_DISK_SER)
        return new RowMatrix(X_rows)
    }

    def blank_row_matrix(sc: SparkContext) : RowMatrix = {
        val tmp = sc.parallelize(
            Seq(Vectors.dense(1,2,3), Vectors.dense(1,2,3)))
        return new RowMatrix(tmp)
    }
}

Running: spark-submit --class SparkMLAlgorithms   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=logit inputPath=/scratch/adclick_clean_1_sparse.parquet nodes=4 featureNames=features stub=_1

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SQLContext
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.runtime.instructions.spark.utils._
import org.apache.sysml.runtime.matrix.MatrixCharacteristics
import org.apache.spark.storage.StorageLevel._
import scala.tools.nsc.io._
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.sql.DataFrame
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms {
    def main(args: Array[String]) {
        val spark = SparkSession.builder.getOrCreate()
        val sc = spark.sparkContext
        val ml = new MLContext(sc)

        val root = sys.env("BENCHMARK_PROJECT_ROOT")
        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val inputPath = argMap("inputPath")
        val nodes = argMap("nodes")
        val featureNames = argMap("featureNames")
        val isSparse = inputPath.contains("sparse")
        val execSpark = argMap.get("execSpark")

        val execution_type = execSpark match {
            case Some(arg) => MLContext.ExecutionType.SPARK
            case None => MLContext.ExecutionType.DRIVER_AND_SPARK
        }

        val exec_type_stub = execSpark match {
            case Some(arg) => "spark"
            case None      => "spark_and_driver"
        }

        println(execution_type)
        ml.setExecutionType(execution_type)
        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val sparse_stub = if (isSparse) "_sparse" else "_dense"
        var stub = "/tests/MLAlgorithms (Native Implementations)/output/"
        val base = s"systemml_${opType}${nodes}${sparse_stub}_${exec_type_stub}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val script_stub = "tests/MLAlgorithms (Native Implementations)/src"
        val script_path = s"${root}/${script_stub}/systemml/src/main/dml"
        val input_df = spark.read.parquet(inputPath)
        val x = input_df.select(featureNames).repartition(1000)
        val y = input_df.select("y")
        
        println( x.count )
        var times = Array.ofDim[Double](5,1)
        if (opType == "logit") {
            val script = dmlFromFile(s"${script_path}/MultiLogitReg.dml").
                 in(Map("X"     -> x,
                        "Y_vec" -> y,
                        "$moi"  -> 3,
                        "$mii"  -> 0)).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else if (opType == "reg") {
             val script = dmlFromFile(s"${script_path}/LinearRegCG.dml").
                 in("X", x).
                 in("$maxi", 3).
                 in("y", y).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else if (opType == "pca") {
             val script = dmlFromFile(s"${script_path}/PCA.dml").
                 in("A", x).
                 in("K", 5).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else {
             throw new Exception("Invalid operator")
        }

        File(path).appendAll(
            nodes + "," + times(0).map(x => x/1000.0).mkString(",") + '\n')
    }
}
./systemml/src/main/dml/PCA.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
# 
#   http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# 
# This script performs Principal Component Analysis (PCA) on the given input data.
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME   TYPE   DEFAULT  MEANING
# ---------------------------------------------------------------------------------------------
# INPUT  String ---      Location to read the matrix A of feature vectors
# K      Int    ---      Indicates dimension of the new vector space constructed from eigen vectors
# CENTER Int    0        Indicates whether or not to center data 
# SCALE  Int    0        Indicates whether or not to scale data 
# OFMT   String ---      Output data format
# PROJDATA Int  0        This argument indicates if the data should be projected or not
# MODEL  String ---      Location to already existing model: eigenvectors and eigenvalues 
# OUTPUT String /        Location to write output matrices (covariance matrix, new basis vectors, 
#                           and data projected onto new basis vectors)
# hadoop jar SystemML.jar -f PCA.dml -nvargs INPUT=INPUT_DIR/pca-1000x1000 
# OUTPUT=OUTPUT_DIR/pca-1000x1000-model PROJDATA=1 CENTER=1 SCALE=1
# ---------------------------------------------------------------------------------------------

#A = read($INPUT);

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

sum_A = sum( A );
times = matrix(0, rows = 5, cols = 1)
for (ix in 1:5) {
    if ( sum_A > 0.0 ) {
        start = time(1)
    }

    K = ifdef($K, ncol(A));
    ofmt = ifdef($OFMT, "CSV");
    projectData = ifdef($PROJDATA,1);
    model = ifdef($MODEL,"");
    center = ifdef($CENTER,0);
    scale = ifdef($SCALE,0);
    output = ifdef($OUTPUT,"/");

    evec_dominant = matrix(0,cols=1,rows=1);

    if (model != "") {
        pass = 1.0
        # reuse existing model to project data
        #evec_dominant = read(model+"/dominant.eigen.vectors");
    } else {
        if (model == "" ){
            model = output; 
        }   

        N = nrow(A);
        D = ncol(A);

        # perform z-scoring (centering and scaling)
        if (center == 1) {
            cm = colMeans(A);
            A = A - cm;
        }
        if (scale == 1) {
            cvars = (colSums (A^2));    
            if (center == 1){
            cm = colMeans(A);
                cvars = (cvars - N*(cm^2))/(N-1);           
            }
            Azscored = (A)/sqrt(cvars);
                A = Azscored;
        }   

        # co-variance matrix 
        mu = colSums(A)/N;
        C = (t(A) %*% A)/(N-1) - (N/(N-1))*t(mu) %*% mu;


        # compute eigen vectors and values
        [evalues, evectors] = eigen(C);

        decreasing_Idx = order(target=evalues,by=1,decreasing=TRUE,index.return=TRUE);
        diagmat = table(seq(1,D),decreasing_Idx);
        # sorts eigenvalues by decreasing order
        evalues = diagmat %*% evalues;
        # sorts eigenvectors column-wise in the order of decreasing eigenvalues
        evectors = evectors %*% diagmat;


        # select K dominant eigen vectors 
        nvec = ncol(evectors);

        eval_dominant = evalues[1:K, 1];
        evec_dominant = evectors[,1:K];
        
        # the square root of eigenvalues
        eval_stdev_dominant = sqrt(eval_dominant);
        
        #write(eval_stdev_dominant, model+"/dominant.eigen.standard.deviations", format=ofmt);
        #write(eval_dominant, model+"/dominant.eigen.values", format=ofmt);
        #write(evec_dominant, model+"/dominant.eigen.vectors", format=ofmt);
    }
    if (projectData == 1 | model != ""){
        # Construct new data set by treating computed dominant eigenvectors as the basis vectors
        newA = A %*% evec_dominant;
        sum_newA = sum( newA )
        if (sum_newA > 0) {
            print(sum_newA)
        }
        #write(newA, output+"/projected.data", format=ofmt);
    }

    if (sum_A > 0.0) {
        stop = time(1)
    }
    times[ix,1] = (stop - start)
}

times = t(times)
./systemml/src/main/dml/LinearRegCG.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

#
# THIS SCRIPT SOLVES LINEAR REGRESSION USING THE CONJUGATE GRADIENT ALGORITHM
#
# INPUT PARAMETERS:
# --------------------------------------------------------------------------------------------
# NAME  TYPE   DEFAULT  MEANING
# --------------------------------------------------------------------------------------------
# X     String  ---     Location (on HDFS) to read the matrix X of feature vectors
# Y     String  ---     Location (on HDFS) to read the 1-column matrix Y of response values
# B     String  ---     Location to store estimated regression parameters (the betas)
# O     String  " "     Location to write the printed statistics; by default is standard output
# Log   String  " "     Location to write per-iteration variables for log/debugging purposes
# icpt  Int      0      Intercept presence, shifting and rescaling the columns of X:
#                       0 = no intercept, no shifting, no rescaling;
#                       1 = add intercept, but neither shift nor rescale X;
#                       2 = add intercept, shift & rescale X columns to mean = 0, variance = 1
# reg   Double 0.000001 Regularization constant (lambda) for L2-regularization; set to nonzero
#                       for highly dependend/sparse/numerous features
# tol   Double 0.000001 Tolerance (epsilon); conjugate graduent procedure terminates early if
#                       L2 norm of the beta-residual is less than tolerance * its initial norm
# maxi  Int      0      Maximum number of conjugate gradient iterations, 0 = no maximum
# fmt   String "text"   Matrix output format for B (the betas) only, usually "text" or "csv"
# --------------------------------------------------------------------------------------------
# OUTPUT: Matrix of regression parameters (the betas) and its size depend on icpt input value:
#         OUTPUT SIZE:   OUTPUT CONTENTS:                HOW TO PREDICT Y FROM X AND B:
# icpt=0: ncol(X)   x 1  Betas for X only                Y ~ X %*% B[1:ncol(X), 1], or just X %*% B
# icpt=1: ncol(X)+1 x 1  Betas for X and intercept       Y ~ X %*% B[1:ncol(X), 1] + B[ncol(X)+1, 1]
# icpt=2: ncol(X)+1 x 2  Col.1: betas for X & intercept  Y ~ X %*% B[1:ncol(X), 1] + B[ncol(X)+1, 1]
#                        Col.2: betas for shifted/rescaled X and intercept
#
# In addition, some regression statistics are provided in CSV format, one comma-separated
# name-value pair per each line, as follows:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------
# AVG_TOT_Y             Average of the response value Y
# STDEV_TOT_Y           Standard Deviation of the response value Y
# AVG_RES_Y             Average of the residual Y - pred(Y|X), i.e. residual bias
# STDEV_RES_Y           Standard Deviation of the residual Y - pred(Y|X)
# DISPERSION            GLM-style dispersion, i.e. residual sum of squares / # deg. fr.
# R2                    R^2 of residual with bias included vs. total average
# ADJUSTED_R2           Adjusted R^2 of residual with bias included vs. total average
# R2_NOBIAS             R^2 of residual with bias subtracted vs. total average
# ADJUSTED_R2_NOBIAS    Adjusted R^2 of residual with bias subtracted vs. total average
# R2_VS_0               * R^2 of residual with bias included vs. zero constant
# ADJUSTED_R2_VS_0      * Adjusted R^2 of residual with bias included vs. zero constant
# -------------------------------------------------------------------------------------
# * The last two statistics are only printed if there is no intercept (icpt=0)
#
# The Log file, when requested, contains the following per-iteration variables in CSV
# format, each line containing triple (NAME, ITERATION, VALUE) with ITERATION = 0 for
# initial values:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------
# CG_RESIDUAL_NORM      L2-norm of Conj.Grad.residual, which is A %*% beta - t(X) %*% y
#                           where A = t(X) %*% X + diag (lambda), or a similar quantity
# CG_RESIDUAL_RATIO     Ratio of current L2-norm of Conj.Grad.residual over the initial
# -------------------------------------------------------------------------------------
#
# HOW TO INVOKE THIS SCRIPT - EXAMPLE:
# hadoop jar SystemML.jar -f LinearRegCG.dml -nvargs X=INPUT_DIR/X Y=INPUT_DIR/Y B=OUTPUT_DIR/B
#     O=OUTPUT_DIR/Out icpt=2 reg=1.0 tol=0.001 maxi=100 fmt=csv Log=OUTPUT_DIR/log

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

#fileX = $X;
#fileY = $Y;
#fileB = $B;
#fileO = ifdef ($O, " ");
#fileLog = ifdef ($Log, " ");
#fmtB = ifdef ($fmt, "text");

intercept_status = ifdef ($icpt, 0);     # $icpt=0;
tolerance = ifdef ($tol, 0.000001);      # $tol=0.000001;
max_iteration = ifdef ($maxi, 0);        # $maxi=0;
regularization = ifdef ($reg, 0.000001); # $reg=0.000001;

print ("BEGIN LINEAR REGRESSION SCRIPT");
print ("Reading X and Y...");

sum_x = sum( X )
sum_y = sum( y )
times = matrix(0, rows = 5, cols = 1)
    for (ix in 1:5) {
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        start = time(1)
    }

    n = nrow (X);
    m = ncol (X);
    ones_n = matrix (1, rows = n, cols = 1);
    zero_cell = matrix (0, rows = 1, cols = 1);

    # Introduce the intercept, shift and rescale the columns of X if needed

    m_ext = m;
    if (intercept_status == 1 | intercept_status == 2)  # add the intercept column
    {
        X = cbind (X, ones_n);
        m_ext = ncol (X);
    }

    scale_lambda = matrix (1, rows = m_ext, cols = 1);
    if (intercept_status == 1 | intercept_status == 2)
    {
        scale_lambda [m_ext, 1] = 0;
    }

    if (intercept_status == 2)  # scale-&-shift X columns to mean 0, variance 1
    {                           # Important assumption: X [, m_ext] = ones_n
        avg_X_cols = t(colSums(X)) / n;
        var_X_cols = (t(colSums (X ^ 2)) - n * (avg_X_cols ^ 2)) / (n - 1);
        is_unsafe = (var_X_cols <= 0);
        scale_X = 1.0 / sqrt (var_X_cols * (1 - is_unsafe) + is_unsafe);
        scale_X [m_ext, 1] = 1;
        shift_X = - avg_X_cols * scale_X;
        shift_X [m_ext, 1] = 0;
    } else {
        scale_X = matrix (1, rows = m_ext, cols = 1);
        shift_X = matrix (0, rows = m_ext, cols = 1);
    }

    # Henceforth, if intercept_status == 2, we use "X %*% (SHIFT/SCALE TRANSFORM)"
    # instead of "X".  However, in order to preserve the sparsity of X,
    # we apply the transform associatively to some other part of the expression
    # in which it occurs.  To avoid materializing a large matrix, we rewrite it:
    #
    # ssX_A  = (SHIFT/SCALE TRANSFORM) %*% A    --- is rewritten as:
    # ssX_A  = diag (scale_X) %*% A;
    # ssX_A [m_ext, ] = ssX_A [m_ext, ] + t(shift_X) %*% A;
    #
    # tssX_A = t(SHIFT/SCALE TRANSFORM) %*% A   --- is rewritten as:
    # tssX_A = diag (scale_X) %*% A + shift_X %*% A [m_ext, ];

    lambda = scale_lambda * regularization;
    beta_unscaled = matrix (0, rows = m_ext, cols = 1);

    if (max_iteration == 0) {
        max_iteration = m_ext;
    }
    i = 0;

    # BEGIN THE CONJUGATE GRADIENT ALGORITHM
    print ("Running the CG algorithm...");

    r = - t(X) %*% y;

    if (intercept_status == 2) {
        r = scale_X * r + shift_X %*% r [m_ext, ];
    }

    p = - r;
    norm_r2 = sum (r ^ 2);
    norm_r2_initial = norm_r2;
    norm_r2_target = norm_r2_initial * tolerance ^ 2;
    print ("||r|| initial value = " + sqrt (norm_r2_initial) + ",  target value = " + sqrt (norm_r2_target));
    log_str = "CG_RESIDUAL_NORM,0," + sqrt (norm_r2_initial);
    log_str = append (log_str, "CG_RESIDUAL_RATIO,0,1.0");

    while (i < max_iteration & norm_r2 > norm_r2_target)
    {
        if (intercept_status == 2) {
            ssX_p = scale_X * p;
            ssX_p [m_ext, ] = ssX_p [m_ext, ] + t(shift_X) %*% p;
        } else {
            ssX_p = p;
        }

        q = t(X) %*% (X %*% ssX_p);

        if (intercept_status == 2) {
            q = scale_X * q + shift_X %*% q [m_ext, ];
        }

    	q = q + lambda * p;
    	a = norm_r2 / sum (p * q);
    	beta_unscaled = beta_unscaled + a * p;
    	r = r + a * q;
    	old_norm_r2 = norm_r2;
    	norm_r2 = sum (r ^ 2);
    	p = -r + (norm_r2 / old_norm_r2) * p;
    	i = i + 1;
    	print ("Iteration " + i + ":  ||r|| / ||r init|| = " + sqrt (norm_r2 / norm_r2_initial));
    	log_str = append (log_str, "CG_RESIDUAL_NORM,"  + i + "," + sqrt (norm_r2));
        log_str = append (log_str, "CG_RESIDUAL_RATIO," + i + "," + sqrt (norm_r2 / norm_r2_initial));
    }

    if (i >= max_iteration) {
        print ("Warning: the maximum number of iterations has been reached.");
    }
    print ("The CG algorithm is done.");
    # END THE CONJUGATE GRADIENT ALGORITHM

    if (intercept_status == 2) {
        beta = scale_X * beta_unscaled;
        beta [m_ext, ] = beta [m_ext, ] + t(shift_X) %*% beta_unscaled;
    } else {
        beta = beta_unscaled;
    }

    print ("Computing the statistics...");

    avg_tot = sum (y) / n;
    ss_tot = sum (y ^ 2);
    ss_avg_tot = ss_tot - n * avg_tot ^ 2;
    var_tot = ss_avg_tot / (n - 1);
    y_residual = y - X %*% beta;
    avg_res = sum (y_residual) / n;
    ss_res = sum (y_residual ^ 2);
    ss_avg_res = ss_res - n * avg_res ^ 2;

    R2 = 1 - ss_res / ss_avg_tot;
    if (n > m_ext) {
        dispersion  = ss_res / (n - m_ext);
        adjusted_R2 = 1 - dispersion / (ss_avg_tot / (n - 1));
    } else {
        dispersion  = 0.0 / 0.0;
        adjusted_R2 = 0.0 / 0.0;
    }

    R2_nobias = 1 - ss_avg_res / ss_avg_tot;
    deg_freedom = n - m - 1;
    if (deg_freedom > 0) {
        var_res = ss_avg_res / deg_freedom;
        adjusted_R2_nobias = 1 - var_res / (ss_avg_tot / (n - 1));
    } else {
        var_res = 0.0 / 0.0;
        adjusted_R2_nobias = 0.0 / 0.0;
        print ("Warning: zero or negative number of degrees of freedom.");
    }

    R2_vs_0 = 1 - ss_res / ss_tot;
    if (n > m) {
        adjusted_R2_vs_0 = 1 - (ss_res / (n - m)) / (ss_tot / n);
    } else {
        adjusted_R2_vs_0 = 0.0 / 0.0;
    }

    str = "AVG_TOT_Y," + avg_tot;                                    #  Average of the response value Y
    str = append (str, "STDEV_TOT_Y," + sqrt (var_tot));             #  Standard Deviation of the response value Y
    str = append (str, "AVG_RES_Y," + avg_res);                      #  Average of the residual Y - pred(Y|X), i.e. residual bias
    str = append (str, "STDEV_RES_Y," + sqrt (var_res));             #  Standard Deviation of the residual Y - pred(Y|X)
    str = append (str, "DISPERSION," + dispersion);                  #  GLM-style dispersion, i.e. residual sum of squares / # d.f.
    str = append (str, "R2," + R2);                                  #  R^2 of residual with bias included vs. total average
    str = append (str, "ADJUSTED_R2," + adjusted_R2);                #  Adjusted R^2 of residual with bias included vs. total average
    str = append (str, "R2_NOBIAS," + R2_nobias);                    #  R^2 of residual with bias subtracted vs. total average
    str = append (str, "ADJUSTED_R2_NOBIAS," + adjusted_R2_nobias);  #  Adjusted R^2 of residual with bias subtracted vs. total average
    if (intercept_status == 0) {
        str = append (str, "R2_VS_0," + R2_vs_0);                    #  R^2 of residual with bias included vs. zero constant
        str = append (str, "ADJUSTED_R2_VS_0," + adjusted_R2_vs_0);  #  Adjusted R^2 of residual with bias included vs. zero constant
    }

    print (str);

    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        stop = time(1)
    }
    times[ix,1] = stop - start
}

times = t(times)
./systemml/src/main/dml/MultiLogitReg.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# Solves Multinomial Logistic Regression using Trust Region methods.
# (See: Trust Region Newton Method for Logistic Regression, Lin, Weng and Keerthi, JMLR 9 (2008) 627-650)

# INPUT PARAMETERS:
# --------------------------------------------------------------------------------------------
# NAME  TYPE   DEFAULT  MEANING
# --------------------------------------------------------------------------------------------
# X     String  ---     Location to read the matrix of feature vectors
# Y     String  ---     Location to read the matrix with category labels
# B     String  ---     Location to store estimated regression parameters (the betas)
# Log   String  " "     Location to write per-iteration variables for log/debugging purposes
# icpt  Int      0      Intercept presence, shifting and rescaling X columns:
#                       0 = no intercept, no shifting, no rescaling;
#                       1 = add intercept, but neither shift nor rescale X;
#                       2 = add intercept, shift & rescale X columns to mean = 0, variance = 1
# reg   Double  0.0     regularization parameter (lambda = 1/C); intercept is not regularized
# tol   Double 0.000001 tolerance ("epsilon")
# moi   Int     100     max. number of outer (Newton) iterations
# mii   Int      0      max. number of inner (conjugate gradient) iterations, 0 = no max
# fmt   String "text"   Matrix output format, usually "text" or "csv" (for matrices only)
# --------------------------------------------------------------------------------------------
# The largest label represents the baseline category; if label -1 or 0 is present, then it is
# the baseline label (and it is converted to the largest label).
#
# The Log file, when requested, contains the following per-iteration variables in CSV format,
# each line containing triple (NAME, ITERATION, VALUE) with ITERATION = 0 for initial values:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------------
# LINEAR_TERM_MIN       The minimum value of X %*% B, used to check for overflows
# LINEAR_TERM_MAX       The maximum value of X %*% B, used to check for overflows
# NUM_CG_ITERS          Number of inner (Conj.Gradient) iterations in this outer iteration
# IS_TRUST_REACHED      1 = trust region boundary was reached, 0 = otherwise
# POINT_STEP_NORM       L2-norm of iteration step from old point (i.e. matrix B) to new point
# OBJECTIVE             The loss function we minimize (negative regularized log-likelihood)
# OBJ_DROP_REAL         Reduction in the objective during this iteration, actual value
# OBJ_DROP_PRED         Reduction in the objective predicted by a quadratic approximation
# OBJ_DROP_RATIO        Actual-to-predicted reduction ratio, used to update the trust region
# IS_POINT_UPDATED      1 = new point accepted; 0 = new point rejected, old point restored
# GRADIENT_NORM         L2-norm of the loss function gradient (omitted if point is rejected)
# TRUST_DELTA           Updated trust region size, the "delta"
# -------------------------------------------------------------------------------------------
#
# Script invocation example:
# hadoop jar SystemML.jar -f MultiLogReg.dml -nvargs icpt=2 reg=1.0 tol=0.000001 moi=100 mii=20
#     X=INPUT_DIR/X123 Y=INPUT_DIR/Y123 B=OUTPUT_DIR/B123 fmt=csv Log=OUTPUT_DIR/log

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

#fileX = $X;
#fileY = $Y;
#fileB = $B;
fileLog = ifdef ($Log, " ");
fmtB = ifdef ($fmt, "text");

intercept_status = ifdef ($icpt, 0); # $icpt = 0;
regularization = ifdef ($reg, 0.0);  # $reg  = 0.0;
tol = ifdef ($tol, 0.000001);        # $tol  = 0.000001;
maxiter = ifdef ($moi, 100);         # $moi  = 100;
maxinneriter = ifdef ($mii, 0);      # $mii  = 0;
tol = as.double (tol);

print ("BEGIN MULTINOMIAL LOGISTIC REGRESSION SCRIPT");
print ("Reading X...");
#X = read (fileX);
print ("Reading Y...");
#Y_vec = read (fileY);

# force a pass over the data

sum_x = sum( X )
sum_y = sum( Y_vec )
times = matrix(0, rows=5, cols=1)

for (ix in 1:5) {
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        start = time(1)
    }

    eta0 = 0.0001;
    eta1 = 0.25;
    eta2 = 0.75;
    sigma1 = 0.25;
    sigma2 = 0.5;
    sigma3 = 4.0;
    psi = 0.1;

    N = nrow (X);
    D = ncol (X);

    # Introduce the intercept, shift and rescale the columns of X if needed
    if (intercept_status == 1 | intercept_status == 2)  # add the intercept column
    {
        X = cbind (X, matrix (1, rows = N, cols = 1));
        D = ncol (X);
    }

    scale_lambda = matrix (1, rows = D, cols = 1);
    if (intercept_status == 1 | intercept_status == 2)
    {
        scale_lambda [D, 1] = 0;
    }

    if (intercept_status == 2)  # scale-&-shift X columns to mean 0, variance 1
    {                           # Important assumption: X [, D] = matrix (1, rows = N, cols = 1)
        avg_X_cols = t(colSums(X)) / N;
        var_X_cols = (t(colSums (X ^ 2)) - N * (avg_X_cols ^ 2)) / (N - 1);
        is_unsafe = var_X_cols <= 0;
        scale_X = 1.0 / sqrt (var_X_cols * (1 - is_unsafe) + is_unsafe);
        scale_X [D, 1] = 1;
        shift_X = - avg_X_cols * scale_X;
        shift_X [D, 1] = 0;
        rowSums_X_sq = (X ^ 2) %*% (scale_X ^ 2) + X %*% (2 * scale_X * shift_X) + sum (shift_X ^ 2);
    } else {
        scale_X = matrix (1, rows = D, cols = 1);
        shift_X = matrix (0, rows = D, cols = 1);
        rowSums_X_sq = rowSums (X ^ 2);
    }

    # Henceforth we replace "X" with "X %*% (SHIFT/SCALE TRANSFORM)" and rowSums(X ^ 2)
    # with "rowSums_X_sq" in order to preserve the sparsity of X under shift and scale.
    # The transform is then associatively applied to the other side of the expression,
    # and is rewritten via "scale_X" and "shift_X" as follows:
    #
    # ssX_A  = (SHIFT/SCALE TRANSFORM) %*% A    --- is rewritten as:
    # ssX_A  = diag (scale_X) %*% A;
    # ssX_A [D, ] = ssX_A [D, ] + t(shift_X) %*% A;
    #
    # tssX_A = t(SHIFT/SCALE TRANSFORM) %*% A   --- is rewritten as:
    # tssX_A = diag (scale_X) %*% A + shift_X %*% A [D, ];

    # Convert "Y_vec" into indicator matrix:
    max_y = max (Y_vec);
    if (min (Y_vec) <= 0) {
        # Category labels "0", "-1" etc. are converted into the largest label
        Y_vec  = Y_vec  + (- Y_vec  + max_y + 1) * (Y_vec <= 0);
        max_y = max_y + 1;
    }
    Y = table (seq (1, N, 1), Y_vec, N, max_y);
    K = ncol (Y) - 1;   # The number of  non-baseline categories

    lambda = (scale_lambda %*% matrix (1, rows = 1, cols = K)) * regularization;
    delta = 0.5 * sqrt (D) / max (sqrt (rowSums_X_sq));

    B = matrix (0, rows = D, cols = K);     ### LT = X %*% (SHIFT/SCALE TRANSFORM) %*% B;
                                            ### LT = cbind (LT, matrix (0, rows = N, cols = 1));
                                            ### LT = LT - rowMaxs (LT) %*% matrix (1, rows = 1, cols = K+1);
    P = matrix (1, rows = N, cols = K+1);   ### exp_LT = exp (LT);
    P = P / (K + 1);                        ### P =  exp_LT / (rowSums (exp_LT) %*% matrix (1, rows = 1, cols = K+1));
    obj = N * log (K + 1);                  ### obj = - sum (Y * LT) + sum (log (rowSums (exp_LT))) + 0.5 * sum (lambda * (B_new ^ 2));

    Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
    if (intercept_status == 2) {
        Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
    }
    Grad = Grad + lambda * B;
    norm_Grad = sqrt (sum (Grad ^ 2));
    norm_Grad_initial = norm_Grad;

    if (maxinneriter == 0) {
        maxinneriter = D * K;
    }
    iter = 1;

    # boolean for convergence check
    converge = (norm_Grad < tol) | (iter > maxiter);

    print ("-- Initially:  Objective = " + obj + ",  Gradient Norm = " + norm_Grad + ",  Trust Delta = " + delta);

    if (fileLog != " ") {
        log_str = "OBJECTIVE,0," + obj;
        log_str = append (log_str, "GRADIENT_NORM,0," + norm_Grad);
        log_str = append (log_str, "TRUST_DELTA,0," + delta);
    } else {
        log_str = " ";
    }

    while (! converge)
    {
        # SOLVE TRUST REGION SUB-PROBLEM
        S = matrix (0, rows = D, cols = K);
        R = - Grad;
        V = R;
        delta2 = delta ^ 2;
        inneriter = 1;
        norm_R2 = sum (R ^ 2);
        innerconverge = (sqrt (norm_R2) <= psi * norm_Grad);
        is_trust_boundary_reached = 0;

        while (! innerconverge)
        {
            if (intercept_status == 2) {
                ssX_V = diag (scale_X) %*% V;
                ssX_V [D, ] = ssX_V [D, ] + t(shift_X) %*% V;
            } else {
                ssX_V = V;
            }
            Q = P [, 1:K] * (X %*% ssX_V);
            HV = t(X) %*% (Q - P [, 1:K] * (rowSums (Q) %*% matrix (1, rows = 1, cols = K)));
            if (intercept_status == 2) {
                HV = diag (scale_X) %*% HV + shift_X %*% HV [D, ];
            }
            HV = HV + lambda * V;
            alpha = norm_R2 / sum (V * HV);
            Snew = S + alpha * V;
            norm_Snew2 = sum (Snew ^ 2);
            if (norm_Snew2 <= delta2)
            {
                S = Snew;
                R = R - alpha * HV;
                old_norm_R2 = norm_R2
                norm_R2 = sum (R ^ 2);
                V = R + (norm_R2 / old_norm_R2) * V;
                innerconverge = (sqrt (norm_R2) <= psi * norm_Grad);
            } else {
                is_trust_boundary_reached = 1;
                sv = sum (S * V);
                v2 = sum (V ^ 2);
                s2 = sum (S ^ 2);
                rad = sqrt (sv ^ 2 + v2 * (delta2 - s2));
                if (sv >= 0) {
                    alpha = (delta2 - s2) / (sv + rad);
                } else {
                    alpha = (rad - sv) / v2;
                }
                S = S + alpha * V;
                R = R - alpha * HV;
                innerconverge = TRUE;
            }
            inneriter = inneriter + 1;
            innerconverge = innerconverge | (inneriter > maxinneriter);
        }

        # END TRUST REGION SUB-PROBLEM

        # compute rho, update B, obtain delta
        gs = sum (S * Grad);
        qk = - 0.5 * (gs - sum (S * R));
        B_new = B + S;
        if (intercept_status == 2) {
            ssX_B_new = diag (scale_X) %*% B_new;
            ssX_B_new [D, ] = ssX_B_new [D, ] + t(shift_X) %*% B_new;
        } else {
            ssX_B_new = B_new;
        }

        LT = cbind ((X %*% ssX_B_new), matrix (0, rows = N, cols = 1));
        if (fileLog != " ") {
            log_str = append (log_str, "LINEAR_TERM_MIN,"  + iter + "," + min (LT));
            log_str = append (log_str, "LINEAR_TERM_MAX,"  + iter + "," + max (LT));
        }
        LT = LT - rowMaxs (LT) %*% matrix (1, rows = 1, cols = K+1);
        exp_LT = exp (LT);
        P_new  = exp_LT / (rowSums (exp_LT) %*% matrix (1, rows = 1, cols = K+1));
        obj_new = - sum (Y * LT) + sum (log (rowSums (exp_LT))) + 0.5 * sum (lambda * (B_new ^ 2));

        # Consider updating LT in the inner loop
        # Consider the big "obj" and "obj_new" rounding-off their small difference below:

        actred = (obj - obj_new);

        rho = actred / qk;
        is_rho_accepted = (rho > eta0);
        snorm = sqrt (sum (S ^ 2));

        if (fileLog != " ") {
            log_str = append (log_str, "NUM_CG_ITERS,"     + iter + "," + (inneriter - 1));
            log_str = append (log_str, "IS_TRUST_REACHED," + iter + "," + is_trust_boundary_reached);
            log_str = append (log_str, "POINT_STEP_NORM,"  + iter + "," + snorm);
            log_str = append (log_str, "OBJECTIVE,"        + iter + "," + obj_new);
            log_str = append (log_str, "OBJ_DROP_REAL,"    + iter + "," + actred);
            log_str = append (log_str, "OBJ_DROP_PRED,"    + iter + "," + qk);
            log_str = append (log_str, "OBJ_DROP_RATIO,"   + iter + "," + rho);
        }

    	if (iter == 1) {
    	   delta = min (delta, snorm);
    	}

    	alpha2 = obj_new - obj - gs;
    	if (alpha2 <= 0) {
    	   alpha = sigma3;
    	}
    	else {
    	   alpha = max (sigma1, -0.5 * gs / alpha2);
    	}

    	if (rho < eta0) {
    		delta = min (max (alpha, sigma1) * snorm, sigma2 * delta);
    	}
    	else {
    		if (rho < eta1) {
    			delta = max (sigma1 * delta, min (alpha * snorm, sigma2 * delta));
    		}
    		else {
    			if (rho < eta2) {
    				delta = max (sigma1 * delta, min (alpha * snorm, sigma3 * delta));
    			}
    			else {
    				delta = max (delta, min (alpha * snorm, sigma3 * delta));
    			}
    		}
    	}

    	if (is_trust_boundary_reached == 1)
    	{
    	    print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations, trust bound REACHED");
    	} else {
    	    print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations");
    	}
    	print ("   -- Obj.Reduction:  Actual = " + actred + ",  Predicted = " + qk +
    	       "  (A/P: " + (round (10000.0 * rho) / 10000.0) + "),  Trust Delta = " + delta);

    	if (is_rho_accepted)
    	{
    		B = B_new;
    		P = P_new;
    		Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
    		if (intercept_status == 2) {
    		    Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
    		}
    		Grad = Grad + lambda * B;
    		norm_Grad = sqrt (sum (Grad ^ 2));
    		obj = obj_new;
    	    print ("   -- New Objective = " + obj + ",  Beta Change Norm = " + snorm + ",  Gradient Norm = " + norm_Grad);
        }
        if (iter == 1) {
           delta = min (delta, snorm);
        }

        alpha2 = obj_new - obj - gs;
        if (alpha2 <= 0) {
           alpha = sigma3;
        }
        else {
           alpha = max (sigma1, -0.5 * gs / alpha2);
        }

        if (rho < eta0) {
            delta = min (max (alpha, sigma1) * snorm, sigma2 * delta);
        }
        else {
            if (rho < eta1) {
                delta = max (sigma1 * delta, min (alpha * snorm, sigma2 * delta));
            }
            else {
                if (rho < eta2) {
                    delta = max (sigma1 * delta, min (alpha * snorm, sigma3 * delta));
                }
                else {
                    delta = max (delta, min (alpha * snorm, sigma3 * delta));
                }
            }
        }

        if (is_trust_boundary_reached == 1)
        {
            print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations, trust bound REACHED");
        } else {
            print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations");
        }
        print ("   -- Obj.Reduction:  Actual = " + actred + ",  Predicted = " + qk +
               "  (A/P: " + (round (10000.0 * rho) / 10000.0) + "),  Trust Delta = " + delta);

        if (is_rho_accepted)
        {
            B = B_new;
            P = P_new;
            Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
            if (intercept_status == 2) {
                Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
            }
            Grad = Grad + lambda * B;
            norm_Grad = sqrt (sum (Grad ^ 2));
            obj = obj_new;
            print ("   -- New Objective = " + obj + ",  Beta Change Norm = " + snorm + ",  Gradient Norm = " + norm_Grad);
            if (fileLog != " ") {
                log_str = append (log_str, "IS_POINT_UPDATED," + iter + ",1");
                log_str = append (log_str, "GRADIENT_NORM,"    + iter + "," + norm_Grad);
            }
        } else {
            if (fileLog != " ") {
                log_str = append (log_str, "IS_POINT_UPDATED," + iter + ",0");
            }
        }

        if (fileLog != " ") {
            log_str = append (log_str, "TRUST_DELTA," + iter + "," + delta);
        }

        iter = iter + 1;
        converge = ((norm_Grad < (tol * norm_Grad_initial)) | (iter > maxiter) |
            ((is_trust_boundary_reached == 0) & (abs (actred) < (abs (obj) + abs (obj_new)) * 0.00000000000001)));
        if (converge) { print ("Termination / Convergence condition satisfied."); } else { print (" "); }
    }

    if (intercept_status == 2) {
        B_out = diag (scale_X) %*% B;
        B_out [D, ] = B_out [D, ] + t(shift_X) %*% B;
    } else {
        B_out = B;
    }
    #write (B_out, fileB, format=fmtB);

    if (sum_x > 0.0) {
        print (as.scalar(B[1,1]))
    }
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        stop = time(1)
    }
    times[ix,1] = stop-start
}

if (fileLog != " ") {
    write (log_str, fileLog);
}

times = t(times)

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 64G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit inputPath=/scratch/adclick_clean_1_sparse.parquet nodes=4 featureNames=features stub=_1

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SQLContext
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.runtime.instructions.spark.utils._
import org.apache.sysml.runtime.matrix.MatrixCharacteristics
import org.apache.spark.storage.StorageLevel._
import scala.tools.nsc.io._
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.sql.DataFrame
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms {
    def main(args: Array[String]) {
        val spark = SparkSession.builder.getOrCreate()
        val sc = spark.sparkContext
        val ml = new MLContext(sc)

        val root = sys.env("BENCHMARK_PROJECT_ROOT")
        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val inputPath = argMap("inputPath")
        val nodes = argMap("nodes")
        val featureNames = argMap("featureNames")
        val isSparse = inputPath.contains("sparse")
        val execSpark = argMap.get("execSpark")

        val execution_type = execSpark match {
            case Some(arg) => MLContext.ExecutionType.SPARK
            case None => MLContext.ExecutionType.DRIVER_AND_SPARK
        }

        val exec_type_stub = execSpark match {
            case Some(arg) => "spark"
            case None      => "spark_and_driver"
        }

        println(execution_type)
        ml.setExecutionType(execution_type)
        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val sparse_stub = if (isSparse) "_sparse" else "_dense"
        var stub = "/tests/MLAlgorithms (Native Implementations)/output/"
        val base = s"systemml_${opType}${nodes}${sparse_stub}_${exec_type_stub}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val script_stub = "tests/MLAlgorithms (Native Implementations)/src"
        val script_path = s"${root}/${script_stub}/systemml/src/main/dml"
        val input_df = spark.read.parquet(inputPath)
        val x = input_df.select(featureNames).repartition(1000)
        val y = input_df.select("y")
        
        println( x.count )
        var times = Array.ofDim[Double](5,1)
        if (opType == "logit") {
            val script = dmlFromFile(s"${script_path}/MultiLogitReg.dml").
                 in(Map("X"     -> x,
                        "Y_vec" -> y,
                        "$moi"  -> 3,
                        "$mii"  -> 0)).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else if (opType == "reg") {
             val script = dmlFromFile(s"${script_path}/LinearRegCG.dml").
                 in("X", x).
                 in("$maxi", 3).
                 in("y", y).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else if (opType == "pca") {
             val script = dmlFromFile(s"${script_path}/PCA.dml").
                 in("A", x).
                 in("K", 5).
                 out("times")
             val res = ml.execute(script)
             times = res.getTuple[Matrix]("times")._1.to2DDoubleArray
         } else {
             throw new Exception("Invalid operator")
        }

        File(path).appendAll(
            nodes + "," + times(0).map(x => x/1000.0).mkString(",") + '\n')
    }
}
./systemml/src/main/dml/PCA.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
# 
#   http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# 
# This script performs Principal Component Analysis (PCA) on the given input data.
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME   TYPE   DEFAULT  MEANING
# ---------------------------------------------------------------------------------------------
# INPUT  String ---      Location to read the matrix A of feature vectors
# K      Int    ---      Indicates dimension of the new vector space constructed from eigen vectors
# CENTER Int    0        Indicates whether or not to center data 
# SCALE  Int    0        Indicates whether or not to scale data 
# OFMT   String ---      Output data format
# PROJDATA Int  0        This argument indicates if the data should be projected or not
# MODEL  String ---      Location to already existing model: eigenvectors and eigenvalues 
# OUTPUT String /        Location to write output matrices (covariance matrix, new basis vectors, 
#                           and data projected onto new basis vectors)
# hadoop jar SystemML.jar -f PCA.dml -nvargs INPUT=INPUT_DIR/pca-1000x1000 
# OUTPUT=OUTPUT_DIR/pca-1000x1000-model PROJDATA=1 CENTER=1 SCALE=1
# ---------------------------------------------------------------------------------------------

#A = read($INPUT);

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

sum_A = sum( A );
times = matrix(0, rows = 5, cols = 1)
for (ix in 1:5) {
    if ( sum_A > 0.0 ) {
        start = time(1)
    }

    K = ifdef($K, ncol(A));
    ofmt = ifdef($OFMT, "CSV");
    projectData = ifdef($PROJDATA,1);
    model = ifdef($MODEL,"");
    center = ifdef($CENTER,0);
    scale = ifdef($SCALE,0);
    output = ifdef($OUTPUT,"/");

    evec_dominant = matrix(0,cols=1,rows=1);

    if (model != "") {
        pass = 1.0
        # reuse existing model to project data
        #evec_dominant = read(model+"/dominant.eigen.vectors");
    } else {
        if (model == "" ){
            model = output; 
        }   

        N = nrow(A);
        D = ncol(A);

        # perform z-scoring (centering and scaling)
        if (center == 1) {
            cm = colMeans(A);
            A = A - cm;
        }
        if (scale == 1) {
            cvars = (colSums (A^2));    
            if (center == 1){
            cm = colMeans(A);
                cvars = (cvars - N*(cm^2))/(N-1);           
            }
            Azscored = (A)/sqrt(cvars);
                A = Azscored;
        }   

        # co-variance matrix 
        mu = colSums(A)/N;
        C = (t(A) %*% A)/(N-1) - (N/(N-1))*t(mu) %*% mu;


        # compute eigen vectors and values
        [evalues, evectors] = eigen(C);

        decreasing_Idx = order(target=evalues,by=1,decreasing=TRUE,index.return=TRUE);
        diagmat = table(seq(1,D),decreasing_Idx);
        # sorts eigenvalues by decreasing order
        evalues = diagmat %*% evalues;
        # sorts eigenvectors column-wise in the order of decreasing eigenvalues
        evectors = evectors %*% diagmat;


        # select K dominant eigen vectors 
        nvec = ncol(evectors);

        eval_dominant = evalues[1:K, 1];
        evec_dominant = evectors[,1:K];
        
        # the square root of eigenvalues
        eval_stdev_dominant = sqrt(eval_dominant);
        
        #write(eval_stdev_dominant, model+"/dominant.eigen.standard.deviations", format=ofmt);
        #write(eval_dominant, model+"/dominant.eigen.values", format=ofmt);
        #write(evec_dominant, model+"/dominant.eigen.vectors", format=ofmt);
    }
    if (projectData == 1 | model != ""){
        # Construct new data set by treating computed dominant eigenvectors as the basis vectors
        newA = A %*% evec_dominant;
        sum_newA = sum( newA )
        if (sum_newA > 0) {
            print(sum_newA)
        }
        #write(newA, output+"/projected.data", format=ofmt);
    }

    if (sum_A > 0.0) {
        stop = time(1)
    }
    times[ix,1] = (stop - start)
}

times = t(times)
./systemml/src/main/dml/LinearRegCG.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

#
# THIS SCRIPT SOLVES LINEAR REGRESSION USING THE CONJUGATE GRADIENT ALGORITHM
#
# INPUT PARAMETERS:
# --------------------------------------------------------------------------------------------
# NAME  TYPE   DEFAULT  MEANING
# --------------------------------------------------------------------------------------------
# X     String  ---     Location (on HDFS) to read the matrix X of feature vectors
# Y     String  ---     Location (on HDFS) to read the 1-column matrix Y of response values
# B     String  ---     Location to store estimated regression parameters (the betas)
# O     String  " "     Location to write the printed statistics; by default is standard output
# Log   String  " "     Location to write per-iteration variables for log/debugging purposes
# icpt  Int      0      Intercept presence, shifting and rescaling the columns of X:
#                       0 = no intercept, no shifting, no rescaling;
#                       1 = add intercept, but neither shift nor rescale X;
#                       2 = add intercept, shift & rescale X columns to mean = 0, variance = 1
# reg   Double 0.000001 Regularization constant (lambda) for L2-regularization; set to nonzero
#                       for highly dependend/sparse/numerous features
# tol   Double 0.000001 Tolerance (epsilon); conjugate graduent procedure terminates early if
#                       L2 norm of the beta-residual is less than tolerance * its initial norm
# maxi  Int      0      Maximum number of conjugate gradient iterations, 0 = no maximum
# fmt   String "text"   Matrix output format for B (the betas) only, usually "text" or "csv"
# --------------------------------------------------------------------------------------------
# OUTPUT: Matrix of regression parameters (the betas) and its size depend on icpt input value:
#         OUTPUT SIZE:   OUTPUT CONTENTS:                HOW TO PREDICT Y FROM X AND B:
# icpt=0: ncol(X)   x 1  Betas for X only                Y ~ X %*% B[1:ncol(X), 1], or just X %*% B
# icpt=1: ncol(X)+1 x 1  Betas for X and intercept       Y ~ X %*% B[1:ncol(X), 1] + B[ncol(X)+1, 1]
# icpt=2: ncol(X)+1 x 2  Col.1: betas for X & intercept  Y ~ X %*% B[1:ncol(X), 1] + B[ncol(X)+1, 1]
#                        Col.2: betas for shifted/rescaled X and intercept
#
# In addition, some regression statistics are provided in CSV format, one comma-separated
# name-value pair per each line, as follows:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------
# AVG_TOT_Y             Average of the response value Y
# STDEV_TOT_Y           Standard Deviation of the response value Y
# AVG_RES_Y             Average of the residual Y - pred(Y|X), i.e. residual bias
# STDEV_RES_Y           Standard Deviation of the residual Y - pred(Y|X)
# DISPERSION            GLM-style dispersion, i.e. residual sum of squares / # deg. fr.
# R2                    R^2 of residual with bias included vs. total average
# ADJUSTED_R2           Adjusted R^2 of residual with bias included vs. total average
# R2_NOBIAS             R^2 of residual with bias subtracted vs. total average
# ADJUSTED_R2_NOBIAS    Adjusted R^2 of residual with bias subtracted vs. total average
# R2_VS_0               * R^2 of residual with bias included vs. zero constant
# ADJUSTED_R2_VS_0      * Adjusted R^2 of residual with bias included vs. zero constant
# -------------------------------------------------------------------------------------
# * The last two statistics are only printed if there is no intercept (icpt=0)
#
# The Log file, when requested, contains the following per-iteration variables in CSV
# format, each line containing triple (NAME, ITERATION, VALUE) with ITERATION = 0 for
# initial values:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------
# CG_RESIDUAL_NORM      L2-norm of Conj.Grad.residual, which is A %*% beta - t(X) %*% y
#                           where A = t(X) %*% X + diag (lambda), or a similar quantity
# CG_RESIDUAL_RATIO     Ratio of current L2-norm of Conj.Grad.residual over the initial
# -------------------------------------------------------------------------------------
#
# HOW TO INVOKE THIS SCRIPT - EXAMPLE:
# hadoop jar SystemML.jar -f LinearRegCG.dml -nvargs X=INPUT_DIR/X Y=INPUT_DIR/Y B=OUTPUT_DIR/B
#     O=OUTPUT_DIR/Out icpt=2 reg=1.0 tol=0.001 maxi=100 fmt=csv Log=OUTPUT_DIR/log

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

#fileX = $X;
#fileY = $Y;
#fileB = $B;
#fileO = ifdef ($O, " ");
#fileLog = ifdef ($Log, " ");
#fmtB = ifdef ($fmt, "text");

intercept_status = ifdef ($icpt, 0);     # $icpt=0;
tolerance = ifdef ($tol, 0.000001);      # $tol=0.000001;
max_iteration = ifdef ($maxi, 0);        # $maxi=0;
regularization = ifdef ($reg, 0.000001); # $reg=0.000001;

print ("BEGIN LINEAR REGRESSION SCRIPT");
print ("Reading X and Y...");

sum_x = sum( X )
sum_y = sum( y )
times = matrix(0, rows = 5, cols = 1)
    for (ix in 1:5) {
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        start = time(1)
    }

    n = nrow (X);
    m = ncol (X);
    ones_n = matrix (1, rows = n, cols = 1);
    zero_cell = matrix (0, rows = 1, cols = 1);

    # Introduce the intercept, shift and rescale the columns of X if needed

    m_ext = m;
    if (intercept_status == 1 | intercept_status == 2)  # add the intercept column
    {
        X = cbind (X, ones_n);
        m_ext = ncol (X);
    }

    scale_lambda = matrix (1, rows = m_ext, cols = 1);
    if (intercept_status == 1 | intercept_status == 2)
    {
        scale_lambda [m_ext, 1] = 0;
    }

    if (intercept_status == 2)  # scale-&-shift X columns to mean 0, variance 1
    {                           # Important assumption: X [, m_ext] = ones_n
        avg_X_cols = t(colSums(X)) / n;
        var_X_cols = (t(colSums (X ^ 2)) - n * (avg_X_cols ^ 2)) / (n - 1);
        is_unsafe = (var_X_cols <= 0);
        scale_X = 1.0 / sqrt (var_X_cols * (1 - is_unsafe) + is_unsafe);
        scale_X [m_ext, 1] = 1;
        shift_X = - avg_X_cols * scale_X;
        shift_X [m_ext, 1] = 0;
    } else {
        scale_X = matrix (1, rows = m_ext, cols = 1);
        shift_X = matrix (0, rows = m_ext, cols = 1);
    }

    # Henceforth, if intercept_status == 2, we use "X %*% (SHIFT/SCALE TRANSFORM)"
    # instead of "X".  However, in order to preserve the sparsity of X,
    # we apply the transform associatively to some other part of the expression
    # in which it occurs.  To avoid materializing a large matrix, we rewrite it:
    #
    # ssX_A  = (SHIFT/SCALE TRANSFORM) %*% A    --- is rewritten as:
    # ssX_A  = diag (scale_X) %*% A;
    # ssX_A [m_ext, ] = ssX_A [m_ext, ] + t(shift_X) %*% A;
    #
    # tssX_A = t(SHIFT/SCALE TRANSFORM) %*% A   --- is rewritten as:
    # tssX_A = diag (scale_X) %*% A + shift_X %*% A [m_ext, ];

    lambda = scale_lambda * regularization;
    beta_unscaled = matrix (0, rows = m_ext, cols = 1);

    if (max_iteration == 0) {
        max_iteration = m_ext;
    }
    i = 0;

    # BEGIN THE CONJUGATE GRADIENT ALGORITHM
    print ("Running the CG algorithm...");

    r = - t(X) %*% y;

    if (intercept_status == 2) {
        r = scale_X * r + shift_X %*% r [m_ext, ];
    }

    p = - r;
    norm_r2 = sum (r ^ 2);
    norm_r2_initial = norm_r2;
    norm_r2_target = norm_r2_initial * tolerance ^ 2;
    print ("||r|| initial value = " + sqrt (norm_r2_initial) + ",  target value = " + sqrt (norm_r2_target));
    log_str = "CG_RESIDUAL_NORM,0," + sqrt (norm_r2_initial);
    log_str = append (log_str, "CG_RESIDUAL_RATIO,0,1.0");

    while (i < max_iteration & norm_r2 > norm_r2_target)
    {
        if (intercept_status == 2) {
            ssX_p = scale_X * p;
            ssX_p [m_ext, ] = ssX_p [m_ext, ] + t(shift_X) %*% p;
        } else {
            ssX_p = p;
        }

        q = t(X) %*% (X %*% ssX_p);

        if (intercept_status == 2) {
            q = scale_X * q + shift_X %*% q [m_ext, ];
        }

    	q = q + lambda * p;
    	a = norm_r2 / sum (p * q);
    	beta_unscaled = beta_unscaled + a * p;
    	r = r + a * q;
    	old_norm_r2 = norm_r2;
    	norm_r2 = sum (r ^ 2);
    	p = -r + (norm_r2 / old_norm_r2) * p;
    	i = i + 1;
    	print ("Iteration " + i + ":  ||r|| / ||r init|| = " + sqrt (norm_r2 / norm_r2_initial));
    	log_str = append (log_str, "CG_RESIDUAL_NORM,"  + i + "," + sqrt (norm_r2));
        log_str = append (log_str, "CG_RESIDUAL_RATIO," + i + "," + sqrt (norm_r2 / norm_r2_initial));
    }

    if (i >= max_iteration) {
        print ("Warning: the maximum number of iterations has been reached.");
    }
    print ("The CG algorithm is done.");
    # END THE CONJUGATE GRADIENT ALGORITHM

    if (intercept_status == 2) {
        beta = scale_X * beta_unscaled;
        beta [m_ext, ] = beta [m_ext, ] + t(shift_X) %*% beta_unscaled;
    } else {
        beta = beta_unscaled;
    }

    print ("Computing the statistics...");

    avg_tot = sum (y) / n;
    ss_tot = sum (y ^ 2);
    ss_avg_tot = ss_tot - n * avg_tot ^ 2;
    var_tot = ss_avg_tot / (n - 1);
    y_residual = y - X %*% beta;
    avg_res = sum (y_residual) / n;
    ss_res = sum (y_residual ^ 2);
    ss_avg_res = ss_res - n * avg_res ^ 2;

    R2 = 1 - ss_res / ss_avg_tot;
    if (n > m_ext) {
        dispersion  = ss_res / (n - m_ext);
        adjusted_R2 = 1 - dispersion / (ss_avg_tot / (n - 1));
    } else {
        dispersion  = 0.0 / 0.0;
        adjusted_R2 = 0.0 / 0.0;
    }

    R2_nobias = 1 - ss_avg_res / ss_avg_tot;
    deg_freedom = n - m - 1;
    if (deg_freedom > 0) {
        var_res = ss_avg_res / deg_freedom;
        adjusted_R2_nobias = 1 - var_res / (ss_avg_tot / (n - 1));
    } else {
        var_res = 0.0 / 0.0;
        adjusted_R2_nobias = 0.0 / 0.0;
        print ("Warning: zero or negative number of degrees of freedom.");
    }

    R2_vs_0 = 1 - ss_res / ss_tot;
    if (n > m) {
        adjusted_R2_vs_0 = 1 - (ss_res / (n - m)) / (ss_tot / n);
    } else {
        adjusted_R2_vs_0 = 0.0 / 0.0;
    }

    str = "AVG_TOT_Y," + avg_tot;                                    #  Average of the response value Y
    str = append (str, "STDEV_TOT_Y," + sqrt (var_tot));             #  Standard Deviation of the response value Y
    str = append (str, "AVG_RES_Y," + avg_res);                      #  Average of the residual Y - pred(Y|X), i.e. residual bias
    str = append (str, "STDEV_RES_Y," + sqrt (var_res));             #  Standard Deviation of the residual Y - pred(Y|X)
    str = append (str, "DISPERSION," + dispersion);                  #  GLM-style dispersion, i.e. residual sum of squares / # d.f.
    str = append (str, "R2," + R2);                                  #  R^2 of residual with bias included vs. total average
    str = append (str, "ADJUSTED_R2," + adjusted_R2);                #  Adjusted R^2 of residual with bias included vs. total average
    str = append (str, "R2_NOBIAS," + R2_nobias);                    #  R^2 of residual with bias subtracted vs. total average
    str = append (str, "ADJUSTED_R2_NOBIAS," + adjusted_R2_nobias);  #  Adjusted R^2 of residual with bias subtracted vs. total average
    if (intercept_status == 0) {
        str = append (str, "R2_VS_0," + R2_vs_0);                    #  R^2 of residual with bias included vs. zero constant
        str = append (str, "ADJUSTED_R2_VS_0," + adjusted_R2_vs_0);  #  Adjusted R^2 of residual with bias included vs. zero constant
    }

    print (str);

    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        stop = time(1)
    }
    times[ix,1] = stop - start
}

times = t(times)
./systemml/src/main/dml/MultiLogitReg.dml
================================================================================
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# Solves Multinomial Logistic Regression using Trust Region methods.
# (See: Trust Region Newton Method for Logistic Regression, Lin, Weng and Keerthi, JMLR 9 (2008) 627-650)

# INPUT PARAMETERS:
# --------------------------------------------------------------------------------------------
# NAME  TYPE   DEFAULT  MEANING
# --------------------------------------------------------------------------------------------
# X     String  ---     Location to read the matrix of feature vectors
# Y     String  ---     Location to read the matrix with category labels
# B     String  ---     Location to store estimated regression parameters (the betas)
# Log   String  " "     Location to write per-iteration variables for log/debugging purposes
# icpt  Int      0      Intercept presence, shifting and rescaling X columns:
#                       0 = no intercept, no shifting, no rescaling;
#                       1 = add intercept, but neither shift nor rescale X;
#                       2 = add intercept, shift & rescale X columns to mean = 0, variance = 1
# reg   Double  0.0     regularization parameter (lambda = 1/C); intercept is not regularized
# tol   Double 0.000001 tolerance ("epsilon")
# moi   Int     100     max. number of outer (Newton) iterations
# mii   Int      0      max. number of inner (conjugate gradient) iterations, 0 = no max
# fmt   String "text"   Matrix output format, usually "text" or "csv" (for matrices only)
# --------------------------------------------------------------------------------------------
# The largest label represents the baseline category; if label -1 or 0 is present, then it is
# the baseline label (and it is converted to the largest label).
#
# The Log file, when requested, contains the following per-iteration variables in CSV format,
# each line containing triple (NAME, ITERATION, VALUE) with ITERATION = 0 for initial values:
#
# NAME                  MEANING
# -------------------------------------------------------------------------------------------
# LINEAR_TERM_MIN       The minimum value of X %*% B, used to check for overflows
# LINEAR_TERM_MAX       The maximum value of X %*% B, used to check for overflows
# NUM_CG_ITERS          Number of inner (Conj.Gradient) iterations in this outer iteration
# IS_TRUST_REACHED      1 = trust region boundary was reached, 0 = otherwise
# POINT_STEP_NORM       L2-norm of iteration step from old point (i.e. matrix B) to new point
# OBJECTIVE             The loss function we minimize (negative regularized log-likelihood)
# OBJ_DROP_REAL         Reduction in the objective during this iteration, actual value
# OBJ_DROP_PRED         Reduction in the objective predicted by a quadratic approximation
# OBJ_DROP_RATIO        Actual-to-predicted reduction ratio, used to update the trust region
# IS_POINT_UPDATED      1 = new point accepted; 0 = new point rejected, old point restored
# GRADIENT_NORM         L2-norm of the loss function gradient (omitted if point is rejected)
# TRUST_DELTA           Updated trust region size, the "delta"
# -------------------------------------------------------------------------------------------
#
# Script invocation example:
# hadoop jar SystemML.jar -f MultiLogReg.dml -nvargs icpt=2 reg=1.0 tol=0.000001 moi=100 mii=20
#     X=INPUT_DIR/X123 Y=INPUT_DIR/Y123 B=OUTPUT_DIR/B123 fmt=csv Log=OUTPUT_DIR/log

time = externalFunction(Integer i) return (Double B)
       implemented in (classname="org.apache.sysml.udf.lib.TimeWrapper", exectype="mem");

#fileX = $X;
#fileY = $Y;
#fileB = $B;
fileLog = ifdef ($Log, " ");
fmtB = ifdef ($fmt, "text");

intercept_status = ifdef ($icpt, 0); # $icpt = 0;
regularization = ifdef ($reg, 0.0);  # $reg  = 0.0;
tol = ifdef ($tol, 0.000001);        # $tol  = 0.000001;
maxiter = ifdef ($moi, 100);         # $moi  = 100;
maxinneriter = ifdef ($mii, 0);      # $mii  = 0;
tol = as.double (tol);

print ("BEGIN MULTINOMIAL LOGISTIC REGRESSION SCRIPT");
print ("Reading X...");
#X = read (fileX);
print ("Reading Y...");
#Y_vec = read (fileY);

# force a pass over the data

sum_x = sum( X )
sum_y = sum( Y_vec )
times = matrix(0, rows=5, cols=1)

for (ix in 1:5) {
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        start = time(1)
    }

    eta0 = 0.0001;
    eta1 = 0.25;
    eta2 = 0.75;
    sigma1 = 0.25;
    sigma2 = 0.5;
    sigma3 = 4.0;
    psi = 0.1;

    N = nrow (X);
    D = ncol (X);

    # Introduce the intercept, shift and rescale the columns of X if needed
    if (intercept_status == 1 | intercept_status == 2)  # add the intercept column
    {
        X = cbind (X, matrix (1, rows = N, cols = 1));
        D = ncol (X);
    }

    scale_lambda = matrix (1, rows = D, cols = 1);
    if (intercept_status == 1 | intercept_status == 2)
    {
        scale_lambda [D, 1] = 0;
    }

    if (intercept_status == 2)  # scale-&-shift X columns to mean 0, variance 1
    {                           # Important assumption: X [, D] = matrix (1, rows = N, cols = 1)
        avg_X_cols = t(colSums(X)) / N;
        var_X_cols = (t(colSums (X ^ 2)) - N * (avg_X_cols ^ 2)) / (N - 1);
        is_unsafe = var_X_cols <= 0;
        scale_X = 1.0 / sqrt (var_X_cols * (1 - is_unsafe) + is_unsafe);
        scale_X [D, 1] = 1;
        shift_X = - avg_X_cols * scale_X;
        shift_X [D, 1] = 0;
        rowSums_X_sq = (X ^ 2) %*% (scale_X ^ 2) + X %*% (2 * scale_X * shift_X) + sum (shift_X ^ 2);
    } else {
        scale_X = matrix (1, rows = D, cols = 1);
        shift_X = matrix (0, rows = D, cols = 1);
        rowSums_X_sq = rowSums (X ^ 2);
    }

    # Henceforth we replace "X" with "X %*% (SHIFT/SCALE TRANSFORM)" and rowSums(X ^ 2)
    # with "rowSums_X_sq" in order to preserve the sparsity of X under shift and scale.
    # The transform is then associatively applied to the other side of the expression,
    # and is rewritten via "scale_X" and "shift_X" as follows:
    #
    # ssX_A  = (SHIFT/SCALE TRANSFORM) %*% A    --- is rewritten as:
    # ssX_A  = diag (scale_X) %*% A;
    # ssX_A [D, ] = ssX_A [D, ] + t(shift_X) %*% A;
    #
    # tssX_A = t(SHIFT/SCALE TRANSFORM) %*% A   --- is rewritten as:
    # tssX_A = diag (scale_X) %*% A + shift_X %*% A [D, ];

    # Convert "Y_vec" into indicator matrix:
    max_y = max (Y_vec);
    if (min (Y_vec) <= 0) {
        # Category labels "0", "-1" etc. are converted into the largest label
        Y_vec  = Y_vec  + (- Y_vec  + max_y + 1) * (Y_vec <= 0);
        max_y = max_y + 1;
    }
    Y = table (seq (1, N, 1), Y_vec, N, max_y);
    K = ncol (Y) - 1;   # The number of  non-baseline categories

    lambda = (scale_lambda %*% matrix (1, rows = 1, cols = K)) * regularization;
    delta = 0.5 * sqrt (D) / max (sqrt (rowSums_X_sq));

    B = matrix (0, rows = D, cols = K);     ### LT = X %*% (SHIFT/SCALE TRANSFORM) %*% B;
                                            ### LT = cbind (LT, matrix (0, rows = N, cols = 1));
                                            ### LT = LT - rowMaxs (LT) %*% matrix (1, rows = 1, cols = K+1);
    P = matrix (1, rows = N, cols = K+1);   ### exp_LT = exp (LT);
    P = P / (K + 1);                        ### P =  exp_LT / (rowSums (exp_LT) %*% matrix (1, rows = 1, cols = K+1));
    obj = N * log (K + 1);                  ### obj = - sum (Y * LT) + sum (log (rowSums (exp_LT))) + 0.5 * sum (lambda * (B_new ^ 2));

    Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
    if (intercept_status == 2) {
        Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
    }
    Grad = Grad + lambda * B;
    norm_Grad = sqrt (sum (Grad ^ 2));
    norm_Grad_initial = norm_Grad;

    if (maxinneriter == 0) {
        maxinneriter = D * K;
    }
    iter = 1;

    # boolean for convergence check
    converge = (norm_Grad < tol) | (iter > maxiter);

    print ("-- Initially:  Objective = " + obj + ",  Gradient Norm = " + norm_Grad + ",  Trust Delta = " + delta);

    if (fileLog != " ") {
        log_str = "OBJECTIVE,0," + obj;
        log_str = append (log_str, "GRADIENT_NORM,0," + norm_Grad);
        log_str = append (log_str, "TRUST_DELTA,0," + delta);
    } else {
        log_str = " ";
    }

    while (! converge)
    {
        # SOLVE TRUST REGION SUB-PROBLEM
        S = matrix (0, rows = D, cols = K);
        R = - Grad;
        V = R;
        delta2 = delta ^ 2;
        inneriter = 1;
        norm_R2 = sum (R ^ 2);
        innerconverge = (sqrt (norm_R2) <= psi * norm_Grad);
        is_trust_boundary_reached = 0;

        while (! innerconverge)
        {
            if (intercept_status == 2) {
                ssX_V = diag (scale_X) %*% V;
                ssX_V [D, ] = ssX_V [D, ] + t(shift_X) %*% V;
            } else {
                ssX_V = V;
            }
            Q = P [, 1:K] * (X %*% ssX_V);
            HV = t(X) %*% (Q - P [, 1:K] * (rowSums (Q) %*% matrix (1, rows = 1, cols = K)));
            if (intercept_status == 2) {
                HV = diag (scale_X) %*% HV + shift_X %*% HV [D, ];
            }
            HV = HV + lambda * V;
            alpha = norm_R2 / sum (V * HV);
            Snew = S + alpha * V;
            norm_Snew2 = sum (Snew ^ 2);
            if (norm_Snew2 <= delta2)
            {
                S = Snew;
                R = R - alpha * HV;
                old_norm_R2 = norm_R2
                norm_R2 = sum (R ^ 2);
                V = R + (norm_R2 / old_norm_R2) * V;
                innerconverge = (sqrt (norm_R2) <= psi * norm_Grad);
            } else {
                is_trust_boundary_reached = 1;
                sv = sum (S * V);
                v2 = sum (V ^ 2);
                s2 = sum (S ^ 2);
                rad = sqrt (sv ^ 2 + v2 * (delta2 - s2));
                if (sv >= 0) {
                    alpha = (delta2 - s2) / (sv + rad);
                } else {
                    alpha = (rad - sv) / v2;
                }
                S = S + alpha * V;
                R = R - alpha * HV;
                innerconverge = TRUE;
            }
            inneriter = inneriter + 1;
            innerconverge = innerconverge | (inneriter > maxinneriter);
        }

        # END TRUST REGION SUB-PROBLEM

        # compute rho, update B, obtain delta
        gs = sum (S * Grad);
        qk = - 0.5 * (gs - sum (S * R));
        B_new = B + S;
        if (intercept_status == 2) {
            ssX_B_new = diag (scale_X) %*% B_new;
            ssX_B_new [D, ] = ssX_B_new [D, ] + t(shift_X) %*% B_new;
        } else {
            ssX_B_new = B_new;
        }

        LT = cbind ((X %*% ssX_B_new), matrix (0, rows = N, cols = 1));
        if (fileLog != " ") {
            log_str = append (log_str, "LINEAR_TERM_MIN,"  + iter + "," + min (LT));
            log_str = append (log_str, "LINEAR_TERM_MAX,"  + iter + "," + max (LT));
        }
        LT = LT - rowMaxs (LT) %*% matrix (1, rows = 1, cols = K+1);
        exp_LT = exp (LT);
        P_new  = exp_LT / (rowSums (exp_LT) %*% matrix (1, rows = 1, cols = K+1));
        obj_new = - sum (Y * LT) + sum (log (rowSums (exp_LT))) + 0.5 * sum (lambda * (B_new ^ 2));

        # Consider updating LT in the inner loop
        # Consider the big "obj" and "obj_new" rounding-off their small difference below:

        actred = (obj - obj_new);

        rho = actred / qk;
        is_rho_accepted = (rho > eta0);
        snorm = sqrt (sum (S ^ 2));

        if (fileLog != " ") {
            log_str = append (log_str, "NUM_CG_ITERS,"     + iter + "," + (inneriter - 1));
            log_str = append (log_str, "IS_TRUST_REACHED," + iter + "," + is_trust_boundary_reached);
            log_str = append (log_str, "POINT_STEP_NORM,"  + iter + "," + snorm);
            log_str = append (log_str, "OBJECTIVE,"        + iter + "," + obj_new);
            log_str = append (log_str, "OBJ_DROP_REAL,"    + iter + "," + actred);
            log_str = append (log_str, "OBJ_DROP_PRED,"    + iter + "," + qk);
            log_str = append (log_str, "OBJ_DROP_RATIO,"   + iter + "," + rho);
        }

    	if (iter == 1) {
    	   delta = min (delta, snorm);
    	}

    	alpha2 = obj_new - obj - gs;
    	if (alpha2 <= 0) {
    	   alpha = sigma3;
    	}
    	else {
    	   alpha = max (sigma1, -0.5 * gs / alpha2);
    	}

    	if (rho < eta0) {
    		delta = min (max (alpha, sigma1) * snorm, sigma2 * delta);
    	}
    	else {
    		if (rho < eta1) {
    			delta = max (sigma1 * delta, min (alpha * snorm, sigma2 * delta));
    		}
    		else {
    			if (rho < eta2) {
    				delta = max (sigma1 * delta, min (alpha * snorm, sigma3 * delta));
    			}
    			else {
    				delta = max (delta, min (alpha * snorm, sigma3 * delta));
    			}
    		}
    	}

    	if (is_trust_boundary_reached == 1)
    	{
    	    print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations, trust bound REACHED");
    	} else {
    	    print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations");
    	}
    	print ("   -- Obj.Reduction:  Actual = " + actred + ",  Predicted = " + qk +
    	       "  (A/P: " + (round (10000.0 * rho) / 10000.0) + "),  Trust Delta = " + delta);

    	if (is_rho_accepted)
    	{
    		B = B_new;
    		P = P_new;
    		Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
    		if (intercept_status == 2) {
    		    Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
    		}
    		Grad = Grad + lambda * B;
    		norm_Grad = sqrt (sum (Grad ^ 2));
    		obj = obj_new;
    	    print ("   -- New Objective = " + obj + ",  Beta Change Norm = " + snorm + ",  Gradient Norm = " + norm_Grad);
        }
        if (iter == 1) {
           delta = min (delta, snorm);
        }

        alpha2 = obj_new - obj - gs;
        if (alpha2 <= 0) {
           alpha = sigma3;
        }
        else {
           alpha = max (sigma1, -0.5 * gs / alpha2);
        }

        if (rho < eta0) {
            delta = min (max (alpha, sigma1) * snorm, sigma2 * delta);
        }
        else {
            if (rho < eta1) {
                delta = max (sigma1 * delta, min (alpha * snorm, sigma2 * delta));
            }
            else {
                if (rho < eta2) {
                    delta = max (sigma1 * delta, min (alpha * snorm, sigma3 * delta));
                }
                else {
                    delta = max (delta, min (alpha * snorm, sigma3 * delta));
                }
            }
        }

        if (is_trust_boundary_reached == 1)
        {
            print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations, trust bound REACHED");
        } else {
            print ("-- Outer Iteration " + iter + ": Had " + (inneriter - 1) + " CG iterations");
        }
        print ("   -- Obj.Reduction:  Actual = " + actred + ",  Predicted = " + qk +
               "  (A/P: " + (round (10000.0 * rho) / 10000.0) + "),  Trust Delta = " + delta);

        if (is_rho_accepted)
        {
            B = B_new;
            P = P_new;
            Grad = t(X) %*% (P [, 1:K] - Y [, 1:K]);
            if (intercept_status == 2) {
                Grad = diag (scale_X) %*% Grad + shift_X %*% Grad [D, ];
            }
            Grad = Grad + lambda * B;
            norm_Grad = sqrt (sum (Grad ^ 2));
            obj = obj_new;
            print ("   -- New Objective = " + obj + ",  Beta Change Norm = " + snorm + ",  Gradient Norm = " + norm_Grad);
            if (fileLog != " ") {
                log_str = append (log_str, "IS_POINT_UPDATED," + iter + ",1");
                log_str = append (log_str, "GRADIENT_NORM,"    + iter + "," + norm_Grad);
            }
        } else {
            if (fileLog != " ") {
                log_str = append (log_str, "IS_POINT_UPDATED," + iter + ",0");
            }
        }

        if (fileLog != " ") {
            log_str = append (log_str, "TRUST_DELTA," + iter + "," + delta);
        }

        iter = iter + 1;
        converge = ((norm_Grad < (tol * norm_Grad_initial)) | (iter > maxiter) |
            ((is_trust_boundary_reached == 0) & (abs (actred) < (abs (obj) + abs (obj_new)) * 0.00000000000001)));
        if (converge) { print ("Termination / Convergence condition satisfied."); } else { print (" "); }
    }

    if (intercept_status == 2) {
        B_out = diag (scale_X) %*% B;
        B_out [D, ] = B_out [D, ] + t(shift_X) %*% B;
    } else {
        B_out = B;
    }
    #write (B_out, fileB, format=fmtB);

    if (sum_x > 0.0) {
        print (as.scalar(B[1,1]))
    }
    if ((sum_x > 0.0) & (sum_y > 0.0)) {
        stop = time(1)
    }
    times[ix,1] = stop-start
}

if (fileLog != " ") {
    write (log_str, fileLog);
}

times = t(times)

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit inputPath=/scratch/adclick_clean_1_sparse.parquet nodes=4 featureNames=features stub=_1 execSpark=true

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
