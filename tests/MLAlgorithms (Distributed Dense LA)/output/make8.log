
 make.py started: 2018-04-29 19:01:19 /home/ubuntu/benchmark/tests/MLAlgorithms (Distributed Dense LA)/src 


name := "SystemMLAlgs"

version := "0.1"

scalaVersion := "2.10.4"

libraryDependencies ++= Seq(
    "org.apache.spark" %% "spark-mllib" % "2.2.0" % "provided"
)

assemblyMergeStrategy in assembly := {
  case PathList("javax", "servlet", xs @ _*) => MergeStrategy.last
  case PathList("javax", "activation", xs @ _*) => MergeStrategy.last
  case PathList("org", "apache", xs @ _*) => MergeStrategy.last
  case PathList("com", "google", xs @ _*) => MergeStrategy.last
  case PathList("com", "esotericsoftware", xs @ _*) => MergeStrategy.last
  case PathList("com", "codahale", xs @ _*) => MergeStrategy.last
  case PathList("com", "yammer", xs @ _*) => MergeStrategy.last
  case "about.html" => MergeStrategy.rename
  case "META-INF/ECLIPSEF.RSA" => MergeStrategy.last
  case "META-INF/mailcap" => MergeStrategy.last
  case "META-INF/mimetypes.default" => MergeStrategy.last
  case "plugin.properties" => MergeStrategy.last
  case "log4j.properties" => MergeStrategy.last
  case x =>
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}

Running: sbt -Dsbt.log.noformat=true assembly 

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
name := "MLLibAlgs"

version := "0.1"

scalaVersion := "2.10.4"

libraryDependencies ++= Seq (
    "org.apache.spark" %% "spark-core" % "2.2.0" % "provided",
    "org.apache.spark" %% "spark-streaming" % "2.2.0" % "provided",
    "org.apache.spark" %% "spark-mllib" % "2.2.0" % "provided"
)

assemblyMergeStrategy in assembly := {
  case PathList("javax", "servlet", xs @ _*) => MergeStrategy.last
  case PathList("javax", "activation", xs @ _*) => MergeStrategy.last
  case PathList("org", "apache", xs @ _*) => MergeStrategy.last
  case PathList("com", "google", xs @ _*) => MergeStrategy.last
  case PathList("com", "esotericsoftware", xs @ _*) => MergeStrategy.last
  case PathList("com", "codahale", xs @ _*) => MergeStrategy.last
  case PathList("com", "yammer", xs @ _*) => MergeStrategy.last
  case "about.html" => MergeStrategy.rename
  case "META-INF/ECLIPSEF.RSA" => MergeStrategy.last
  case "META-INF/mailcap" => MergeStrategy.last
  case "META-INF/mimetypes.default" => MergeStrategy.last
  case "plugin.properties" => MergeStrategy.last
  case "log4j.properties" => MergeStrategy.last
  case x =>
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}

Running: sbt -Dsbt.log.noformat=true assembly 

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
# Copyright 2018 Anthony H Thomas and Arun Kumar
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import sys

sys.path.append('../external/lib/python')
import make_utils as utils
import global_params as params
import gen_data as data

stub = sys.argv[1]
nodes = sys.argv[2]
msize = sys.argv[3].split(' ')
algorithms = sys.argv[4].split(' ')
systems = sys.argv[5].split(' ')

data.gen_data_disk('../temp/pass.csv', 2, 2, 2**12)
utils.hdfs_put('../temp/pass.csv')

args_R = ('mattype=tall '
          'Xpath=../external/disk_data/M{gb}_tall.csv '
          'Ypath=../external/disk_data/y{gb}_tall.csv '
          'nodes={nodes} opType={op}')
args_madlib = ('mattype=tall '
               'xTableName=M{gb}_tall '
               'yTableName=y{gb}_tall '
               'nodes={nodes} opType={op}')
args_hdfs = ('mattype=tall '
             'Xpath=/scratch/M{gb}_tall.csv '
             'Ypath=/scratch/y{gb}_tall.csv '
             'passPath=/scratch/pass.csv '
             'nodes={nodes} opType={op}')

for gb in msize:
    for alg in algorithms:
        if alg == 'logit':
            ytable_name = 'adclick_y_array{}'
        else:
            ytable_name = 'adclick_y_mat{}'
        argv = {'stub': stub,
                'nodes': nodes,
                'op': alg,
                'gb': gb}
        cmd_args_R = args_R.format(**argv)
        cmd_args_madlib = args_madlib.format(**argv)
        cmd_args_hdfs = args_hdfs.format(**argv)
       
        if 'R' in systems:
            utils.run_pbdR(program='ml_algs.R',
                           cmd_args=cmd_args_R)
        if 'SYSTEMML' in systems:
            utils.run_spark(program='SystemMLMLAlgorithms',
                            sbt_dir='./systemml',
                            driver_memory='32G',
                            cmd_args=cmd_args_hdfs)
        if 'MLLIB' in systems:
            utils.run_spark(program='SparkMLAlgorithms',
                            sbt_dir='./mllib',
                            driver_memory='32G',
                            cmd_args=cmd_args_hdfs)
        if 'MADLIB' in systems:
            utils.run_python(program='madlib_algs.py', 
                             cmd_args=cmd_args_madlib)

Running: python _run_scale_tests.py _1 8 "2 4 8 16" "reg logit gnmf robust" "SYSTEMML MLLIB"

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M2_tall.csv Ypath=/scratch/y2_tall.csv passPath=/scratch/pass.csv nodes=8 opType=reg

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M2_tall.csv Ypath=/scratch/y2_tall.csv passPath=/scratch/pass.csv nodes=8 opType=reg

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M2_tall.csv Ypath=/scratch/y2_tall.csv passPath=/scratch/pass.csv nodes=8 opType=logit

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M2_tall.csv Ypath=/scratch/y2_tall.csv passPath=/scratch/pass.csv nodes=8 opType=logit

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M2_tall.csv Ypath=/scratch/y2_tall.csv passPath=/scratch/pass.csv nodes=8 opType=gnmf

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M2_tall.csv Ypath=/scratch/y2_tall.csv passPath=/scratch/pass.csv nodes=8 opType=gnmf

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M2_tall.csv Ypath=/scratch/y2_tall.csv passPath=/scratch/pass.csv nodes=8 opType=robust

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M2_tall.csv Ypath=/scratch/y2_tall.csv passPath=/scratch/pass.csv nodes=8 opType=robust

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M4_tall.csv Ypath=/scratch/y4_tall.csv passPath=/scratch/pass.csv nodes=8 opType=reg

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M4_tall.csv Ypath=/scratch/y4_tall.csv passPath=/scratch/pass.csv nodes=8 opType=reg

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M4_tall.csv Ypath=/scratch/y4_tall.csv passPath=/scratch/pass.csv nodes=8 opType=logit

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M4_tall.csv Ypath=/scratch/y4_tall.csv passPath=/scratch/pass.csv nodes=8 opType=logit

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M4_tall.csv Ypath=/scratch/y4_tall.csv passPath=/scratch/pass.csv nodes=8 opType=gnmf

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M4_tall.csv Ypath=/scratch/y4_tall.csv passPath=/scratch/pass.csv nodes=8 opType=gnmf

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M4_tall.csv Ypath=/scratch/y4_tall.csv passPath=/scratch/pass.csv nodes=8 opType=robust

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M4_tall.csv Ypath=/scratch/y4_tall.csv passPath=/scratch/pass.csv nodes=8 opType=robust

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M8_tall.csv Ypath=/scratch/y8_tall.csv passPath=/scratch/pass.csv nodes=8 opType=reg

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M8_tall.csv Ypath=/scratch/y8_tall.csv passPath=/scratch/pass.csv nodes=8 opType=reg

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M8_tall.csv Ypath=/scratch/y8_tall.csv passPath=/scratch/pass.csv nodes=8 opType=logit

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M8_tall.csv Ypath=/scratch/y8_tall.csv passPath=/scratch/pass.csv nodes=8 opType=logit

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M8_tall.csv Ypath=/scratch/y8_tall.csv passPath=/scratch/pass.csv nodes=8 opType=gnmf

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M8_tall.csv Ypath=/scratch/y8_tall.csv passPath=/scratch/pass.csv nodes=8 opType=gnmf

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M8_tall.csv Ypath=/scratch/y8_tall.csv passPath=/scratch/pass.csv nodes=8 opType=robust

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M8_tall.csv Ypath=/scratch/y8_tall.csv passPath=/scratch/pass.csv nodes=8 opType=robust

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M16_tall.csv Ypath=/scratch/y16_tall.csv passPath=/scratch/pass.csv nodes=8 opType=reg

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M16_tall.csv Ypath=/scratch/y16_tall.csv passPath=/scratch/pass.csv nodes=8 opType=reg

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M16_tall.csv Ypath=/scratch/y16_tall.csv passPath=/scratch/pass.csv nodes=8 opType=logit

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M16_tall.csv Ypath=/scratch/y16_tall.csv passPath=/scratch/pass.csv nodes=8 opType=logit

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M16_tall.csv Ypath=/scratch/y16_tall.csv passPath=/scratch/pass.csv nodes=8 opType=gnmf

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M16_tall.csv Ypath=/scratch/y16_tall.csv passPath=/scratch/pass.csv nodes=8 opType=gnmf

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./systemml/src/main/scala/systemml_ml_algorithms.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.sysml.api.mlcontext._
import org.apache.sysml.api.mlcontext.ScriptFactory._
import org.apache.sysml.api.mlcontext.MatrixFormat._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.collection.immutable._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.{linalg => malg}
import org.apache.spark.mllib.linalg.distributed._

object SystemMLMLAlgorithms extends App {

    override def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val ml = new MLContext(sc)
        val spark = SparkSession.builder.getOrCreate()
        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        ml.setExplain(true)
        ml.setExplainLevel(MLContext.ExplainLevel.RUNTIME)

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")

        val (x, y) = dataPath match {
            case Some(path) => readParquet(path, spark)
            case None => (readMM(Xpath, spark), 
                          readMM(Ypath, spark).select("_c0"))
        }

        val pathname = dataPath match {
            case Some(path) => path
            case None => Xpath
        }

        val isSparse = pathname.contains("sparse")
        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"systemml${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base
        if (!File(path).exists) {
            File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val meta_stub = dataPath match {
            case Some(path) => "data/Criteo (Build Derived Tables)"
            case None => "data/SimpleMatrixOps (Disk Data)"
        }

        val dtype_x = dataPath match {
            case Some(path) => DF_VECTOR
            case None => DF_DOUBLES
        }        

        val name = pathname.split("/").last.replace("parquet","csv")
        val meta_path = s"${root}/${meta_stub}/output/${name}.mtd"
        val (rows,cols,nnz) = parse_meta(meta_path)
        val x_meta = new MatrixMetadata(dtype_x, rows, cols, nnz)
        val y_meta = new MatrixMetadata(DF_DOUBLES, rows, 1, nnz)

        val call = opType match {
            case "logit"  => "logit(X, y, 10)"
            case "gnmf"   => "gnmf(X, 10, 10)"
            case "reg"    => "reg(X, y)"
            case "robust" => "robust_se(X, r2)"
            case "pca"    => "pca(X, 5)"
            case _        => ""
        }

        val print_op = opType match {
           case "gnmf" => ""
           case _      => "res = utils::printRandElements(tmp, 10)"
        }

        val preprocess_string = opType match {
            case "robust" => 
                """
                | b = reg(X,y)
                | y_hat = X %*% b
                | r2 = (y - y_hat)^2
                """.stripMargin
            case _        => ""
        }

        val libDir = root + "/lib/dml"
        val dmlText =
          s"""
            | setwd('${libDir}')
            | source('utils.dml') as utils
            |
            | p = sum( X )
            | q = sum( y )
            | print(p)
            | print(q)
            |
            | ${preprocess_string}
            |
            | times = matrix(0.0, rows = 5, cols = 1)
            | for (ix in 1:5) {
            |   if ((p > 0) & (q > 0)) {
            |       start = utils::time(1)
            |   }
            |   tmp = ${call}
            |   ${print_op}
            |   if ((p > 0) & (q > 0)) {
            |       stop = utils::time(1)
            |   }
            |   times[ix,1] = (stop - start) / 1000
            | }
            | times = t(times)
            |
            | logit = function(matrix[double] X, 
            |                  matrix[double] y, 
            |                  Integer iterations)
            |     return (matrix[double] w) {
            |
            |     N = nrow(X)
            |     w = matrix(0, rows=ncol(X), cols=1)
            |     iteration = 0
            |     stepSize = 10
            |
            |     while (iteration < iterations) {
            |         xb = X %*% w
            |         delta = 1/(1+exp(-xb)) - y
            |         stepSize = stepSize / 2
            |         w = w - ((stepSize * t(X) %*% delta)/N)
            |
            |         iteration = iteration+1
            |     }
            | }
            |
            | gnmf = function(matrix[double] X, Integer r, Integer iterations)
            |     return (integer iteration) {
            |     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
            |     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')
            |
            |     for (i in 1:3) {
            |         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
            |         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
            |     }
            |     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
            |         print(as.scalar(H[1,1]))
            |         print(as.scalar(W[1,1]))
            |     }
            |
            |     iteration = 0
            | }
            |
            | reg = function(matrix[double] X, matrix[double] y)
            |     return (matrix[double] b) {
            |     b = solve(t(X) %*% X, t(X) %*% y)
            | }
            |
            | robust_se = function(matrix[double] X, 
            |                      matrix[double] r2) 
            |     return (matrix[double] se) {
            |     # NOTE: SVD is cheap since XTX is small!
            |     [U,H,V] = svd( t(X) %*% X )
            |     h = diag( H )
            |     XTX_INV = U %*% diag(h^-1) %*% t(V)
            |     S = diag( r2 )
            |     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
            | }
            |            
            | pca = function(matrix[double] X, Integer k) 
            |   return (matrix[double] PRJ) {
            |     N = nrow( X )
            |     K = ncol( X )
            |     XS = X - colMeans( X )
            |     S = (1/(N-1))*(t( XS ) %*% XS)
            |     [eigvals, eigvects] = eigen( S )
            |     
            |     # Thanks to the Sysml implementation for this helpful bit 
            |     # of code to sort the eigenvectors
            |
            |     eigssorted = order(target=eigvals,by=1, 
            |                        decreasing=TRUE,
            |                        index.return=TRUE)
            |     diagmat = table(seq(1,K), eigssorted)
            |     eigvals = diagmat %*% eigvals
            |     eigvects = eigvects %*% diagmat
            |     eigvects = eigvects[,1:k]
            |
            |     PRJ = XS %*% eigvects
            | }
        """.stripMargin
        println("Running DML: ")
        println(dmlText)
        val script = dml(dmlText).in(Seq(("X", x, x_meta),
                                         ("y", y, y_meta))).out("times")
        val res = ml.execute(script)
        val results = res.getTuple[Matrix]("times")._1.to2DDoubleArray

        File(path).appendAll(
            nodes + "," + results(0).mkString(",") + '\n')
    }

    def parse_meta(path: String) : (Long, Long, Long) = {
        val meta = Source.fromFile(path).
            mkString.replace("{","").
                replace("}","").
                replace(" ","").
                split(",\n").map(x => x.split(":")).
                map({case Array(x,y) => (x -> y)}).toMap
        return (meta("\"rows\"").toLong,
                meta("\"cols\"").toLong,
                meta("\"nnz\"").toLong)
    }

    def readMM(path: String, spark: SparkSession) : DataFrame = {
        val M = spark.read.option("header", false).
            option("inferSchema",true).csv(path)
        M.persist(MEMORY_AND_DISK_SER)
        M.count
        return M
    }

    def readParquet(path: String, spark: SparkSession) :
        (DataFrame, DataFrame) = {
        val M = spark.read.parquet(path)
        M.persist(MEMORY_AND_DISK_SER)

        val X = M.select("dense_features_scaled")
        val y = M.select("y")
        return (X,y)
    }

}

Running: spark-submit --class SystemMLMLAlgorithms  --driver-memory 32G   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M16_tall.csv Ypath=/scratch/y16_tall.csv passPath=/scratch/pass.csv nodes=8 opType=robust

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_ml_algs.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import breeze.{math => bMath,
               numerics => bNum,
               linalg => bAlg}
import breeze.linalg.{Vector => bVector,
                      Matrix => bMatrix,
                      SparseVector => bSparseVector,
                      DenseVector => bDenseVector,
                      CSCMatrix => bSparseMatrix,
                      DenseMatrix => bDenseMatrix}
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.ml.{linalg => alg}
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkMLAlgorithms {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibMatrixOps")
        val sc = new SparkContext(conf)
        val spark = SparkSession.builder.getOrCreate()

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val opType = argMap("opType")
        val Xpath = argMap("Xpath")
        val Ypath = argMap("Ypath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val mattype = argMap("mattype")
        val dataPath = argMap.get("dataPath")       

        val stub = "/tests/MLAlgorithms (Distributed Dense LA)/output/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,time1,time2,time3,time4,time5\n")
        }

        val (x, y) = dataPath match {
            case Some(path) => parquet_to_irm(path, spark)
            case None => (readMM(Xpath, sc), readMM(Ypath, sc))
        }

        val r2: IndexedRowMatrix = opType match {
            case "robust" => compute_r2( x, y )
            case _        => readMM(passPath, sc)
        }

        val xr : RowMatrix = opType match {
            case "pca" => x.toRowMatrix
            case _     => random_matrix(10, 10, 10, 10, sc).
                            toIndexedRowMatrix.
                            toRowMatrix
        }
        xr.rows.persist(MEMORY_AND_DISK_SER)
        xr.rows.count

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            println(s"Test: ${ix}")
            val start = System.nanoTime()

            if (opType == "logit")
                logit(x, y, 3, sc)
            if (opType == "gnmf")
                gnmf(x, 10, 3, sc)
            if (opType == "reg")
                reg(x, y)
            if (opType == "robust")
                robust_se(x, r2)
            if (opType == "pca")
                pca(xr, 5)

            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        File(path).appendAll(
            nodes + "," + times.mkString(",") + "\n")
    }

    def compute_r2(x: IndexedRowMatrix, y: IndexedRowMatrix) : 
        IndexedRowMatrix = {
           val b = reg(x, y)
           val y_hat = x.multiply( b )
           val r2 = elem_pow( elem_subtract( y, y_hat ), 2.0 )
           return r2 
        }

    def readMM(path: String, sc: SparkContext) :
            IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }

    def parquet_to_irm(path: String, spark: SparkSession) :
        (IndexedRowMatrix, IndexedRowMatrix) = {
        val df = spark.read.parquet(path)
        df.persist(MEMORY_AND_DISK_SER)

        val X_rows = df.repartition(1000).
            select("dense_features_scaled").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      DenseVector.fromML(
                                          tup._1.getAs[alg.Vector](0).toDense)))
        val X = new IndexedRowMatrix(X_rows)
        X.rows.persist(MEMORY_AND_DISK_SER)

        val y_rows = df.repartition(1000).
            select("y").rdd.zipWithIndex().
            map(tup => new IndexedRow(tup._2.toInt,
                                      Vectors.dense(tup._1.getAs[Int](0))))
        val y = new IndexedRowMatrix(y_rows)
        y.rows.persist(MEMORY_AND_DISK_SER)
        return (X, y)
    }

    def logit(X: IndexedRowMatrix, y: IndexedRowMatrix,
              max_iter: Int = 3, sc: SparkContext) : Matrix = {
        var iteration = 0
        var step_size = 0.001
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var w = Matrices.rand(K, 1, new Random())
        val XT = X.toBlockMatrix(1024,X.numCols.toInt).transpose
        XT.persist(MEMORY_AND_DISK_SER)

        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            val xb = X.multiply( w )
            val gg = new IndexedRowMatrix(
                xb.rows.map(row => new IndexedRow(
                    row.index, from_breeze(
                        bNum.sigmoid(as_breeze(row.vector))))
            ))
            val eps = elem_subtract(gg, y).toBlockMatrix(XT.colsPerBlock,1)
            val XTe = XT.multiply( eps, 500 ).toLocalMatrix
            val w_update = (step_size/N.toDouble)*as_breeze( XTe ) 
            w = from_breeze( as_breeze( w ) :- w_update )
            step_size /= 2.0
            iteration += 1
        }

        return w
    }

    def vectorize(M: IndexedRowMatrix) : Matrix = {
        val loc = M.toCoordinateMatrix.toBlockMatrix()
        return loc.toLocalMatrix()
    }

    // Spark annoyingly does not expose any of these primitive methods
    // which makes their library not very useful
    def as_breeze(v: linalg.Vector) : bVector[Double] = v match {
        case v: linalg.SparseVector =>
            return new bSparseVector[Double](v.indices, v.values, v.size)
        case v: linalg.DenseVector  =>
            return new bDenseVector[Double](v.values)
    }

    def from_breeze(v: bVector[Double]) : linalg.Vector = v match {
        case v: bSparseVector[Double] =>
            return Vectors.sparse(v.length, v.activeIterator.toSeq)
        case v: bDenseVector[Double] =>
            return Vectors.dense(v.data)
    }

    def as_breeze(m: linalg.Matrix) : bMatrix[Double] = m match {
        case m: linalg.DenseMatrix =>
            return new bDenseMatrix(m.numRows, m.numCols, m.toArray)
        case m: linalg.SparseMatrix =>
            return new bSparseMatrix(
                m.values, m.numRows, m.numCols,
                m.colPtrs, m.numActives, m.rowIndices)
    }

    def from_breeze(m: bMatrix[Double]) : linalg.Matrix = m match {
        case m: bDenseMatrix[Double] =>
            return Matrices.dense(m.rows, m.cols, m.toDenseMatrix.data)
        case m: bSparseMatrix[Double] =>
            return Matrices.sparse(m.rows, m.cols,
                    m.colPtrs, m.rowIndices, m.data)
    }

    def elem_subtract(A: IndexedRowMatrix,
                      B: IndexedRowMatrix) : IndexedRowMatrix = {
        val both = join_row_matrices(A,B)
        val AB = both.map(tup => new IndexedRow(tup._1,
            from_breeze(as_breeze(tup._2._1) :- as_breeze(tup._2._2)))
        )
        return new IndexedRowMatrix(AB)
    }

    def transpose_row_matrix(A: IndexedRowMatrix) : IndexedRowMatrix = {
        // hacky way to transpose an IRM
        return A.toCoordinateMatrix.transpose.toIndexedRowMatrix
    }

    def join_row_matrices(A: IndexedRowMatrix,
                          B: IndexedRowMatrix) :
            RDD[(Long, (linalg.Vector,linalg.Vector))] = {
        val pair_A = A.rows.map(row => (row.index, row.vector))
        val pair_B = B.rows.map(row => (row.index, row.vector))
        return pair_A.join(pair_B)
    }

    def scalar_multiply(v: Double, M: IndexedRowMatrix) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(v*as_breeze(row.vector))))
        return new IndexedRowMatrix(rows_rdd)
    }

    def reg(X: IndexedRowMatrix, y: IndexedRowMatrix) : Matrix = {
        val XTX = X.computeGramianMatrix()
        val XTY = X.toBlockMatrix(1024,X.numCols.toInt).transpose.
                    multiply( y.toBlockMatrix(1024,1), 500 ).
                    toLocalMatrix
        val b = from_breeze( to_dense( as_breeze( XTX ) ) \
                             to_dense( as_breeze( XTY ) ) )
        return b
    }

    def to_dense(M: bMatrix[Double]) : bDenseMatrix[Double] = {
        return M.asInstanceOf[bDenseMatrix[Double]]
    }

    def robust_se(X: IndexedRowMatrix, 
                  r2: IndexedRowMatrix) : Matrix = {
        val XTX_INV = from_breeze( bAlg.inv( 
            to_dense( as_breeze( X.computeGramianMatrix() ) ) ) 
        ).asInstanceOf[DenseMatrix]
        val XB = X.toBlockMatrix(1024,X.numCols.toInt)
        val S = diagonalize( r2 )
        val SW = XB.transpose.multiply( S, 500 ).multiply( XB, 500 ).
                    toLocalMatrix
        return XTX_INV.multiply( SW.asInstanceOf[DenseMatrix] ).multiply( XTX_INV )
    }

    def pca(X: RowMatrix, k: Int) : RowMatrix = {
        val S = X.computeCovariance()
        val eigs = bAlg.eigSym( to_dense( as_breeze( S ) ) )
        val eigenvectors = eigs.
                eigenvectors(::,bAlg.argsort(eigs.eigenvalues).reverse)
        val U = from_breeze( eigenvectors(::,0 to k-1).toDenseMatrix )
        val PRJ = X.multiply( U ) // note: we really should subtract the mean
        return PRJ
    }

    def elem_pow(M: IndexedRowMatrix, v: Double) : IndexedRowMatrix = {
        val rows_rdd = M.rows.map(row => new IndexedRow(row.index,
                from_breeze(bNum.pow(as_breeze(row.vector), v))))
        return new IndexedRowMatrix(rows_rdd)
    }

    // converts a distributed vector (as an IRM) to a sparse BlockMatrix
    def diagonalize(M: IndexedRowMatrix) : BlockMatrix = {
        val elements = M.rows.map(row =>
            new MatrixEntry(row.index, 
                            row.index,
                            row.vector.toArray(0)))
        return new CoordinateMatrix(elements).toBlockMatrix
    }

    def gnmf(Xr: IndexedRowMatrix, r: Int, max_iter: Int, sc: SparkContext) :
            (BlockMatrix, BlockMatrix) = {
        val X = Xr.toBlockMatrix(100, 100)
        X.blocks.persist(MEMORY_AND_DISK_SER)
        val RNG = new Random()
        val N = X.numRows.toInt
        val K = X.numCols.toInt
        var W = random_matrix(N, r, 100, 100, sc)
        var H = random_matrix(r, K, 100, 100, sc)

        W.blocks.persist(MEMORY_AND_DISK_SER)
        H.blocks.persist(MEMORY_AND_DISK_SER)

        var iteration = 0
        while (iteration < max_iter) {
            println(s"Iteration => ${iteration}")
            W = elem_multiply(W, elem_divide( X.multiply( H.transpose, 500 ),
                              W.multiply( H.multiply( H.transpose, 500 ), 500)))
            H = elem_multiply(H, elem_divide( W.transpose.multiply( X, 500 ),
                              (W.transpose.multiply( W, 500 ).multiply( H, 500 ))))
            iteration = iteration + 1
        }

        return (W, H)
    }

    def random_matrix(N: Int, K: Int,
                      r: Int, c: Int, sc: SparkContext) : BlockMatrix = {
        val MM = new IndexedRowMatrix(
            normalVectorRDD(sc, N.toLong, K).zipWithIndex().map(
                tup => new IndexedRow(tup._2, tup._1))).toBlockMatrix(r, c)
        return MM
    }

    def join_block_matrices(A: BlockMatrix,
                            B: BlockMatrix) :
            RDD[((Int, Int), (linalg.Matrix,linalg.Matrix))] = {
        val pair_A = A.blocks.map(block => block)
        val pair_B = B.blocks.map(block => block)
        return pair_A.join(pair_B)
    }

    def elem_divide(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => ((block._1._1, block._1._2),
            from_breeze( as_breeze( block._2._1 ) :/ as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }

    def elem_multiply(A: BlockMatrix, B: BlockMatrix) : BlockMatrix = {
        val both = join_block_matrices( A, B )
        val new_blocks = both.map(block => (block._1,
            from_breeze( as_breeze( block._2._1 ) :* as_breeze( block._2._1 ))))
        return new BlockMatrix(new_blocks, A.rowsPerBlock, B.rowsPerBlock)
    }
}


Running: spark-submit --class SparkMLAlgorithms  --driver-memory 32G   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar mattype=tall Xpath=/scratch/M16_tall.csv Ypath=/scratch/y16_tall.csv passPath=/scratch/pass.csv nodes=8 opType=robust

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds

 make.py ended: 2018-04-30 02:43:47
