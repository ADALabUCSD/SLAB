Cleared: ../temp
Cleared: ../external

Start log file:  ../output/make.log
Running: sbt -Dsbt.log.noformat=true assembly 
[warn] Executing in batch mode.
[warn]   For better performance, hit [ENTER] to switch to interactive mode, or
[warn]   consider launching sbt without any commands, or explicitly passing 'shell'
[info] Loading project definition from /home/ubuntu/benchmark/tests/MLAlgorithms (Single Node Dense)/src/systemml/project
[info] Set current project to SystemMLAlgs (in build file:/home/ubuntu/benchmark/tests/MLAlgorithms%20(Single%20Node%20Dense)/src/systemml/)
[info] Including from cache: SystemML.jar
[info] Including from cache: scala-library-2.10.4.jar
[info] Checking every *.class/*.jar file's SHA-1.
[info] Merging files...
[warn] Merging 'META-INF/DEPENDENCIES' with strategy 'discard'
[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'
[warn] Merging 'META-INF/maven/org.antlr/antlr4-runtime/pom.properties' with strategy 'discard'
[warn] Merging 'META-INF/maven/org.antlr/antlr4-runtime/pom.xml' with strategy 'discard'
[warn] Merging 'META-INF/maven/org.apache.systemml/systemml/pom.properties' with strategy 'discard'
[warn] Merging 'META-INF/maven/org.apache.systemml/systemml/pom.xml' with strategy 'discard'
[warn] Strategy 'discard' was applied to 6 files
[info] Assembly up to date: /home/ubuntu/benchmark/tests/MLAlgorithms (Single Node Dense)/src/systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar
[success] Total time: 2 s, completed Jan 28, 2018 5:02:26 AM
Running: sbt -Dsbt.log.noformat=true assembly 
[warn] Executing in batch mode.
[warn]   For better performance, hit [ENTER] to switch to interactive mode, or
[warn]   consider launching sbt without any commands, or explicitly passing 'shell'
[info] Loading project definition from /home/ubuntu/benchmark/tests/MLAlgorithms (Single Node Dense)/src/mllib/project
[info] Set current project to MLLibAlgs (in build file:/home/ubuntu/benchmark/tests/MLAlgorithms%20(Single%20Node%20Dense)/src/mllib/)
[info] Including from cache: scala-library-2.10.4.jar
[info] Checking every *.class/*.jar file's SHA-1.
[info] Merging files...
[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'
[warn] Strategy 'discard' was applied to a file
[info] Assembly up to date: /home/ubuntu/benchmark/tests/MLAlgorithms (Single Node Dense)/src/mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar
[success] Total time: 1 s, completed Jan 28, 2018 5:02:34 AM
Running: taskset -c 0 python _np_algs.py opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0 python _tf_algorithms.py opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


Running: taskset -c 0 Rscript ml_algs.R opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
Warning message:
In write.table(runTimes, path, append = TRUE, row.names = FALSE,  :
  appending column names to file
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 1 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
Running: taskset -c 0 spark-submit --class SystemMLMLAlgorithms  --driver-cores 1   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
18/01/28 05:09:11 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:09:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:09:16 WARN MLContext: Project information not available
18/01/28 05:09:16 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = reg(X, y)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 05:09:17 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 05:09:17 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 05:09:17 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 05:09:17 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
5.0000390255624324E8
2000515.0
0.0035511248519620813
0.004202544756904626
0.003974903205943284
0.004202544756904626
0.004351384816053448
0.004681346475079676
0.004674420400254119
0.004805727587992829
0.004118863998564085
0.0034908675837408342
0.004786196832931115
0.004092563386095965
0.004336984271215204
0.004117522435664681
0.0038189394484511073
0.0043020654691284434
0.004117522435664681
0.0038443697395710466
0.0037815808895807538
0.004357531732841842
0.003990653383656299
0.0037904936446021063
0.003728082363754866
0.004515862219893126
0.0037634435085984776
0.0034908675837408342
0.003910105354633732
0.002863953714240241
0.003515582188633362
0.0038259969997532805
0.0038189394484511073
0.0035707755392197582
0.0038443697395710466
0.00438280019429032
0.003990653383656299
0.004489044009324001
0.004336984271215204
0.0037815808895807538
0.004223237063916463
0.003692609787565965
0.003531944118474959
0.003513800009096398
0.003302645323476933
0.003947937645500403
0.003728082363754866
0.004306681802587162
0.004223237063916463
0.004681835425000393
0.00458663850992186
0.003588327365475726
SystemML Statistics:
Total execution time:		248.071 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 1 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
Running: taskset -c 0 spark-submit --class SparkMLAlgorithms  --driver-cores 1   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
18/01/28 05:13:33 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:13:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:13:37 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-1 python _np_algs.py opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-1 python _tf_algorithms.py opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


Running: taskset -c 0-1 Rscript ml_algs.R opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 2 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
Running: taskset -c 0-1 spark-submit --class SystemMLMLAlgorithms  --driver-cores 2   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
18/01/28 05:21:23 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:21:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:21:25 WARN MLContext: Project information not available
18/01/28 05:21:25 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = reg(X, y)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 05:21:26 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 05:21:26 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 05:21:26 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 05:21:26 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
4.999974338587517E8
2001197.0
0.003923937046592962
0.0034022465389833426
0.004024301840774726
0.0034394192187477404
0.003666485041894742
0.004024301840774726
0.003721945461965001
0.004434415648370867
0.003730096762076448
0.0034022465389833426
0.004247521092468265
0.004293138655564188
0.0034394192187477404
0.003887507261586185
0.0040045557314512936
0.004970920542132806
0.003806615384748898
0.004075787899744548
0.004377741702743465
0.004278293113399335
0.003222752949877428
0.004248614023185227
0.003205594852288444
0.003897185498070615
0.003774146357584731
0.004075787899744548
0.004493344995616558
0.004027447837112289
0.004191639302552525
0.004191639302552525
0.004345411030120406
0.0041564840036770094
0.003222752949877428
0.0035942117734562708
0.0044987556296751585
0.004275157348036739
0.0041052016274609575
0.0044870298848178536
0.00370494941075376
0.004294699596605293
0.004422900990080357
0.0041564840036770094
0.004246378108026008
0.0045983483733773685
0.00274650285081825
0.004191639302552525
0.00360742029384215
0.004275157348036739
0.004234997106922641
0.0042496148463368405
SystemML Statistics:
Total execution time:		140.935 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 2 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
Running: taskset -c 0-1 spark-submit --class SparkMLAlgorithms  --driver-cores 2   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
18/01/28 05:23:53 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:23:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:23:56 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-3 python _np_algs.py opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-3 python _tf_algorithms.py opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


Running: taskset -c 0-3 Rscript ml_algs.R opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 4 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
Running: taskset -c 0-3 spark-submit --class SystemMLMLAlgorithms  --driver-cores 4   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
18/01/28 05:30:00 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:30:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:30:04 WARN MLContext: Project information not available
18/01/28 05:30:04 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = reg(X, y)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 05:30:05 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 05:30:05 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 05:30:05 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 05:30:05 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
5.000065380741274E8
1999691.0
0.004800737734599578
0.004271921521688191
0.0037633972337969223
0.0037036235868506376
0.003785736856125394
0.00328504968689123
0.003965566267723515
0.0037633972337969223
0.0033581237969203567
0.004283142678444314
0.0034965160779491526
0.0037858327825107388
0.003574078715944828
0.004076474100064791
0.0043979568345050065
0.00394269806645434
0.004060288344002833
0.0043979568345050065
0.004066307064003745
0.0039035147396025917
0.0039058839536086796
0.003738566843029818
0.003779626266817546
0.0040876890101690586
0.004032191066716172
0.003955439171770404
0.003916700570871263
0.0040040271586316645
0.004585707617810924
0.004066307064003745
0.003681958406229357
0.00336845937371247
0.004283142678444314
0.004162027892614509
0.004283612966126044
0.0039035147396025917
0.00398009741611257
0.0047367372952870765
0.0041472543713317175
0.0040233931040443505
0.004032191066716172
0.003955375656023751
0.003980420434314375
0.0040040271586316645
0.003681958406229357
0.003730065082393179
0.0037858327825107388
0.0037689671708264565
0.0043979568345050065
0.00328504968689123
SystemML Statistics:
Total execution time:		77.385 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 4 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
Running: taskset -c 0-3 spark-submit --class SparkMLAlgorithms  --driver-cores 4   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
18/01/28 05:31:28 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:31:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:31:31 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-7 python _np_algs.py opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-7 python _tf_algorithms.py opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


Running: taskset -c 0-7 Rscript ml_algs.R opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 8 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
Running: taskset -c 0-7 spark-submit --class SystemMLMLAlgorithms  --driver-cores 8   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
18/01/28 05:36:57 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:36:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:36:59 WARN MLContext: Project information not available
18/01/28 05:36:59 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = reg(X, y)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 05:37:00 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 05:37:00 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 05:37:00 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 05:37:00 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
5.0000372822647965E8
2000594.0
0.003977428325669287
0.003494533741091012
0.004119757632785837
0.0035917630519675353
0.003906610288795556
0.004538445079346519
0.0034037487905712407
0.004169167896202597
0.0040536771663115916
0.003079516403671097
0.00426282028164358
0.003229507020075055
0.0031353322555491116
0.0038479096626864253
0.004204507660510714
0.0040536771663115916
0.0040536771663115916
0.004034714741956815
0.003521936907116725
0.004043679186265486
0.004054474672583105
0.00426228662813087
0.003079516403671097
0.004197869051424424
0.004699512250819131
0.0043555661843953835
0.003656401181646494
0.0034440979310898286
0.004204507660510714
0.004518222563042399
0.004326983573952936
0.004441700449321535
0.0038157671480267994
0.0035407190028391607
0.0037320167764080675
0.003521936907116725
0.0031496505561924328
0.004628365240013352
0.0041574208308093104
0.0034037487905712407
0.003377147831936644
0.004678663200833801
0.003587665655098287
0.0036472037498225837
0.003651792912862628
0.004330549840001916
0.0033449331325748725
0.003954787436999409
0.003521936907116725
0.004304177295788641
SystemML Statistics:
Total execution time:		46.807 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 8 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
Running: taskset -c 0-7 spark-submit --class SparkMLAlgorithms  --driver-cores 8   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
18/01/28 05:37:50 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:37:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:37:52 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-15 python _np_algs.py opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-15 python _tf_algorithms.py opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


Running: taskset -c 0-15 Rscript ml_algs.R opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 16 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
Running: taskset -c 0-15 spark-submit --class SystemMLMLAlgorithms  --driver-cores 16   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
18/01/28 05:43:40 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:43:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:43:44 WARN MLContext: Project information not available
18/01/28 05:43:44 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = reg(X, y)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 05:43:44 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 05:43:44 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 05:43:44 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 05:43:44 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
4.999991739238473E8
1997845.0
0.004618602375504396
0.00424041441249952
0.003941911592065761
0.0038387285366842288
0.003502135016974281
0.0044772462355882675
0.0034827860282963512
0.003855212374290846
0.003834259243265993
0.004501737082628438
0.0035474961750675124
0.0038960486022555562
0.003299868075912889
0.0035085078030831227
0.004335332462290799
0.004333989129432782
0.0038536405329106837
0.003959051721687678
0.0038163756168449257
0.003410643658402585
0.0044772462355882675
0.003983249406881715
0.004487231812425478
0.004452602383655721
0.003299868075912889
0.003674944773150275
0.0032628749810618947
0.00371498887161977
0.003860532265394582
0.003629056256522456
0.0032628749810618947
0.004548116650553003
0.003987460971535505
0.0046332215179548655
0.003855212374290846
0.003987460971535505
0.003502135016974281
0.00432166574594716
0.0044772462355882675
0.0032628749810618947
0.004688022113858366
0.00440006659577217
0.0033862158543887864
0.003959051721687678
0.0037871943988994875
0.0032628749810618947
0.003502135016974281
0.0037756545449006223
0.003299868075912889
0.004618602375504396
SystemML Statistics:
Total execution time:		31.957 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 16 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
Running: taskset -c 0-15 spark-submit --class SparkMLAlgorithms  --driver-cores 16   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=reg mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
18/01/28 05:44:20 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:44:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:44:22 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0 python _np_algs.py opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0 Rscript ml_algs.R opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
Warning message:
In write.table(runTimes, path, append = TRUE, row.names = FALSE,  :
  appending column names to file
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 1 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
Running: taskset -c 0 spark-submit --class SystemMLMLAlgorithms  --driver-cores 1   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
18/01/28 05:49:30 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:49:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:49:35 WARN MLContext: Project information not available
18/01/28 05:49:35 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = logit(X, y, 10)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 05:49:37 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 05:49:37 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 05:49:37 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 05:49:37 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
5.000043212275476E8
2001148.0
-0.2502236277674625
-0.2502211865315873
-0.2506466123639878
-0.24986008072040486
-0.2502900000534005
-0.2501904183981253
-0.2507472635171902
-0.24988021391711107
-0.2500587352719753
-0.24988021391711107
-0.2500587352719753
-0.2504953196850103
-0.25028194066607945
-0.2507621790973007
-0.2504526315213129
-0.2504902230465496
-0.2501700790030437
-0.25036559894145877
-0.2501700790030437
-0.24975456991644127
-0.2502158225075119
-0.25041527031040023
-0.2502900000534005
-0.2504706186565771
-0.2500493559041639
-0.25051532452741554
-0.25080033065305707
-0.25036559894145877
-0.2502236277674625
-0.25036559894145877
-0.2502211865315873
-0.2506466123639878
-0.2507472635171902
-0.25012190519483835
-0.2509368662078166
-0.2510662900128733
-0.2502211865315873
-0.2502900000534005
-0.25037412867572506
-0.2507429815859265
-0.2500493559041639
-0.25012190519483835
-0.25057687483763547
-0.250356066988723
-0.2503230541028111
-0.2506001077786142
-0.25080622488370996
-0.2502158225075119
-0.24975456991644127
-0.25068679159893426
SystemML Statistics:
Total execution time:		209.781 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 1 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
Running: taskset -c 0 spark-submit --class SparkMLAlgorithms  --driver-cores 1   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
18/01/28 05:53:13 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:53:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:53:17 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-1 python _np_algs.py opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-1 Rscript ml_algs.R opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 2 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
Running: taskset -c 0-1 spark-submit --class SystemMLMLAlgorithms  --driver-cores 2   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
18/01/28 05:58:47 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 05:58:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 05:58:51 WARN MLContext: Project information not available
18/01/28 05:58:51 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = logit(X, y, 10)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 05:58:52 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 05:58:52 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 05:58:52 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 05:58:52 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
4.999981978601303E8
1999409.0
-0.2509926684236323
-0.25084676193037214
-0.25092058620768387
-0.2506020472801194
-0.25096739997995765
-0.25080213423026987
-0.2512084761468723
-0.2512560489901021
-0.25102072085949045
-0.2511645275217797
-0.25084676193037214
-0.2515129479833814
-0.2512840153832766
-0.25096739997995765
-0.25128721842075236
-0.25127060897986914
-0.25129820211032283
-0.25175700928691425
-0.2510462896782641
-0.25087621347459643
-0.2510462896782641
-0.251950761957258
-0.2511618817785414
-0.25170371595661445
-0.25094368110522225
-0.2511027585538749
-0.25158313987192316
-0.251514972833722
-0.2508090770146067
-0.2511618817785414
-0.2512560489901021
-0.2511618817785414
-0.2514138912674978
-0.2509196263380482
-0.2516809250672481
-0.2511645275217797
-0.25036698752291403
-0.25094615573654494
-0.2511327113986182
-0.2511342407309656
-0.2507841844333329
-0.25139683276447256
-0.2511327113986182
-0.25127060897986914
-0.2509196263380482
-0.2512840153832766
-0.25124667514207333
-0.2508620642169277
-0.2514412542371837
-0.2518611885204341
SystemML Statistics:
Total execution time:		125.369 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 2 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
Running: taskset -c 0-1 spark-submit --class SparkMLAlgorithms  --driver-cores 2   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
18/01/28 06:01:01 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 06:01:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 06:01:03 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-3 python _np_algs.py opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-3 Rscript ml_algs.R opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 4 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
Running: taskset -c 0-3 spark-submit --class SystemMLMLAlgorithms  --driver-cores 4   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
18/01/28 06:05:10 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 06:05:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 06:05:14 WARN MLContext: Project information not available
18/01/28 06:05:14 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = logit(X, y, 10)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 06:05:14 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 06:05:14 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 06:05:14 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 06:05:14 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
4.999955759786253E8
2000712.0
-0.25054465886253313
-0.25109469799072337
-0.25028966977175965
-0.25061369898089786
-0.25091456118417044
-0.25070708898720045
-0.2501135992402328
-0.2503987435292013
-0.2503381242292486
-0.2505361024692826
-0.25040392464205297
-0.25077800952745377
-0.2502342636925256
-0.2504429895466566
-0.25086117435130784
-0.25051085823290015
-0.25024460508096336
-0.25060580999096965
-0.25009114310750064
-0.2507855289219478
-0.2504409231927086
-0.25034582024355606
-0.25045249995164365
-0.25084511919135205
-0.25081636755769354
-0.2504856547292204
-0.2506675075598694
-0.24992306299691558
-0.2504429895466566
-0.24970439325543253
-0.25078997166000855
-0.2504838470981995
-0.2504495711837675
-0.25051085823290015
-0.25059637671334356
-0.25070708898720045
-0.2507731494696029
-0.2504838470981995
-0.25080758210667253
-0.25034582024355606
-0.2505969921576883
-0.2508565396014138
-0.25061522200666897
-0.25034582024355606
-0.25043185278345614
-0.25060580999096965
-0.25061522200666897
-0.25061369898089786
-0.24989298890191858
-0.2503621770757246
SystemML Statistics:
Total execution time:		77.053 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 4 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
Running: taskset -c 0-3 spark-submit --class SparkMLAlgorithms  --driver-cores 4   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
18/01/28 06:06:35 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 06:06:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 06:06:37 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-7 python _np_algs.py opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-7 Rscript ml_algs.R opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 8 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
Running: taskset -c 0-7 spark-submit --class SystemMLMLAlgorithms  --driver-cores 8   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
18/01/28 06:10:04 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 06:10:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 06:10:06 WARN MLContext: Project information not available
18/01/28 06:10:06 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = logit(X, y, 10)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 06:10:07 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 06:10:07 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 06:10:07 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 06:10:07 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
4.999938056486783E8
1999890.0
-0.2512715469271907
-0.2513771826367247
-0.25044160684022676
-0.25127242091906005
-0.250769747125214
-0.25135169350023645
-0.2512739147296077
-0.2508401488883528
-0.2509520343841022
-0.25101603432678277
-0.2506182074312535
-0.25004281533135375
-0.25166161828246153
-0.25044160684022676
-0.25119705153943855
-0.25089137413070894
-0.25159977372298314
-0.2515078626063025
-0.2509978113664832
-0.251616119920144
-0.25066876607903665
-0.2507946705208883
-0.25166161828246153
-0.25109567663865945
-0.25126136952931905
-0.25115440995446536
-0.2509978113664832
-0.25109694338525657
-0.25119705153943855
-0.2507478358815136
-0.2508642202170867
-0.25159977372298314
-0.2510730301316835
-0.25119705153943855
-0.25116466939274107
-0.250804170588868
-0.2516456476092006
-0.2512362936664489
-0.2515440221743235
-0.2505382450134146
-0.2509721110982302
-0.25154548353930917
-0.2506182074312535
-0.25097920981062954
-0.25032723537156887
-0.2506301644912448
-0.2505382450134146
-0.25115855003987453
-0.25109694338525657
-0.2512380637953169
SystemML Statistics:
Total execution time:		53.835 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 8 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
Running: taskset -c 0-7 spark-submit --class SparkMLAlgorithms  --driver-cores 8   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
18/01/28 06:11:05 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 06:11:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 06:11:07 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-15 python _np_algs.py opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-15 Rscript ml_algs.R opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 16 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
Running: taskset -c 0-15 spark-submit --class SystemMLMLAlgorithms  --driver-cores 16   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
18/01/28 06:14:31 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 06:14:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 06:14:34 WARN MLContext: Project information not available
18/01/28 06:14:34 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = logit(X, y, 10)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 06:14:34 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 06:14:34 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 06:14:34 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 06:14:34 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
4.9998519794635844E8
2000353.0
-0.2506403946303432
-0.25096961418969527
-0.25058561340120217
-0.2513785990705993
-0.2508715137210729
-0.2510187914403427
-0.2507711373581204
-0.2506949676396893
-0.2505912140202959
-0.2517036053589074
-0.2510712269204951
-0.2512645233292562
-0.25089373339262244
-0.250499985853805
-0.2501055047743678
-0.2509231133719978
-0.2501055047743678
-0.25041365928041553
-0.25136228447890474
-0.2511492071182274
-0.2516139205985187
-0.2507257442935244
-0.2510712269204951
-0.2507440919190903
-0.2505549438874786
-0.2506403946303432
-0.250991792644613
-0.2510579572867916
-0.2504415706280403
-0.250812476308336
-0.2502947681210488
-0.2503294275252132
-0.25060896650146125
-0.2510579572867916
-0.25083112614284725
-0.25135498530005607
-0.25060896650146125
-0.2504025479821873
-0.2504160151787566
-0.2505549438874786
-0.2504025479821873
-0.25060879388013657
-0.2512645233292562
-0.2508004260345678
-0.2502947681210488
-0.25117678075109834
-0.2504029653893135
-0.250969983753512
-0.25110013102772255
-0.2509338077575977
SystemML Statistics:
Total execution time:		44.067 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 16 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
Running: taskset -c 0-15 spark-submit --class SparkMLAlgorithms  --driver-cores 16   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=logit mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
18/01/28 06:15:22 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 06:15:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 06:15:24 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0 python _np_algs.py opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0 python _tf_algorithms.py opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


Running: taskset -c 0 Rscript ml_algs.R opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
Warning message:
In write.table(runTimes, path, append = TRUE, row.names = FALSE,  :
  appending column names to file
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 1 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
Running: taskset -c 0 spark-submit --class SystemMLMLAlgorithms  --driver-cores 1   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
18/01/28 06:24:42 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 06:24:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 06:24:47 WARN MLContext: Project information not available
18/01/28 06:24:47 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=1, cols=1)
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = gnmf(X, 10, 10)
   
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 06:24:48 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 06:24:48 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 06:24:48 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 06:24:48 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
4.9998789653804684E8
FALSE
0.708990462596394
0.03234158921289769
0.5015802863720923
0.0432814554914056
0.1900034078166151
0.08045105585911463
0.7097697173275382
0.13963587238520397
0.5012933594610018
0.1203307662447281
SystemML Statistics:
Total execution time:		419.335 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 1 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
Running: taskset -c 0 spark-submit --class SparkMLAlgorithms  --driver-cores 1   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
18/01/28 06:31:54 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 06:31:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 06:31:57 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-1 python _np_algs.py opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-1 python _tf_algorithms.py opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


Running: taskset -c 0-1 Rscript ml_algs.R opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 2 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
Running: taskset -c 0-1 spark-submit --class SystemMLMLAlgorithms  --driver-cores 2   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
18/01/28 06:44:31 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 06:44:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 06:44:33 WARN MLContext: Project information not available
18/01/28 06:44:33 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=1, cols=1)
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = gnmf(X, 10, 10)
   
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 06:44:34 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 06:44:34 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 06:44:34 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 06:44:34 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
4.999978877427886E8
FALSE
0.7057260670893932
0.1439986396517474
0.8019271317184921
0.18353783264973925
0.6391538411151647
0.20291873066896834
0.8574322661203757
0.059538841501127926
0.7155465715419093
0.12666543989389745
SystemML Statistics:
Total execution time:		297.633 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 2 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
Running: taskset -c 0-1 spark-submit --class SparkMLAlgorithms  --driver-cores 2   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
18/01/28 06:49:38 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 06:49:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 06:49:40 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-3 python _np_algs.py opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-3 python _tf_algorithms.py opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


Running: taskset -c 0-3 Rscript ml_algs.R opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 4 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
Running: taskset -c 0-3 spark-submit --class SystemMLMLAlgorithms  --driver-cores 4   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
18/01/28 07:00:54 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 07:00:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 07:00:58 WARN MLContext: Project information not available
18/01/28 07:00:58 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=1, cols=1)
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = gnmf(X, 10, 10)
   
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 07:00:58 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 07:00:58 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 07:00:58 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 07:00:58 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
4.999902527125161E8
TRUE
0.16467054047318816
0.0576462408398408
0.12600055092538648
0.025301778877362163
0.44306195080632027
0.003047149899130759
0.781359193632766
0.1340225292391074
0.6549055965362124
0.046055657985249146
SystemML Statistics:
Total execution time:		157.278 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 4 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
Running: taskset -c 0-3 spark-submit --class SparkMLAlgorithms  --driver-cores 4   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
18/01/28 07:03:41 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 07:03:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 07:03:43 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-7 python _np_algs.py opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-7 python _tf_algorithms.py opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


Running: taskset -c 0-7 Rscript ml_algs.R opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 8 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
Running: taskset -c 0-7 spark-submit --class SystemMLMLAlgorithms  --driver-cores 8   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
18/01/28 07:13:13 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 07:13:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 07:13:15 WARN MLContext: Project information not available
18/01/28 07:13:15 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=1, cols=1)
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = gnmf(X, 10, 10)
   
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 07:13:15 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 07:13:15 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 07:13:15 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 07:13:15 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
4.999907180576666E8
FALSE
0.6285315580777492
0.13818693701085938
0.7971508392128587
0.04531485117918119
0.42225375325016706
0.123957102095786
0.5023429664906783
0.0326808484503764
1.2035866765071732
0.07142716576798305
SystemML Statistics:
Total execution time:		109.651 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 8 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
Running: taskset -c 0-7 spark-submit --class SparkMLAlgorithms  --driver-cores 8   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
18/01/28 07:15:09 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 07:15:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 07:15:11 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-15 python _np_algs.py opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-15 python _tf_algorithms.py opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


Running: taskset -c 0-15 Rscript ml_algs.R opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 16 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
Running: taskset -c 0-15 spark-submit --class SystemMLMLAlgorithms  --driver-cores 16   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
18/01/28 07:24:06 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 07:24:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 07:24:08 WARN MLContext: Project information not available
18/01/28 07:24:08 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=1, cols=1)
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = gnmf(X, 10, 10)
   
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 07:24:09 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 07:24:09 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on if-else execution
18/01/28 07:24:09 WARN StatementBlock: WARNING: [line 23:7] -> stop -- Initialization of stop depends on for execution
18/01/28 07:24:09 WARN StatementBlock: WARNING: [line 18:7] -> start -- Initialization of start depends on for execution
5.0000257415308416E8
FALSE
0.6293479559173858
0.16419470538525757
0.7443559864248926
0.007497481422723077
0.523795064304398
0.08561760048608032
0.43464036851576393
0.06401132599053022
0.5244529305452356
0.17784214183116775
SystemML Statistics:
Total execution time:		95.501 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 16 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
Running: taskset -c 0-15 spark-submit --class SparkMLAlgorithms  --driver-cores 16   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=gnmf mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
18/01/28 07:25:49 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 07:25:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 07:25:51 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Test: 1
Test: 2
Test: 3
Test: 4
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0 python _np_algs.py opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0 python _tf_algorithms.py opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


2018-01-28 07:30:12.316283: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[10000000,10000000]
Traceback (most recent call last):
  File "_tf_algorithms.py", line 157, in <module>
    main(args)
  File "_tf_algorithms.py", line 59, in main
    runTimes.ix[:,1:] = utils.timeOp(call, env)
  File "/home/ubuntu/benchmark/lib/python/np_timing_utils.py", line 20, in timeOp
    res = eval(string, envr)
  File "<string>", line 1, in <module>
  File "_tf_algorithms.py", line 151, in robust_se
    res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 889, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1317, in _do_run
    options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[10000000,10000000]
	 [[Node: Diag = Diag[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_Placeholder_1_0_1)]]

Caused by op u'Diag', defined at:
  File "_tf_algorithms.py", line 157, in <module>
    main(args)
  File "_tf_algorithms.py", line 59, in main
    runTimes.ix[:,1:] = utils.timeOp(call, env)
  File "/home/ubuntu/benchmark/lib/python/np_timing_utils.py", line 20, in timeOp
    res = eval(string, envr)
  File "<string>", line 1, in <module>
  File "_tf_algorithms.py", line 146, in robust_se
    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py", line 1069, in diag
    "Diag", diagonal=diagonal, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2956, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000000,10000000]
	 [[Node: Diag = Diag[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_Placeholder_1_0_1)]]

Running: taskset -c 0 Rscript ml_algs.R opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
[1] 10000000
Error: cannot allocate vector of size 745058.1 Gb
Timing stopped at: 21.4 4.288 25.69
Execution halted
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 1 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
Running: taskset -c 0 spark-submit --class SystemMLMLAlgorithms  --driver-cores 1   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
18/01/28 07:32:42 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 07:32:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 07:32:47 WARN MLContext: Project information not available
18/01/28 07:32:47 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 
 b = reg(X,y)
 y_hat = X %*% b
 r2 = (y - y_hat)^2
                

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = robust_se(X, r2)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 07:32:48 WARN StatementBlock: WARNING: [line 27:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 07:32:48 WARN StatementBlock: WARNING: [line 22:7] -> start -- Initialization of start depends on if-else execution
18/01/28 07:32:48 WARN StatementBlock: WARNING: [line 27:7] -> stop -- Initialization of stop depends on for execution
18/01/28 07:32:48 WARN StatementBlock: WARNING: [line 22:7] -> start -- Initialization of start depends on for execution
5.000036791346718E8
2000175.0
-1.7840303381498405E-9
-1.9211276484234616E-9
-1.9665614974692342E-9
-2.1337215750640346E-9
-2.17949374735605E-9
-1.984745945561572E-9
1.9033186654675064E-7
-1.88801705989305E-9
-1.902713032140902E-9
-1.9282136430983046E-9
-1.7006309082574515E-9
-1.8439751396236507E-9
-1.9013970653298376E-9
-1.8920963078433466E-9
-1.7441237541691525E-9
-1.945766561265156E-9
-2.0247123574617503E-9
-2.025367348011609E-9
-1.7851237914057102E-9
-1.8678506920465572E-9
-1.9038986330329432E-9
-1.8679995875575526E-9
-1.73834387392271E-9
-1.929136695889955E-9
-1.8257576903245749E-9
-1.9017612876627727E-9
-1.8549618160076328E-9
-1.8180006679874007E-9
-1.816514642125252E-9
-1.8846333828184443E-9
-2.063750299844155E-9
-1.94937640361106E-9
-2.1149178032660136E-9
-1.9043816516777697E-9
-1.810278246571485E-9
-1.9260639667768676E-9
-1.9577879946235323E-9
-1.945524334262959E-9
-1.85505693433321E-9
-1.968140527373417E-9
-1.759148670063121E-9
-1.8475552057630948E-9
-1.7403930766455658E-9
-1.8417183348371505E-9
-2.063750299844155E-9
-2.0367573258023795E-9
-1.8678514351387486E-9
-1.8792224746446718E-9
-1.8848823903711066E-9
-2.012272573229704E-9
SystemML Statistics:
Total execution time:		790.144 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 1 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
Running: taskset -c 0 spark-submit --class SparkMLAlgorithms  --driver-cores 1   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=1
18/01/28 07:46:04 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 07:46:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 07:46:07 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Exception in thread "main" java.lang.IllegalArgumentException: requirement failed: 10000000 x 10000000 dense matrix is too large to allocate
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.mllib.linalg.DenseMatrix$.zeros(Matrices.scala:459)
	at org.apache.spark.mllib.linalg.DenseMatrix$.diag(Matrices.scala:530)
	at org.apache.spark.mllib.linalg.Matrices$.diag(Matrices.scala:1098)
	at SparkMLAlgorithms$.robust_se(spark_ml_algs.scala:151)
	at SparkMLAlgorithms$$anonfun$main$1.apply$mcVI$sp(spark_ml_algs.scala:78)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at SparkMLAlgorithms$.main(spark_ml_algs.scala:67)
	at SparkMLAlgorithms.main(spark_ml_algs.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-1 python _np_algs.py opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-1 python _tf_algorithms.py opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


2018-01-28 07:49:11.448146: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[10000000,10000000]
Traceback (most recent call last):
  File "_tf_algorithms.py", line 157, in <module>
    main(args)
  File "_tf_algorithms.py", line 59, in main
    runTimes.ix[:,1:] = utils.timeOp(call, env)
  File "/home/ubuntu/benchmark/lib/python/np_timing_utils.py", line 20, in timeOp
    res = eval(string, envr)
  File "<string>", line 1, in <module>
  File "_tf_algorithms.py", line 151, in robust_se
    res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 889, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1317, in _do_run
    options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[10000000,10000000]
	 [[Node: Diag = Diag[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_Placeholder_1_0_1)]]

Caused by op u'Diag', defined at:
  File "_tf_algorithms.py", line 157, in <module>
    main(args)
  File "_tf_algorithms.py", line 59, in main
    runTimes.ix[:,1:] = utils.timeOp(call, env)
  File "/home/ubuntu/benchmark/lib/python/np_timing_utils.py", line 20, in timeOp
    res = eval(string, envr)
  File "<string>", line 1, in <module>
  File "_tf_algorithms.py", line 146, in robust_se
    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py", line 1069, in diag
    "Diag", diagonal=diagonal, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2956, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000000,10000000]
	 [[Node: Diag = Diag[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_Placeholder_1_0_1)]]

Running: taskset -c 0-1 Rscript ml_algs.R opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
[1] 10000000
Error: cannot allocate vector of size 745058.1 Gb
Timing stopped at: 25.7 5.32 22.46
Execution halted
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 2 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
Running: taskset -c 0-1 spark-submit --class SystemMLMLAlgorithms  --driver-cores 2   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
18/01/28 07:51:37 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 07:51:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 07:51:39 WARN MLContext: Project information not available
18/01/28 07:51:39 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 
 b = reg(X,y)
 y_hat = X %*% b
 r2 = (y - y_hat)^2
                

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = robust_se(X, r2)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 07:51:40 WARN StatementBlock: WARNING: [line 27:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 07:51:40 WARN StatementBlock: WARNING: [line 22:7] -> start -- Initialization of start depends on if-else execution
18/01/28 07:51:40 WARN StatementBlock: WARNING: [line 27:7] -> stop -- Initialization of stop depends on for execution
18/01/28 07:51:40 WARN StatementBlock: WARNING: [line 22:7] -> start -- Initialization of start depends on for execution
4.999975754235469E8
1999420.0
-1.9039359934654306E-9
-1.8610340838344438E-9
-1.9337075865577013E-9
-1.8222315630246522E-9
-1.9564369131535545E-9
-1.8189113822306785E-9
-1.921269077165181E-9
-2.06263465217479E-9
-1.8912874219126795E-9
-2.1041014831904263E-9
-1.8209221894797547E-9
-1.788628371741675E-9
-1.7460675233450773E-9
-2.091973335692741E-9
-1.8024919772478264E-9
-1.8167050580950293E-9
-2.0085178588853026E-9
-1.72526638472537E-9
-1.844514585815497E-9
-1.9016544660757983E-9
-2.0175516604754327E-9
-1.999099717478313E-9
-1.7244852395353884E-9
-1.9393643909454345E-9
-2.0301908140679354E-9
-2.1534841986432956E-9
-2.0701042970716494E-9
-1.8361765234653453E-9
-2.0555346180941206E-9
-1.6579949081061126E-9
-1.8836014641141775E-9
-1.8460524698290072E-9
-1.963192027845816E-9
-1.810122804689005E-9
1.901671067406519E-7
-1.8318936769116116E-9
-1.8727100886701178E-9
-1.9772735354549736E-9
-2.1223065330516074E-9
-1.901113818646299E-9
-1.7816057583302364E-9
-1.923033273045569E-9
-1.9255336358834544E-9
-1.911889263410822E-9
-1.8068148288454262E-9
-2.0446845951354495E-9
-2.0368985368332623E-9
-1.868178709802818E-9
-1.7621629943528096E-9
-1.825148856385336E-9
SystemML Statistics:
Total execution time:		476.981 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 2 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
Running: taskset -c 0-1 spark-submit --class SparkMLAlgorithms  --driver-cores 2   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=2
18/01/28 07:59:42 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 07:59:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 07:59:45 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Exception in thread "main" java.lang.IllegalArgumentException: requirement failed: 10000000 x 10000000 dense matrix is too large to allocate
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.mllib.linalg.DenseMatrix$.zeros(Matrices.scala:459)
	at org.apache.spark.mllib.linalg.DenseMatrix$.diag(Matrices.scala:530)
	at org.apache.spark.mllib.linalg.Matrices$.diag(Matrices.scala:1098)
	at SparkMLAlgorithms$.robust_se(spark_ml_algs.scala:151)
	at SparkMLAlgorithms$$anonfun$main$1.apply$mcVI$sp(spark_ml_algs.scala:78)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at SparkMLAlgorithms$.main(spark_ml_algs.scala:67)
	at SparkMLAlgorithms.main(spark_ml_algs.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-3 python _np_algs.py opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-3 python _tf_algorithms.py opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


2018-01-28 08:01:40.510183: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[10000000,10000000]
Traceback (most recent call last):
  File "_tf_algorithms.py", line 157, in <module>
    main(args)
  File "_tf_algorithms.py", line 59, in main
    runTimes.ix[:,1:] = utils.timeOp(call, env)
  File "/home/ubuntu/benchmark/lib/python/np_timing_utils.py", line 20, in timeOp
    res = eval(string, envr)
  File "<string>", line 1, in <module>
  File "_tf_algorithms.py", line 151, in robust_se
    res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 889, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1317, in _do_run
    options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[10000000,10000000]
	 [[Node: Diag = Diag[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_Placeholder_1_0_1)]]

Caused by op u'Diag', defined at:
  File "_tf_algorithms.py", line 157, in <module>
    main(args)
  File "_tf_algorithms.py", line 59, in main
    runTimes.ix[:,1:] = utils.timeOp(call, env)
  File "/home/ubuntu/benchmark/lib/python/np_timing_utils.py", line 20, in timeOp
    res = eval(string, envr)
  File "<string>", line 1, in <module>
  File "_tf_algorithms.py", line 146, in robust_se
    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py", line 1069, in diag
    "Diag", diagonal=diagonal, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2956, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000000,10000000]
	 [[Node: Diag = Diag[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_Placeholder_1_0_1)]]

Running: taskset -c 0-3 Rscript ml_algs.R opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
[1] 10000000
Error: cannot allocate vector of size 745058.1 Gb
Timing stopped at: 32.66 9.936 18.77
Execution halted
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 4 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
Running: taskset -c 0-3 spark-submit --class SystemMLMLAlgorithms  --driver-cores 4   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
18/01/28 08:03:59 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 08:03:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 08:04:01 WARN MLContext: Project information not available
18/01/28 08:04:01 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 
 b = reg(X,y)
 y_hat = X %*% b
 r2 = (y - y_hat)^2
                

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = robust_se(X, r2)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 08:04:01 WARN StatementBlock: WARNING: [line 27:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 08:04:01 WARN StatementBlock: WARNING: [line 22:7] -> start -- Initialization of start depends on if-else execution
18/01/28 08:04:01 WARN StatementBlock: WARNING: [line 27:7] -> stop -- Initialization of stop depends on for execution
18/01/28 08:04:01 WARN StatementBlock: WARNING: [line 22:7] -> start -- Initialization of start depends on for execution
4.9999451039453334E8
1997942.0
-1.9606888828986304E-9
-1.855047995534055E-9
-1.9094378972339102E-9
-1.8970061267921673E-9
-1.756238126617119E-9
-1.912200919776453E-9
-1.975935556423609E-9
-1.8719213753965685E-9
-1.887327355342859E-9
-2.0129496304683426E-9
-1.805675828759916E-9
-2.0352589156658875E-9
1.900939413306093E-7
-1.938134064978376E-9
-1.859924215461807E-9
-1.8385729532989953E-9
-2.0395643547184736E-9
-1.8454372483115851E-9
-1.9860083298249964E-9
-1.9232598355659115E-9
-1.920819232700908E-9
-2.033454517558948E-9
-2.032371253237847E-9
-1.917186759403595E-9
-2.0233648110166025E-9
-1.916857960110926E-9
-1.9866048009774277E-9
-1.9114236155257457E-9
-1.8399864877622513E-9
-1.7052264475957365E-9
-1.835015338209963E-9
-2.1981598395702626E-9
-1.7007006382180564E-9
-1.8268991159296198E-9
-1.9808717499584145E-9
-1.8616552456281525E-9
-1.883977829540293E-9
-1.94697710870742E-9
-1.9312317176954694E-9
-1.8415795700777882E-9
-1.6923600254535534E-9
-1.9745116312627277E-9
-1.9869734140412305E-9
-2.116273085423701E-9
-1.990231756616603E-9
-2.177564063685078E-9
-2.028718718283439E-9
-1.7056678468609043E-9
-1.9676185317029107E-9
-1.9607566575895194E-9
SystemML Statistics:
Total execution time:		266.441 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 4 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
Running: taskset -c 0-3 spark-submit --class SparkMLAlgorithms  --driver-cores 4   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=4
18/01/28 08:08:33 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 08:08:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 08:08:35 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Exception in thread "main" java.lang.IllegalArgumentException: requirement failed: 10000000 x 10000000 dense matrix is too large to allocate
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.mllib.linalg.DenseMatrix$.zeros(Matrices.scala:459)
	at org.apache.spark.mllib.linalg.DenseMatrix$.diag(Matrices.scala:530)
	at org.apache.spark.mllib.linalg.Matrices$.diag(Matrices.scala:1098)
	at SparkMLAlgorithms$.robust_se(spark_ml_algs.scala:151)
	at SparkMLAlgorithms$$anonfun$main$1.apply$mcVI$sp(spark_ml_algs.scala:78)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at SparkMLAlgorithms$.main(spark_ml_algs.scala:67)
	at SparkMLAlgorithms.main(spark_ml_algs.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-7 python _np_algs.py opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-7 python _tf_algorithms.py opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


2018-01-28 08:10:13.838693: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[10000000,10000000]
Traceback (most recent call last):
  File "_tf_algorithms.py", line 157, in <module>
    main(args)
  File "_tf_algorithms.py", line 59, in main
    runTimes.ix[:,1:] = utils.timeOp(call, env)
  File "/home/ubuntu/benchmark/lib/python/np_timing_utils.py", line 20, in timeOp
    res = eval(string, envr)
  File "<string>", line 1, in <module>
  File "_tf_algorithms.py", line 151, in robust_se
    res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 889, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1317, in _do_run
    options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[10000000,10000000]
	 [[Node: Diag = Diag[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_Placeholder_1_0_1)]]

Caused by op u'Diag', defined at:
  File "_tf_algorithms.py", line 157, in <module>
    main(args)
  File "_tf_algorithms.py", line 59, in main
    runTimes.ix[:,1:] = utils.timeOp(call, env)
  File "/home/ubuntu/benchmark/lib/python/np_timing_utils.py", line 20, in timeOp
    res = eval(string, envr)
  File "<string>", line 1, in <module>
  File "_tf_algorithms.py", line 146, in robust_se
    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py", line 1069, in diag
    "Diag", diagonal=diagonal, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2956, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000000,10000000]
	 [[Node: Diag = Diag[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_Placeholder_1_0_1)]]

Running: taskset -c 0-7 Rscript ml_algs.R opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
[1] 10000000
Error: cannot allocate vector of size 745058.1 Gb
Timing stopped at: 39.51 13.37 17.85
Execution halted
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 8 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
Running: taskset -c 0-7 spark-submit --class SystemMLMLAlgorithms  --driver-cores 8   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
18/01/28 08:12:30 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 08:12:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 08:12:33 WARN MLContext: Project information not available
18/01/28 08:12:33 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 
 b = reg(X,y)
 y_hat = X %*% b
 r2 = (y - y_hat)^2
                

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = robust_se(X, r2)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 08:12:33 WARN StatementBlock: WARNING: [line 27:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 08:12:33 WARN StatementBlock: WARNING: [line 22:7] -> start -- Initialization of start depends on if-else execution
18/01/28 08:12:33 WARN StatementBlock: WARNING: [line 27:7] -> stop -- Initialization of stop depends on for execution
18/01/28 08:12:33 WARN StatementBlock: WARNING: [line 22:7] -> start -- Initialization of start depends on for execution
4.999975187306366E8
2002382.0
-1.968015892464778E-9
-2.0334951018098746E-9
-1.8045766854421593E-9
-2.0418585557992075E-9
-1.802208985780195E-9
-1.9190383840797224E-9
-1.877603327833307E-9
-1.873455509726163E-9
-2.070622303750345E-9
-2.0247433055767582E-9
-2.062447922541598E-9
-1.9974793539078546E-9
-1.8107117378761655E-9
-1.7762632393345609E-9
-2.0876591127497268E-9
-1.6441933471609612E-9
-1.929905052340352E-9
-1.883411695171849E-9
-1.5958499265169738E-9
-1.948712932818109E-9
-1.902849699919679E-9
-1.8747413983954017E-9
-2.004372752606731E-9
-1.8870592350735698E-9
-1.820405858739677E-9
-1.952674439281672E-9
-2.051892112346166E-9
-1.9178363849986343E-9
-1.8556611014228378E-9
-1.9957580731241785E-9
-1.954528019938345E-9
-1.937291654241261E-9
-1.9401274641811195E-9
-1.8702690594542092E-9
-1.824636243521828E-9
-2.02324959998241E-9
-1.8793441132708046E-9
-1.8219496959249654E-9
-2.0238440408663287E-9
-1.8398647626785783E-9
-1.7763275500977203E-9
-1.815026510170744E-9
-2.0417239969486956E-9
-1.934251563793177E-9
-2.0482230291308377E-9
-1.98810476871395E-9
-1.955668662396467E-9
-1.9982993656524147E-9
-1.9997064071970284E-9
-1.9498322111638193E-9
SystemML Statistics:
Total execution time:		182.186 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 8 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
Running: taskset -c 0-7 spark-submit --class SparkMLAlgorithms  --driver-cores 8   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=8
18/01/28 08:15:41 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 08:15:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 08:15:43 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Exception in thread "main" java.lang.IllegalArgumentException: requirement failed: 10000000 x 10000000 dense matrix is too large to allocate
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.mllib.linalg.DenseMatrix$.zeros(Matrices.scala:459)
	at org.apache.spark.mllib.linalg.DenseMatrix$.diag(Matrices.scala:530)
	at org.apache.spark.mllib.linalg.Matrices$.diag(Matrices.scala:1098)
	at SparkMLAlgorithms$.robust_se(spark_ml_algs.scala:151)
	at SparkMLAlgorithms$$anonfun$main$1.apply$mcVI$sp(spark_ml_algs.scala:78)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at SparkMLAlgorithms$.main(spark_ml_algs.scala:67)
	at SparkMLAlgorithms.main(spark_ml_algs.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: requested CPU count exceeds number available
Setting CPU count to: 24
Running: taskset -c 0-15 python _np_algs.py opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/np_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol)
    y = np.random.rand(nrow,1).ravel() if opType != 'gnmf' else None

    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2)
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(X, y, iterations=3):
    N,K = X.shape
    w = np.random.rand(K,1).ravel()

    iteration = 0
    step_size = 0.001

    while iteration < iterations:
        xb = X.dot(w)
        delta = y - (1/1+np.exp(-xb))
        step_size /= 2
        w = w + step_size*(X.T.dot(delta)/float(N))
        iteration += 1

    return w

def gnmf(X, r, iterations=3):
    N,K = X.shape
    W = np.random.rand(N, r)
    H = np.random.rand(r, K)

    iteration = 0
    while iteration < iterations:
        W = W*((X.dot(H.T))/(W.dot(H.dot(H.T))))
        H = H*((W.T.dot(X))/((W.T.dot(W).dot(H))))
        iteration += 1

    return W,H

def reg(X,y):
    return alg.solve(X.T.dot(X), X.T.dot(y))

def robust_se(X, r2):
    XTX_INV = alg.inv(X.T.dot(X))
    return XTX_INV.dot(X.T).dot(np.diag(r2)).dot(X).dot(XTX_INV)

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)

Running: taskset -c 0-15 python _tf_algorithms.py opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
with open(__file__) as fh: print fh.read()
import os
import sys
import numpy as np
import numpy.linalg as alg
import pandas as pd
import tensorflow as tf

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from sql_cxn import SQLCxn
import np_timing_utils as utils

def main(kwargs):
    mattype = kwargs['mattype']
    opType = kwargs['opType']
    nrow = int(kwargs['nrow'])
    ncol = int(kwargs['ncol'])
    nproc = int(kwargs['nproc'])

    path = '../output/tf_{}.txt'.format(opType)
    colnames = ['nproc','time1','time2','time3','time4','time5']
    runTimes = pd.DataFrame(np.zeros((1,len(colnames))))
    runTimes.columns = colnames

    env = {
        'np': np, 'tf': tf,
        'logit_reg': logit_reg,
        'reg': reg,
        'gnmf': gnmf,
        'robust_se': robust_se
    }
    X = np.random.rand(nrow, ncol).astype(np.float32)
    if opType != 'gnmf':
        y = (np.random.rand(nrow,1) >= 0.80).astype(np.int64)
    else:
        y = None
    if opType == 'logit':
        call = 'logit_reg(X,y)'
    elif opType == 'reg':
        call = 'reg(X,y)'
    elif opType == 'gnmf':
        call = 'gnmf(X, 10)'
    elif opType == 'robust':
        b = reg(X, y)
        y_hat = X.dot(b)
        env['eps'] = np.power(y_hat, 2).ravel()
        call = 'robust_se(X, eps)'
    else:
        raise StandardError('Invalid Operation')

    env['X'] = X
    env['y'] = y
    runTimes.ix[:,'nproc'] = nproc
    runTimes.ix[:,1:] = utils.timeOp(call, env)
    writeHeader = os.path.exists(path)
    runTimes.to_csv(path, index=False, header = writeHeader, mode = 'a')

def logit_reg(Xdata, ydata, iterations = None):
    G = tf.Graph()
    with G.as_default():
        def doLogitIter(w, stepSize, iteration):
            iteration += 1
            xb = tf.matmul(X,w)
            delta = tf.subtract(1/(1+tf.exp(-xb)),y)
            stepSize /= float(4.0)
            w = w - stepSize*(tf.matmul(Xt, delta)/N)
            return (w, stepSize, iteration)

        N = Xdata.shape[0]
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)
        
        Xt = tf.transpose(X)
        w = tf.Variable(tf.zeros(shape=(Xdata.shape[1],1)))
        stepSize = 10.0
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doLogitIter, (w, stepSize, iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X: Xdata})

    return res[0]

def reg(Xdata, ydata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        y = tf.placeholder(tf.float32, shape=ydata.shape)

        b = tf.matrix_solve(
                tf.matmul(X, X, transpose_a=True),
                tf.matmul(X, y, transpose_a=True)
            )

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(b, feed_dict={X: Xdata, y: ydata})

    return res

def gnmf(Xdata, r, iterations=3):
    def doGNMFIter(W, H, iteration):
        W = tf.multiply(W, tf.div(tf.matmul(X, H, transpose_b=True),
                        tf.matmul(W, tf.matmul(H, H, transpose_b=True))))
        H = tf.multiply(H, tf.div(tf.matmul(W, X, transpose_a=True),
                        tf.matmul(tf.matmul(W, W, transpose_a=True), H)))
        iteration += 1
        return (W, H, iteration)

    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)

        W = tf.Variable(tf.random_uniform((Xdata.shape[0],r)))
        H = tf.Variable(tf.random_uniform((r, Xdata.shape[1])))
        iteration = tf.constant(0)

        cond = lambda x,y,z: tf.less(z, iterations)
        loop = tf.while_loop(cond, doGNMFIter, (W,H,iteration))

        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(loop, feed_dict={X : Xdata})
            
    return (res[0], res[1])

def robust_se(Xdata, epsdata):
    G = tf.Graph()
    with G.as_default():
        X = tf.placeholder(tf.float32, shape=Xdata.shape)
        eps = tf.placeholder(tf.float32, shape=epsdata.shape)

        XTX_INV = tf.matrix_inverse(tf.matmul(X,X,transpose_a=True))
        VAR = tf.matmul(tf.matmul(tf.matmul(
                    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
                    X), XTX_INV)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})

    return res

if __name__=='__main__':
    args = utils.parse_cmd_args(sys.argv[1:])
    main(args)


2018-01-28 08:17:17.553511: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[10000000,10000000]
Traceback (most recent call last):
  File "_tf_algorithms.py", line 157, in <module>
    main(args)
  File "_tf_algorithms.py", line 59, in main
    runTimes.ix[:,1:] = utils.timeOp(call, env)
  File "/home/ubuntu/benchmark/lib/python/np_timing_utils.py", line 20, in timeOp
    res = eval(string, envr)
  File "<string>", line 1, in <module>
  File "_tf_algorithms.py", line 151, in robust_se
    res = sess.run(VAR, feed_dict={X: Xdata, eps: epsdata})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 889, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1317, in _do_run
    options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[10000000,10000000]
	 [[Node: Diag = Diag[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_Placeholder_1_0_1)]]

Caused by op u'Diag', defined at:
  File "_tf_algorithms.py", line 157, in <module>
    main(args)
  File "_tf_algorithms.py", line 59, in main
    runTimes.ix[:,1:] = utils.timeOp(call, env)
  File "/home/ubuntu/benchmark/lib/python/np_timing_utils.py", line 20, in timeOp
    res = eval(string, envr)
  File "<string>", line 1, in <module>
  File "_tf_algorithms.py", line 146, in robust_se
    tf.matmul(XTX_INV, X, transpose_b=True), tf.diag(eps)),
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py", line 1069, in diag
    "Diag", diagonal=diagonal, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2956, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000000,10000000]
	 [[Node: Diag = Diag[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_Placeholder_1_0_1)]]

Running: taskset -c 0-15 Rscript ml_algs.R opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
[1] 10000000
Error: cannot allocate vector of size 745058.1 Gb
Timing stopped at: 64.46 20.03 15.59
Execution halted
Running: spark-submit --class SystemMLMLAlgorithms 

 --driver-cores 16 


  
./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
Running: taskset -c 0-15 spark-submit --class SystemMLMLAlgorithms  --driver-cores 16   ./systemml/target/scala-2.10/SystemMLAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
18/01/28 08:19:30 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 08:19:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 08:19:32 WARN MLContext: Project information not available
18/01/28 08:19:32 ERROR MLContext: Minimum recommended Spark version could not be determined from SystemML jar file manifest or pom.xml

Welcome to Apache SystemML!

Running DML: 

 setwd('/home/ubuntu/benchmark/lib/dml')
 source('utils.dml') as utils

 X = rand(rows=10000000, cols=100)
 rvect = rand(rows=10000000, cols=1, pdf='uniform')
 y = rvect > 0.80
 p = sum( X )
 q = sum( y )
 print(p)
 print(q)

 
 b = reg(X,y)
 y_hat = X %*% b
 r2 = (y - y_hat)^2
                

 times = matrix(0.0, rows = 5, cols = 1)
 for (ix in 1:5) {
   if ((p != 0) | (q != 0)) {
       start = utils::time(1)
   }
   tmp = robust_se(X, r2)
   res = utils::printRandElements(tmp, 10)
   if ((p != 0) | (q != 0)) {
       stop = utils::time(1)
   }
   times[ix,1] = (stop - start) / 1000
 }
 times = t(times)

 logit = function(matrix[double] X, 
                  matrix[double] y, 
                  Integer iterations)
     return (matrix[double] w) {

     N = nrow(X)
     w = matrix(0, rows=ncol(X), cols=1)
     iteration = 0
     stepSize = 10

     while (iteration < iterations) {
         xb = X %*% w
         delta = 1/(1+exp(-xb)) - y
         stepSize = stepSize / 2
         w = w - ((stepSize * t(X) %*% delta)/N)

         iteration = iteration+1
     }
 }

 gnmf = function(matrix[double] X, Integer r, Integer iterations)
     return (integer iteration) {
     W = rand(rows = nrow(X), cols = r, pdf = 'uniform')
     H = rand(rows = r, cols = ncol(X), pdf = 'uniform')

     for (i in 1:3) {
         W = W * ((X %*% t(H)) / (W %*% (H %*% t(H))))
         H = H * ((t(W) %*% X) / ((t(W) %*% W) %*% H))
     }
     if ((as.scalar(W[1,1]) >  0) & (as.scalar(H[1,1]) > 0)) {
         print(as.scalar(H[1,1]))
         print(as.scalar(W[1,1]))
     }

     iteration = 0
 }

 reg = function(matrix[double] X, matrix[double] y)
     return (matrix[double] b) {
     b = solve(t(X) %*% X, t(X) %*% y)
 }

 robust_se = function(matrix[double] X, 
                      matrix[double] r2) 
     return (matrix[double] se) {
     # NOTE: SVD is cheap since XTX is small!
     [U,H,V] = svd( t(X) %*% X )
     h = diag( H )
     XTX_INV = U %*% diag(h^-1) %*% t(V)
     S = diag( r2 )
     se = XTX_INV %*% (t(X) %*% S %*% X) %*% XTX_INV
 }
            
 pca = function(matrix[double] X, Integer k) 
   return (matrix[double] PRJ) {
     N = nrow( X )
     K = ncol( X )
     XS = X - colMeans( X )
     S = (1/(N-1))*(t( XS ) %*% XS)
     [eigvals, eigvects] = eigen( S )
     
     # Thanks to the Sysml implementation for this helpful bit 
     # of code to sort the eigenvectors

     eigssorted = order(target=eigvals,by=1, 
                        decreasing=TRUE,
                        index.return=TRUE)
     diagmat = table(seq(1,K), eigssorted)
     eigvals = diagmat %*% eigvals
     eigvects = eigvects %*% diagmat
     eigvects = eigvects[,1:k]

     PRJ = XS %*% eigvects
 }
        
18/01/28 08:19:33 WARN StatementBlock: WARNING: [line 27:7] -> stop -- Initialization of stop depends on if-else execution
18/01/28 08:19:33 WARN StatementBlock: WARNING: [line 22:7] -> start -- Initialization of start depends on if-else execution
18/01/28 08:19:33 WARN StatementBlock: WARNING: [line 27:7] -> stop -- Initialization of stop depends on for execution
18/01/28 08:19:33 WARN StatementBlock: WARNING: [line 22:7] -> start -- Initialization of start depends on for execution
5.000140971708769E8
1998653.0
-1.884474910038255E-9
-1.874411665157821E-9
-1.6793927429547499E-9
-2.004206651514943E-9
-1.7468157321156958E-9
-1.8806478462377777E-9
-1.8562516208083028E-9
-2.0124403989972663E-9
-1.685027598981999E-9
-1.7599807334122434E-9
-1.9500378703719007E-9
-1.8535555778394035E-9
-1.8911254539680827E-9
-2.1216237443812615E-9
-1.8851229225369923E-9
-2.0820034169697727E-9
-1.860741867085597E-9
-1.9817328948349073E-9
-1.706076547754433E-9
-1.9715944540746044E-9
-1.888840354567171E-9
-2.0136455875146353E-9
-1.7497909810950412E-9
-1.9173040910303298E-9
-1.9484552163892866E-9
-1.75461281171563E-9
-1.6895192716858934E-9
-1.977665721964048E-9
-1.9161181022615846E-9
-1.7124270992474587E-9
-2.10562261353376E-9
-1.927606445499094E-9
-1.8999468496311458E-9
-1.8623587712736638E-9
-2.0618890067350488E-9
-2.0127398928265088E-9
-1.9234324407647673E-9
-1.8440465792114317E-9
-1.7059980680548531E-9
-1.7195007472092497E-9
-2.0272854824493315E-9
-1.9586867653649628E-9
-2.0126203390185054E-9
-1.997811183086099E-9
-1.9933187320393003E-9
-1.9795499745219076E-9
-1.7301672910073477E-9
-1.7798987061805742E-9
-1.9816261491780657E-9
-2.13916402875411E-9
SystemML Statistics:
Total execution time:		120.566 sec.
Number of executed Spark inst:	0.

Running: spark-submit --class SparkMLAlgorithms 

 --driver-cores 16 


  
./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
Running: taskset -c 0-15 spark-submit --class SparkMLAlgorithms  --driver-cores 16   ./mllib/target/scala-2.10/MLLibAlgs-assembly-0.1.jar opType=robust mattype=tall nrow="10000000" ncol=100 fixedAxis=100 step=10 nproc=16
18/01/28 08:21:39 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 10.11.10.8 instead (on interface ens3)
18/01/28 08:21:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/28 08:21:42 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
Test: 0
Exception in thread "main" java.lang.IllegalArgumentException: requirement failed: 10000000 x 10000000 dense matrix is too large to allocate
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.mllib.linalg.DenseMatrix$.zeros(Matrices.scala:459)
	at org.apache.spark.mllib.linalg.DenseMatrix$.diag(Matrices.scala:530)
	at org.apache.spark.mllib.linalg.Matrices$.diag(Matrices.scala:1098)
	at SparkMLAlgorithms$.robust_se(spark_ml_algs.scala:151)
	at SparkMLAlgorithms$$anonfun$main$1.apply$mcVI$sp(spark_ml_algs.scala:78)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at SparkMLAlgorithms$.main(spark_ml_algs.scala:67)
	at SparkMLAlgorithms.main(spark_ml_algs.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: requested CPU count exceeds number available
Setting CPU count to: 24

End log file:  ../output/make.log
