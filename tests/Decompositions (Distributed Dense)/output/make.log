
 make.py started: 2018-05-17 05:09:04 /home/ubuntu/SLAB/tests/Decompositions (Distributed Dense)/src 


name := "SystemMLMatrixOps"

version := "0.1"

scalaVersion := "2.10.4"

libraryDependencies ++= Seq(
    "org.apache.spark"     % "spark-sql_2.11"  % "2.1.1" % "provided"
)

assemblyMergeStrategy in assembly := {
  case PathList("javax", "servlet", xs @ _*) => MergeStrategy.last
  case PathList("javax", "activation", xs @ _*) => MergeStrategy.last
  case PathList("org", "apache", xs @ _*) => MergeStrategy.last
  case PathList("com", "google", xs @ _*) => MergeStrategy.last
  case PathList("com", "esotericsoftware", xs @ _*) => MergeStrategy.last
  case PathList("com", "codahale", xs @ _*) => MergeStrategy.last
  case PathList("com", "yammer", xs @ _*) => MergeStrategy.last
  case "about.html" => MergeStrategy.rename
  case "META-INF/ECLIPSEF.RSA" => MergeStrategy.last
  case "META-INF/mailcap" => MergeStrategy.last
  case "META-INF/mimetypes.default" => MergeStrategy.last
  case "plugin.properties" => MergeStrategy.last
  case "log4j.properties" => MergeStrategy.last
  case x =>
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}

Running: sbt -Dsbt.log.noformat=true assembly 

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
name := "MLLibMatrixops"

version := "0.1"

scalaVersion := "2.10.4"

libraryDependencies ++= Seq (
    "org.apache.spark" %% "spark-core" % "2.2.0" % "provided",
    "org.apache.spark" %% "spark-streaming" % "2.2.0" % "provided",
    "org.apache.spark" %% "spark-mllib" % "2.2.0" % "provided"
)

assemblyMergeStrategy in assembly := {
  case PathList("javax", "servlet", xs @ _*) => MergeStrategy.last
  case PathList("javax", "activation", xs @ _*) => MergeStrategy.last
  case PathList("org", "apache", xs @ _*) => MergeStrategy.last
  case PathList("com", "google", xs @ _*) => MergeStrategy.last
  case PathList("com", "esotericsoftware", xs @ _*) => MergeStrategy.last
  case PathList("com", "codahale", xs @ _*) => MergeStrategy.last
  case PathList("com", "yammer", xs @ _*) => MergeStrategy.last
  case "about.html" => MergeStrategy.rename
  case "META-INF/ECLIPSEF.RSA" => MergeStrategy.last
  case "META-INF/mailcap" => MergeStrategy.last
  case "META-INF/mimetypes.default" => MergeStrategy.last
  case "plugin.properties" => MergeStrategy.last
  case "log4j.properties" => MergeStrategy.last
  case x =>
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}

Running: sbt -Dsbt.log.noformat=true assembly 

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
# Copyright 2018 Anthony H Thomas and Arun Kumar
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import sys
import shutil

sys.path.append('../external/lib/python')
import make_utils as utils
import global_params as params
import gen_data as data

nodes = sys.argv[1]
matsize = sys.argv[2].split(' ')
systems = sys.argv[3].split(' ')
ops = ["SVD"]

cmd_args = ('opType={opType} mattype={mattype}'
            ' Mpath={Mpath} Npath={Npath}'
            ' wPath={wPath} tableStub={tableStub}'
            ' nodes={nodes} passPath=/scratch/pass.csv'
            ' outdir=scale_mat_size savestub={savestub}')

data.gen_data_disk('../temp/pass.csv', 2, 2, 2**12)
utils.hdfs_put('../temp/pass.csv')

for op in ops:
    for gb in matsize:
        mattype_m = 'tall' if op != 'GMM' else 'wide'
        mattype_n = 'tall'

        Mpath_disk = '../external/disk_data/M{}_{}.csv'.format(gb,mattype_m)
        wPath_disk = '../external/disk_data/w{}_{}.csv'.format(gb,mattype_m)
        Npath_disk = '../external/disk_data/N{}_{}.csv'.format(gb,mattype_n)
        if op == 'GMM':
            NPath_disk = '../external/disk_data/M{}_tall.csv'.format(gb)

        Mpath_hdfs = Mpath_disk.replace('../external/disk_data', '/scratch')
        wPath_hdfs = wPath_disk.replace('../external/disk_data', '/scratch')
        Npath_hdfs = Npath_disk.replace('../external/disk_data', '/scratch')

        cmd_params_disk = {'mattype' : mattype_m,
                   'Mpath'   : Mpath_disk,
                   'wPath'   : wPath_disk,
                   'Npath'   : Npath_disk,
                   'nodes'   : nodes,
                   'savestub': gb,
                   'tableStub' : '{}_{}'.format(gb, mattype_m)}
        cmd_params_hdfs = {'mattype' : mattype_m,
                   'Mpath'   : Mpath_hdfs,
                   'wPath'   : wPath_hdfs,
                   'Npath'   : Npath_hdfs,
                   'nodes'   : nodes,
                   'savestub': gb,
                   'tableStub' : '{}_{}'.format(gb, mattype_m)}

        cmd_params_disk['opType'] = op
        cmd_params_hdfs['opType'] = op
        args_disk = cmd_args.format(**cmd_params_disk)
        args_hdfs = cmd_args.format(**cmd_params_hdfs)

        if 'MLLIB' in systems:
          utils.run_spark(program = 'SparkDecompositions',
                 sbt_dir = './mllib',
                 cmd_args = args_hdfs)
        if 'MADLIB' in systems:
          utils.run_python(program = 'madlib_matrix_ops.py',
                  cmd_args = args_disk)
        if 'R' in systems:
          utils.run_pbdR(program = 'R_matrix_ops.R',
                cmd_args = args_disk)

Running: python _msize_scaling_tests.py None "2 4 8 16" "SYSTEMML MLLIB"

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds

hdfs dfs -copyFromLocal -f "../temp/pass.csv" /scratch//pass.csv

hdfs dfs -copyFromLocal -f ../temp/pass.csv.mtd /scratch//pass.csv.mtd
./mllib/src/main/scala/spark_matrix_ops.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkDecompositions {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibDecompositions")
        val sc = new SparkContext(conf)

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val mattype = argMap("mattype")
        val opType = argMap("opType")
        val mPath = argMap("Mpath")
        val nPath = argMap("Npath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val outdir = argMap("outdir")

        val stub = s"/tests/Decompositions (Distributed Dense)/output/${outdir}/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val results : Array[Double] = opType match {
            case "SVD" => doRowMatrixOp(mPath, "SVD", sc)
            case _       => Array[Double](0.0, 0.0, 0.0, 0.0, 0.0)
        }

        File(path).appendAll(
            nodes + "," + results.mkString(",") + '\n')
    }

    def doRowMatrixOp(Mpath: String,
                      opType: String,
                      sc: SparkContext) : Array[Double] = {

        val M = readMM(Mpath, sc)
        val rows = M.numRows()
        val cols = M.numCols()
        println( M.rows.count )

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            val start = System.nanoTime()
            
            if (opType == "SVD") {
                val res = M.computeSVD(10, true)
                println(res.U.rows.count)
                println(res.V.toArray.length)
            }
            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        val results = Array[Double](rows, cols) ++ times
        return results
    }

    def readMM(path: String, sc: SparkContext) : IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }
    
}

Running: spark-submit --class SparkDecompositions   ./mllib/target/scala-2.10/MLLibMatrixops-assembly-0.1.jar opType=SVD mattype=tall Mpath=/scratch/M2_tall.csv Npath=/scratch/N2_tall.csv wPath=/scratch/w2_tall.csv tableStub=2_tall nodes=None passPath=/scratch/pass.csv outdir=scale_mat_size savestub=2

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_matrix_ops.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkDecompositions {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibDecompositions")
        val sc = new SparkContext(conf)

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val mattype = argMap("mattype")
        val opType = argMap("opType")
        val mPath = argMap("Mpath")
        val nPath = argMap("Npath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val outdir = argMap("outdir")

        val stub = s"/tests/Decompositions (Distributed Dense)/output/${outdir}/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val results : Array[Double] = opType match {
            case "SVD" => doRowMatrixOp(mPath, "SVD", sc)
            case _       => Array[Double](0.0, 0.0, 0.0, 0.0, 0.0)
        }

        File(path).appendAll(
            nodes + "," + results.mkString(",") + '\n')
    }

    def doRowMatrixOp(Mpath: String,
                      opType: String,
                      sc: SparkContext) : Array[Double] = {

        val M = readMM(Mpath, sc)
        val rows = M.numRows()
        val cols = M.numCols()
        println( M.rows.count )

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            val start = System.nanoTime()
            
            if (opType == "SVD") {
                val res = M.computeSVD(10, true)
                println(res.U.rows.count)
                println(res.V.toArray.length)
            }
            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        val results = Array[Double](rows, cols) ++ times
        return results
    }

    def readMM(path: String, sc: SparkContext) : IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }
    
}

Running: spark-submit --class SparkDecompositions   ./mllib/target/scala-2.10/MLLibMatrixops-assembly-0.1.jar opType=SVD mattype=tall Mpath=/scratch/M4_tall.csv Npath=/scratch/N4_tall.csv wPath=/scratch/w4_tall.csv tableStub=4_tall nodes=None passPath=/scratch/pass.csv outdir=scale_mat_size savestub=4

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_matrix_ops.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkDecompositions {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibDecompositions")
        val sc = new SparkContext(conf)

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val mattype = argMap("mattype")
        val opType = argMap("opType")
        val mPath = argMap("Mpath")
        val nPath = argMap("Npath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val outdir = argMap("outdir")

        val stub = s"/tests/Decompositions (Distributed Dense)/output/${outdir}/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val results : Array[Double] = opType match {
            case "SVD" => doRowMatrixOp(mPath, "SVD", sc)
            case _       => Array[Double](0.0, 0.0, 0.0, 0.0, 0.0)
        }

        File(path).appendAll(
            nodes + "," + results.mkString(",") + '\n')
    }

    def doRowMatrixOp(Mpath: String,
                      opType: String,
                      sc: SparkContext) : Array[Double] = {

        val M = readMM(Mpath, sc)
        val rows = M.numRows()
        val cols = M.numCols()
        println( M.rows.count )

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            val start = System.nanoTime()
            
            if (opType == "SVD") {
                val res = M.computeSVD(10, true)
                println(res.U.rows.count)
                println(res.V.toArray.length)
            }
            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        val results = Array[Double](rows, cols) ++ times
        return results
    }

    def readMM(path: String, sc: SparkContext) : IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }
    
}

Running: spark-submit --class SparkDecompositions   ./mllib/target/scala-2.10/MLLibMatrixops-assembly-0.1.jar opType=SVD mattype=tall Mpath=/scratch/M8_tall.csv Npath=/scratch/N8_tall.csv wPath=/scratch/w8_tall.csv tableStub=8_tall nodes=None passPath=/scratch/pass.csv outdir=scale_mat_size savestub=8

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
./mllib/src/main/scala/spark_matrix_ops.scala
================================================================================
// Copyright 2018 Anthony H Thomas and Arun Kumar
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import scala.math._
import java.nio.file.{Paths, Files}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.linalg
import org.apache.spark.mllib.random.RandomRDDs._
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.sql._
import scala.tools.nsc.io._
import scala.io.Source
import java.util.Random
import scala.collection.immutable._

object SparkDecompositions {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("MLLibDecompositions")
        val sc = new SparkContext(conf)

        val root = sys.env("BENCHMARK_PROJECT_ROOT")

        val argMap = Map[String,String](
                args.map(_.split("=")).map({
                    case Array(x,y) => (x -> y)
                }):_*
            )

        val mattype = argMap("mattype")
        val opType = argMap("opType")
        val mPath = argMap("Mpath")
        val nPath = argMap("Npath")
        val passPath = argMap("passPath")
        val nodes = argMap("nodes")
        val outdir = argMap("outdir")

        val stub = s"/tests/Decompositions (Distributed Dense)/output/${outdir}/"
        val base = s"mllib_${mattype}_${opType}${nodes}.txt"
        val path = root + stub + base

        if (!Files.exists(Paths.get(path))) {
          File(path).writeAll("nodes,rows,cols,time1,time2,time3,time4,time5\n")
        }

        val results : Array[Double] = opType match {
            case "SVD" => doRowMatrixOp(mPath, "SVD", sc)
            case _       => Array[Double](0.0, 0.0, 0.0, 0.0, 0.0)
        }

        File(path).appendAll(
            nodes + "," + results.mkString(",") + '\n')
    }

    def doRowMatrixOp(Mpath: String,
                      opType: String,
                      sc: SparkContext) : Array[Double] = {

        val M = readMM(Mpath, sc)
        val rows = M.numRows()
        val cols = M.numCols()
        println( M.rows.count )

        val times = Array[Double](0,0,0,0,0)
        for (ix <- 0 to 4) {
            val start = System.nanoTime()
            
            if (opType == "SVD") {
                val res = M.computeSVD(10, true)
                println(res.U.rows.count)
                println(res.V.toArray.length)
            }
            val stop = System.nanoTime()
            times(ix) = (stop - start)/1e9
        }

        val results = Array[Double](rows, cols) ++ times
        return results
    }

    def readMM(path: String, sc: SparkContext) : IndexedRowMatrix = {
        val M = sc.textFile(path, 500).zipWithIndex().
                         map(
                            tup => new IndexedRow(
                                tup._2.toInt,
                                new DenseVector(tup._1.split(",").
                                    map(_.toDouble))))
        val BM = new IndexedRowMatrix(M)
        BM.rows.persist(MEMORY_AND_DISK_SER)
        return BM
    }
    
}

Running: spark-submit --class SparkDecompositions   ./mllib/target/scala-2.10/MLLibMatrixops-assembly-0.1.jar opType=SVD mattype=tall Mpath=/scratch/M16_tall.csv Npath=/scratch/N16_tall.csv wPath=/scratch/w16_tall.csv tableStub=16_tall nodes=None passPath=/scratch/pass.csv outdir=scale_mat_size savestub=16

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds

 make.py ended: 2018-05-17 05:33:25
