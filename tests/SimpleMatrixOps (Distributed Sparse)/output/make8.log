
 make.py started: 2018-06-13 02:43:54 /home/ubuntu/SLAB/tests/SimpleMatrixOps (Distributed Sparse)/src 


name := "SystemMLMatrixOps"

version := "0.1"

scalaVersion := "2.10.4"

libraryDependencies ++= Seq(
    "org.apache.spark" %% "spark-mllib"  % "2.2.0" % "provided"
)

assemblyMergeStrategy in assembly := {
  case PathList("javax", "servlet", xs @ _*) => MergeStrategy.last
  case PathList("javax", "activation", xs @ _*) => MergeStrategy.last
  case PathList("org", "apache", xs @ _*) => MergeStrategy.last
  case PathList("com", "google", xs @ _*) => MergeStrategy.last
  case PathList("com", "esotericsoftware", xs @ _*) => MergeStrategy.last
  case PathList("com", "codahale", xs @ _*) => MergeStrategy.last
  case PathList("com", "yammer", xs @ _*) => MergeStrategy.last
  case "about.html" => MergeStrategy.rename
  case "META-INF/ECLIPSEF.RSA" => MergeStrategy.last
  case "META-INF/mailcap" => MergeStrategy.last
  case "META-INF/mimetypes.default" => MergeStrategy.last
  case "plugin.properties" => MergeStrategy.last
  case "log4j.properties" => MergeStrategy.last
  case x =>
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}

Running: sbt -Dsbt.log.noformat=true assembly 

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
name := "MLLibMatrixops"

version := "0.1"

scalaVersion := "2.10.4"

libraryDependencies ++= Seq (
    "org.apache.spark" %% "spark-core" % "2.2.0" % "provided",
    "org.apache.spark" %% "spark-streaming" % "2.2.0" % "provided",
    "org.apache.spark" %% "spark-mllib" % "2.2.0" % "provided"
)

assemblyMergeStrategy in assembly := {
  case PathList("javax", "servlet", xs @ _*) => MergeStrategy.last
  case PathList("javax", "activation", xs @ _*) => MergeStrategy.last
  case PathList("org", "apache", xs @ _*) => MergeStrategy.last
  case PathList("com", "google", xs @ _*) => MergeStrategy.last
  case PathList("com", "esotericsoftware", xs @ _*) => MergeStrategy.last
  case PathList("com", "codahale", xs @ _*) => MergeStrategy.last
  case PathList("com", "yammer", xs @ _*) => MergeStrategy.last
  case "about.html" => MergeStrategy.rename
  case "META-INF/ECLIPSEF.RSA" => MergeStrategy.last
  case "META-INF/mailcap" => MergeStrategy.last
  case "META-INF/mimetypes.default" => MergeStrategy.last
  case "plugin.properties" => MergeStrategy.last
  case "log4j.properties" => MergeStrategy.last
  case x =>
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}

Running: sbt -Dsbt.log.noformat=true assembly 

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
# Copyright 2018 Anthony H Thomas and Arun Kumar
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import sys

sys.path.append('../external/lib/python')
import make_utils as utils
import global_params as params
import gen_data as data

nodes = sys.argv[1]
sparsity = sys.argv[2].split(' ')
systems = sys.argv[3].split(' ')
op_types = sys.argv[4].split(' ')
sparse_gb = sys.argv[5]

all_files = os.listdir('../output/scale_mat_size')
#for s in systems:
#    for op in op_types:
#        relevant_files = filter(
#            lambda x: (s in x) and (op in x) and (nodes in x), all_files)
#        map(lambda x: 
#            os.unlink('../output/scale_mat_size/{}'.format(x)), relevant_files)

cmd_args = ('opType={opType} mattype={mattype}'
            ' Mpath={Mpath} Npath={Npath}'
            ' wPath={wPath} tableStub={tableStub}'
            ' nodes={nodes} passPath=/scratch/pass.csv'
            ' savestub={savestub} sr={sr} '
            ' outdir=../output/scale_mat_size')

data.gen_data_disk('../temp/pass.csv', 2, 2, 2**12)
#utils.hdfs_put('../temp/pass.csv')

gb = sparse_gb
for op in op_types:
    for sr in sparsity:
        mattype_m = 'tall' if op != 'GMM' else 'wide'
        mattype_n = 'tall'
        fmt = (sr, gb, mattype_m)

        Mpath_disk = '../external/disk_data/M_{}{}_sparse_{}.mtx'.format(*fmt)
        wPath_disk = '../external/disk_data/w_{}{}_sparse_{}.mtx'.format(*fmt)
        Npath_disk = '../external/disk_data/N_{}{}_sparse_{}.mtx'.format(*fmt)
        if op == 'GMM':
            Npath_disk = Mpath_disk.replace('wide','tall')

        Mpath_hdfs = Mpath_disk.replace('../external/disk_data', '/scratch')
        wPath_hdfs = wPath_disk.replace('../external/disk_data', '/scratch')
        Npath_hdfs = Npath_disk.replace('../external/disk_data', '/scratch')

        cmd_params_disk = {'mattype' : mattype_m,
                           'Mpath'   : Mpath_disk,
                           'wPath'   : wPath_disk,
                           'Npath'   : Npath_disk,
                           'nodes'   : 8,
                           'savestub': gb,
                           'sr'      : sr,
                           'tableStub' : '_{}{}_sparse_{}'.format(*fmt)}
        cmd_params_hdfs = {'mattype' : mattype_m,
                           'Mpath'   : Mpath_hdfs,
                           'wPath'   : wPath_hdfs,
                           'Npath'   : Npath_hdfs,
                           'nodes'   : 8,
                           'savestub': gb,
                           'sr'      : sr,
                           'tableStub' : '_{}{}_sparse_{}'.format(*fmt)}

        cmd_params_disk['opType'] = op
        cmd_params_hdfs['opType'] = op
        args_disk = cmd_args.format(**cmd_params_disk)
        args_hdfs = cmd_args.format(**cmd_params_hdfs)

        if 'SYSTEMML' in systems:
            utils.run_spark(program = 'SystemMLMatrixOps',
                            sbt_dir = './systemml',
                            cmd_args = args_hdfs)
        if 'MLLIB' in systems:
            utils.run_spark(program = 'SparkMatrixOps',
                            sbt_dir = './mllib',
                            cmd_args = args_hdfs)
        if ('MADLIB' in systems) and (op != 'MVM'):
            utils.run_python(program = 'madlib_matrix_ops.py',
                             cmd_args = args_disk)

        if 'SCIDB' in systems:
            utils.run_python(program = 'scidb_matrix_ops.py',
                             cmd_args = args_disk)

# stop logging
end_make_logging()

Running: python _msize_scaling_tests.py 8 "0001 001 01 1" "SCIDB" "GMM" 100

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
# Copyright 2018 Anthony H Thomas and Arun Kumar
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

with open(__file__) as fh: print fh.read()
import os
import sys
import time
import json
import scidbpy
import subprocess

import numpy as np
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from np_timing_utils import parse_cmd_args

def do_matrix_op(kwargs):
    op_type  = kwargs.get('opType')
    mattype = kwargs.get('mattype')
    tableStub = kwargs.get('tableStub')
    savestub = kwargs.get('savestub')
    nodes = kwargs.get('nodes')
    outdir = kwargs.get('outdir')
    sr = kwargs.get('sr')
    sr_val = np.float64('0.{}'.format(sr))

    nrow, ncol = 125000000, 100
    path = '../output/scidb_{}_{}{}.txt'.format(mattype, op_type, nodes)

    cxn = scidbpy.connect()
    print cxn.iquery("list('instances')", fetch=True)
    colnames = ['nodes','sr','time1','time2','time3','time4','time5']
    run_times = pd.DataFrame(np.zeros((1,len(colnames))))
    run_times.columns = colnames

    M_name = 'M{}'.format(sr)
    if not M_name in dir(cxn.arrays):
        alloc_matrix(nrow, ncol, M_name, cxn, density=sr_val)
    if op_type == 'GMM':
        if not 'M{}W' in dir(cxn.arrays):
            alloc_matrix(
                ncol, nrow, 'M{}W'.format(sr), cxn, density=sr_val)
        if not 'N{}'.format(sr) in dir(cxn.arrays):
            alloc_matrix(
                nrow, ncol, 'N{}'.format(sr), cxn, density=sr_val)
        N_name = 'N{}'.format(sr)
        M_name = 'M{}W'.format(sr)
    if op_type == 'ADD':
        if not 'N{}'.format(sr) in dir(cxn.arrays):
            alloc_matrix(
                nrow, ncol, 'N{}'.format(sr), cxn, density=sr_val)
        N_name = 'N{}'.format(sr)
    if op_type == 'MVM':
        v_name = 'v{}'.format(ncol)
        if not v_name in dir(cxn.arrays):
            alloc_vector(ncol, v_name, cxn)

    cxn.iquery("load_library('linear_algebra')")
    if op_type == 'TRANS':
        call = 'consume(transpose({}))'.format(M_name)
    elif op_type == 'NORM':
        call = 'aggregate(apply({}, val2, pow(val,2.0)), sum(val2))'.format(
            M_name)
    elif op_type == 'GMM':
        call = 'spgemm({},{})'.format(M_name, N_name)
    elif op_type == 'MVM':
        call = 'spgemm({},{})'.format(M_name, v_name)
    elif op_type == 'TSM':
        call = 'spgemm(transpose({}),{})'.format(M_name, M_name)
    elif op_type == 'ADD':
        call = 'consume(apply(join({0},{1}), sum, {0}.val+{1}.val))'.format(
            M_name, N_name)
    else:
        raise StandardError('Invalid operator type')
    
    run_times.ix[:,:2] = (nodes, sr)
    run_times.ix[:,2:] = time_stmt(call, cxn)
    write_header = False if (os.path.exists(path)) else True
    run_times.to_csv(path, index=False, header=write_header, mode='a')

def get_dims(name):
    with open('../external/disk_data/{}.csv.mtd'.format(name), 'rb') as fh:
        res = json.load(fh)
    return (res['rows'], res['cols'])

def zeros(rows, cols, name, cxn, chunksize=1000, overwrite=True):
    if overwrite and (name in dir(cxn.arrays)):
        cxn.iquery('remove({})'.format(name))
    stmt = """
        CREATE ARRAY {n}<val:double not null>[row=0:{r}:0:{cz};col=0:{c}:0:{cz}]
    """.format(r=rows, n=name, c=cols, cz=chunksize)
    cxn.iquery(stmt)

def alloc_matrix(rows, cols, name, 
        cxn, chunksize=1000, overwrite=False, density=0.0):
    
    if name in dir(cxn) and overwrite is False:
        return
    if name in dir(cxn) and overwrite:
        cxn.iquery()
    if 'tmp' in dir(cxn):
        cxn.iquery('remove(tmp)')

    method = 'iif((RANDOM() % 100000) / 100000.0 >= {}, (RANDOM() % 100) / 10.0, 0)'.format(1-density)
    stmt = """
        store(build(<val:double not null>[row=0:{r}:0:{cz};col=0:{c}:0:{cz}], {m}), tmp)
    """.format(r=rows, c=cols, cz=chunksize, m=method)
    print stmt
    cxn.iquery(stmt)

    print "store(filter(tmp, val <> 0), {n})".format(n=name)
    cxn.iquery("store(filter(tmp, val <> 0), {n})".format(n=name))
    cxn.iquery("remove(tmp)")

def alloc_vector(length, name, cxn, method='(RANDOM() % 100) / 10.0'):
    stmt = """
        store(build(<val:double not null>[row=0:{};col=0:0], {}), {})
    """.format(length, method, name)
    print stmt
    cxn.iquery(stmt)

def time_stmt(stmt, cxn, cleanup=None, n_reps=5):
    times = []
    cleanup = cleanup if cleanup is not None else []
    for ix in range(n_reps):
        if ix == 0:
            print stmt
        start = time.time()
        cxn.iquery(stmt)
        stop = time.time()
        print 'Test {} => {}'.format(ix, stop-start)
        if len(cleanup) > 0:
            for obj in cleanup:
                cxn.iquery('remove({})'.format(obj))
        times.append(stop-start)
    return times

if __name__=='__main__':
    argv = parse_cmd_args(sys.argv[1:])
    do_matrix_op(argv)

Running: python _scidb_matrix_ops.py opType=GMM mattype=wide Mpath=../external/disk_data/M_0001100_sparse_wide.mtx Npath=../external/disk_data/M_0001100_sparse_tall.mtx wPath=../external/disk_data/w_0001100_sparse_wide.mtx tableStub=_0001100_sparse_wide nodes=8 passPath=/scratch/pass.csv savestub=100 sr=0001  outdir=../output/scale_mat_size

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
# Copyright 2018 Anthony H Thomas and Arun Kumar
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

with open(__file__) as fh: print fh.read()
import os
import sys
import time
import json
import scidbpy
import subprocess

import numpy as np
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from np_timing_utils import parse_cmd_args

def do_matrix_op(kwargs):
    op_type  = kwargs.get('opType')
    mattype = kwargs.get('mattype')
    tableStub = kwargs.get('tableStub')
    savestub = kwargs.get('savestub')
    nodes = kwargs.get('nodes')
    outdir = kwargs.get('outdir')
    sr = kwargs.get('sr')
    sr_val = np.float64('0.{}'.format(sr))

    nrow, ncol = 125000000, 100
    path = '../output/scidb_{}_{}{}.txt'.format(mattype, op_type, nodes)

    cxn = scidbpy.connect()
    print cxn.iquery("list('instances')", fetch=True)
    colnames = ['nodes','sr','time1','time2','time3','time4','time5']
    run_times = pd.DataFrame(np.zeros((1,len(colnames))))
    run_times.columns = colnames

    M_name = 'M{}'.format(sr)
    if not M_name in dir(cxn.arrays):
        alloc_matrix(nrow, ncol, M_name, cxn, density=sr_val)
    if op_type == 'GMM':
        if not 'M{}W' in dir(cxn.arrays):
            alloc_matrix(
                ncol, nrow, 'M{}W'.format(sr), cxn, density=sr_val)
        if not 'N{}'.format(sr) in dir(cxn.arrays):
            alloc_matrix(
                nrow, ncol, 'N{}'.format(sr), cxn, density=sr_val)
        N_name = 'N{}'.format(sr)
        M_name = 'M{}W'.format(sr)
    if op_type == 'ADD':
        if not 'N{}'.format(sr) in dir(cxn.arrays):
            alloc_matrix(
                nrow, ncol, 'N{}'.format(sr), cxn, density=sr_val)
        N_name = 'N{}'.format(sr)
    if op_type == 'MVM':
        v_name = 'v{}'.format(ncol)
        if not v_name in dir(cxn.arrays):
            alloc_vector(ncol, v_name, cxn)

    cxn.iquery("load_library('linear_algebra')")
    if op_type == 'TRANS':
        call = 'consume(transpose({}))'.format(M_name)
    elif op_type == 'NORM':
        call = 'aggregate(apply({}, val2, pow(val,2.0)), sum(val2))'.format(
            M_name)
    elif op_type == 'GMM':
        call = 'spgemm({},{})'.format(M_name, N_name)
    elif op_type == 'MVM':
        call = 'spgemm({},{})'.format(M_name, v_name)
    elif op_type == 'TSM':
        call = 'spgemm(transpose({}),{})'.format(M_name, M_name)
    elif op_type == 'ADD':
        call = 'consume(apply(join({0},{1}), sum, {0}.val+{1}.val))'.format(
            M_name, N_name)
    else:
        raise StandardError('Invalid operator type')
    
    run_times.ix[:,:2] = (nodes, sr)
    run_times.ix[:,2:] = time_stmt(call, cxn)
    write_header = False if (os.path.exists(path)) else True
    run_times.to_csv(path, index=False, header=write_header, mode='a')

def get_dims(name):
    with open('../external/disk_data/{}.csv.mtd'.format(name), 'rb') as fh:
        res = json.load(fh)
    return (res['rows'], res['cols'])

def zeros(rows, cols, name, cxn, chunksize=1000, overwrite=True):
    if overwrite and (name in dir(cxn.arrays)):
        cxn.iquery('remove({})'.format(name))
    stmt = """
        CREATE ARRAY {n}<val:double not null>[row=0:{r}:0:{cz};col=0:{c}:0:{cz}]
    """.format(r=rows, n=name, c=cols, cz=chunksize)
    cxn.iquery(stmt)

def alloc_matrix(rows, cols, name, 
        cxn, chunksize=1000, overwrite=False, density=0.0):
    
    if name in dir(cxn) and overwrite is False:
        return
    if name in dir(cxn) and overwrite:
        cxn.iquery()
    if 'tmp' in dir(cxn):
        cxn.iquery('remove(tmp)')

    method = 'iif((RANDOM() % 100000) / 100000.0 >= {}, (RANDOM() % 100) / 10.0, 0)'.format(1-density)
    stmt = """
        store(build(<val:double not null>[row=0:{r}:0:{cz};col=0:{c}:0:{cz}], {m}), tmp)
    """.format(r=rows, c=cols, cz=chunksize, m=method)
    print stmt
    cxn.iquery(stmt)

    print "store(filter(tmp, val <> 0), {n})".format(n=name)
    cxn.iquery("store(filter(tmp, val <> 0), {n})".format(n=name))
    cxn.iquery("remove(tmp)")

def alloc_vector(length, name, cxn, method='(RANDOM() % 100) / 10.0'):
    stmt = """
        store(build(<val:double not null>[row=0:{};col=0:0], {}), {})
    """.format(length, method, name)
    print stmt
    cxn.iquery(stmt)

def time_stmt(stmt, cxn, cleanup=None, n_reps=5):
    times = []
    cleanup = cleanup if cleanup is not None else []
    for ix in range(n_reps):
        if ix == 0:
            print stmt
        start = time.time()
        cxn.iquery(stmt)
        stop = time.time()
        print 'Test {} => {}'.format(ix, stop-start)
        if len(cleanup) > 0:
            for obj in cleanup:
                cxn.iquery('remove({})'.format(obj))
        times.append(stop-start)
    return times

if __name__=='__main__':
    argv = parse_cmd_args(sys.argv[1:])
    do_matrix_op(argv)

Running: python _scidb_matrix_ops.py opType=GMM mattype=wide Mpath=../external/disk_data/M_001100_sparse_wide.mtx Npath=../external/disk_data/M_001100_sparse_tall.mtx wPath=../external/disk_data/w_001100_sparse_wide.mtx tableStub=_001100_sparse_wide nodes=8 passPath=/scratch/pass.csv savestub=100 sr=001  outdir=../output/scale_mat_size

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
# Copyright 2018 Anthony H Thomas and Arun Kumar
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

with open(__file__) as fh: print fh.read()
import os
import sys
import time
import json
import scidbpy
import subprocess

import numpy as np
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from np_timing_utils import parse_cmd_args

def do_matrix_op(kwargs):
    op_type  = kwargs.get('opType')
    mattype = kwargs.get('mattype')
    tableStub = kwargs.get('tableStub')
    savestub = kwargs.get('savestub')
    nodes = kwargs.get('nodes')
    outdir = kwargs.get('outdir')
    sr = kwargs.get('sr')
    sr_val = np.float64('0.{}'.format(sr))

    nrow, ncol = 125000000, 100
    path = '../output/scidb_{}_{}{}.txt'.format(mattype, op_type, nodes)

    cxn = scidbpy.connect()
    print cxn.iquery("list('instances')", fetch=True)
    colnames = ['nodes','sr','time1','time2','time3','time4','time5']
    run_times = pd.DataFrame(np.zeros((1,len(colnames))))
    run_times.columns = colnames

    M_name = 'M{}'.format(sr)
    if not M_name in dir(cxn.arrays):
        alloc_matrix(nrow, ncol, M_name, cxn, density=sr_val)
    if op_type == 'GMM':
        if not 'M{}W' in dir(cxn.arrays):
            alloc_matrix(
                ncol, nrow, 'M{}W'.format(sr), cxn, density=sr_val)
        if not 'N{}'.format(sr) in dir(cxn.arrays):
            alloc_matrix(
                nrow, ncol, 'N{}'.format(sr), cxn, density=sr_val)
        N_name = 'N{}'.format(sr)
        M_name = 'M{}W'.format(sr)
    if op_type == 'ADD':
        if not 'N{}'.format(sr) in dir(cxn.arrays):
            alloc_matrix(
                nrow, ncol, 'N{}'.format(sr), cxn, density=sr_val)
        N_name = 'N{}'.format(sr)
    if op_type == 'MVM':
        v_name = 'v{}'.format(ncol)
        if not v_name in dir(cxn.arrays):
            alloc_vector(ncol, v_name, cxn)

    cxn.iquery("load_library('linear_algebra')")
    if op_type == 'TRANS':
        call = 'consume(transpose({}))'.format(M_name)
    elif op_type == 'NORM':
        call = 'aggregate(apply({}, val2, pow(val,2.0)), sum(val2))'.format(
            M_name)
    elif op_type == 'GMM':
        call = 'spgemm({},{})'.format(M_name, N_name)
    elif op_type == 'MVM':
        call = 'spgemm({},{})'.format(M_name, v_name)
    elif op_type == 'TSM':
        call = 'spgemm(transpose({}),{})'.format(M_name, M_name)
    elif op_type == 'ADD':
        call = 'consume(apply(join({0},{1}), sum, {0}.val+{1}.val))'.format(
            M_name, N_name)
    else:
        raise StandardError('Invalid operator type')
    
    run_times.ix[:,:2] = (nodes, sr)
    run_times.ix[:,2:] = time_stmt(call, cxn)
    write_header = False if (os.path.exists(path)) else True
    run_times.to_csv(path, index=False, header=write_header, mode='a')

def get_dims(name):
    with open('../external/disk_data/{}.csv.mtd'.format(name), 'rb') as fh:
        res = json.load(fh)
    return (res['rows'], res['cols'])

def zeros(rows, cols, name, cxn, chunksize=1000, overwrite=True):
    if overwrite and (name in dir(cxn.arrays)):
        cxn.iquery('remove({})'.format(name))
    stmt = """
        CREATE ARRAY {n}<val:double not null>[row=0:{r}:0:{cz};col=0:{c}:0:{cz}]
    """.format(r=rows, n=name, c=cols, cz=chunksize)
    cxn.iquery(stmt)

def alloc_matrix(rows, cols, name, 
        cxn, chunksize=1000, overwrite=False, density=0.0):
    
    if name in dir(cxn) and overwrite is False:
        return
    if name in dir(cxn) and overwrite:
        cxn.iquery()
    if 'tmp' in dir(cxn):
        cxn.iquery('remove(tmp)')

    method = 'iif((RANDOM() % 100000) / 100000.0 >= {}, (RANDOM() % 100) / 10.0, 0)'.format(1-density)
    stmt = """
        store(build(<val:double not null>[row=0:{r}:0:{cz};col=0:{c}:0:{cz}], {m}), tmp)
    """.format(r=rows, c=cols, cz=chunksize, m=method)
    print stmt
    cxn.iquery(stmt)

    print "store(filter(tmp, val <> 0), {n})".format(n=name)
    cxn.iquery("store(filter(tmp, val <> 0), {n})".format(n=name))
    cxn.iquery("remove(tmp)")

def alloc_vector(length, name, cxn, method='(RANDOM() % 100) / 10.0'):
    stmt = """
        store(build(<val:double not null>[row=0:{};col=0:0], {}), {})
    """.format(length, method, name)
    print stmt
    cxn.iquery(stmt)

def time_stmt(stmt, cxn, cleanup=None, n_reps=5):
    times = []
    cleanup = cleanup if cleanup is not None else []
    for ix in range(n_reps):
        if ix == 0:
            print stmt
        start = time.time()
        cxn.iquery(stmt)
        stop = time.time()
        print 'Test {} => {}'.format(ix, stop-start)
        if len(cleanup) > 0:
            for obj in cleanup:
                cxn.iquery('remove({})'.format(obj))
        times.append(stop-start)
    return times

if __name__=='__main__':
    argv = parse_cmd_args(sys.argv[1:])
    do_matrix_op(argv)

Running: python _scidb_matrix_ops.py opType=GMM mattype=wide Mpath=../external/disk_data/M_01100_sparse_wide.mtx Npath=../external/disk_data/M_01100_sparse_tall.mtx wPath=../external/disk_data/w_01100_sparse_wide.mtx tableStub=_01100_sparse_wide nodes=8 passPath=/scratch/pass.csv savestub=100 sr=01  outdir=../output/scale_mat_size

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds
# Copyright 2018 Anthony H Thomas and Arun Kumar
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

with open(__file__) as fh: print fh.read()
import os
import sys
import time
import json
import scidbpy
import subprocess

import numpy as np
import pandas as pd

ROOT = os.getenv('BENCHMARK_PROJECT_ROOT')
if (ROOT is None):
    msg = 'Please set environment variable BENCHMARK_PROJECT_ROOT'
    raise StandardError(msg)

sys.path.append(os.path.join(ROOT,'lib','python'))
from np_timing_utils import parse_cmd_args

def do_matrix_op(kwargs):
    op_type  = kwargs.get('opType')
    mattype = kwargs.get('mattype')
    tableStub = kwargs.get('tableStub')
    savestub = kwargs.get('savestub')
    nodes = kwargs.get('nodes')
    outdir = kwargs.get('outdir')
    sr = kwargs.get('sr')
    sr_val = np.float64('0.{}'.format(sr))

    nrow, ncol = 125000000, 100
    path = '../output/scidb_{}_{}{}.txt'.format(mattype, op_type, nodes)

    cxn = scidbpy.connect()
    print cxn.iquery("list('instances')", fetch=True)
    colnames = ['nodes','sr','time1','time2','time3','time4','time5']
    run_times = pd.DataFrame(np.zeros((1,len(colnames))))
    run_times.columns = colnames

    M_name = 'M{}'.format(sr)
    if not M_name in dir(cxn.arrays):
        alloc_matrix(nrow, ncol, M_name, cxn, density=sr_val)
    if op_type == 'GMM':
        if not 'M{}W' in dir(cxn.arrays):
            alloc_matrix(
                ncol, nrow, 'M{}W'.format(sr), cxn, density=sr_val)
        if not 'N{}'.format(sr) in dir(cxn.arrays):
            alloc_matrix(
                nrow, ncol, 'N{}'.format(sr), cxn, density=sr_val)
        N_name = 'N{}'.format(sr)
        M_name = 'M{}W'.format(sr)
    if op_type == 'ADD':
        if not 'N{}'.format(sr) in dir(cxn.arrays):
            alloc_matrix(
                nrow, ncol, 'N{}'.format(sr), cxn, density=sr_val)
        N_name = 'N{}'.format(sr)
    if op_type == 'MVM':
        v_name = 'v{}'.format(ncol)
        if not v_name in dir(cxn.arrays):
            alloc_vector(ncol, v_name, cxn)

    cxn.iquery("load_library('linear_algebra')")
    if op_type == 'TRANS':
        call = 'consume(transpose({}))'.format(M_name)
    elif op_type == 'NORM':
        call = 'aggregate(apply({}, val2, pow(val,2.0)), sum(val2))'.format(
            M_name)
    elif op_type == 'GMM':
        call = 'spgemm({},{})'.format(M_name, N_name)
    elif op_type == 'MVM':
        call = 'spgemm({},{})'.format(M_name, v_name)
    elif op_type == 'TSM':
        call = 'spgemm(transpose({}),{})'.format(M_name, M_name)
    elif op_type == 'ADD':
        call = 'consume(apply(join({0},{1}), sum, {0}.val+{1}.val))'.format(
            M_name, N_name)
    else:
        raise StandardError('Invalid operator type')
    
    run_times.ix[:,:2] = (nodes, sr)
    run_times.ix[:,2:] = time_stmt(call, cxn)
    write_header = False if (os.path.exists(path)) else True
    run_times.to_csv(path, index=False, header=write_header, mode='a')

def get_dims(name):
    with open('../external/disk_data/{}.csv.mtd'.format(name), 'rb') as fh:
        res = json.load(fh)
    return (res['rows'], res['cols'])

def zeros(rows, cols, name, cxn, chunksize=1000, overwrite=True):
    if overwrite and (name in dir(cxn.arrays)):
        cxn.iquery('remove({})'.format(name))
    stmt = """
        CREATE ARRAY {n}<val:double not null>[row=0:{r}:0:{cz};col=0:{c}:0:{cz}]
    """.format(r=rows, n=name, c=cols, cz=chunksize)
    cxn.iquery(stmt)

def alloc_matrix(rows, cols, name, 
        cxn, chunksize=1000, overwrite=False, density=0.0):
    
    if name in dir(cxn) and overwrite is False:
        return
    if name in dir(cxn) and overwrite:
        cxn.iquery()
    if 'tmp' in dir(cxn):
        cxn.iquery('remove(tmp)')

    method = 'iif((RANDOM() % 100000) / 100000.0 >= {}, (RANDOM() % 100) / 10.0, 0)'.format(1-density)
    stmt = """
        store(build(<val:double not null>[row=0:{r}:0:{cz};col=0:{c}:0:{cz}], {m}), tmp)
    """.format(r=rows, c=cols, cz=chunksize, m=method)
    print stmt
    cxn.iquery(stmt)

    print "store(filter(tmp, val <> 0), {n})".format(n=name)
    cxn.iquery("store(filter(tmp, val <> 0), {n})".format(n=name))
    cxn.iquery("remove(tmp)")

def alloc_vector(length, name, cxn, method='(RANDOM() % 100) / 10.0'):
    stmt = """
        store(build(<val:double not null>[row=0:{};col=0:0], {}), {})
    """.format(length, method, name)
    print stmt
    cxn.iquery(stmt)

def time_stmt(stmt, cxn, cleanup=None, n_reps=5):
    times = []
    cleanup = cleanup if cleanup is not None else []
    for ix in range(n_reps):
        if ix == 0:
            print stmt
        start = time.time()
        cxn.iquery(stmt)
        stop = time.time()
        print 'Test {} => {}'.format(ix, stop-start)
        if len(cleanup) > 0:
            for obj in cleanup:
                cxn.iquery('remove({})'.format(obj))
        times.append(stop-start)
    return times

if __name__=='__main__':
    argv = parse_cmd_args(sys.argv[1:])
    do_matrix_op(argv)

Running: python _scidb_matrix_ops.py opType=GMM mattype=wide Mpath=../external/disk_data/M_1100_sparse_wide.mtx Npath=../external/disk_data/M_1100_sparse_tall.mtx wPath=../external/disk_data/w_1100_sparse_wide.mtx tableStub=_1100_sparse_wide nodes=8 passPath=/scratch/pass.csv savestub=100 sr=1  outdir=../output/scale_mat_size

CPU count capped at: None
Memory use capped at: -1e-09GB
CPU Time capped at: -1 seconds

 make.py ended: 2018-06-13 13:38:52
